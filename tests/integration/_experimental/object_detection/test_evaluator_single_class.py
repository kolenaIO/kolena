# Copyright 2021-2023 Kolena Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import Dict
from typing import List
from typing import Tuple

import pytest

from kolena._experimental.object_detection import GroundTruth
from kolena._experimental.object_detection import Inference
from kolena._experimental.object_detection import ObjectDetectionEvaluator
from kolena._experimental.object_detection import TestCase
from kolena._experimental.object_detection import TestSample
from kolena._experimental.object_detection import TestSuite
from kolena._experimental.object_detection import ThresholdConfiguration
from kolena._experimental.object_detection import ThresholdStrategy
from kolena._experimental.object_detection.workflow import TestCaseMetricsSingleClass
from kolena._experimental.object_detection.workflow import TestSampleMetricsSingleClass
from kolena._experimental.object_detection.workflow import TestSuiteMetrics
from kolena.workflow.annotation import LabeledBoundingBox
from kolena.workflow.annotation import ScoredLabeledBoundingBox
from tests.integration.helper import fake_locator
from tests.integration.helper import with_test_prefix

TEST_CASE = TestCase(with_test_prefix("test_evaluator_single_class"))
TEST_CASE_2 = TestCase(with_test_prefix("test_evaluator_single_class_2"))
TEST_SUITE = TestSuite(with_test_prefix("test_evaluator_single_class_suite"))


TEST_DATA: Dict[str, List[Tuple[TestSample, GroundTruth, Inference]]] = {
    "nothing": [
        (
            TestSample(locator=fake_locator(1, "OD")),
            GroundTruth(
                bboxes=[],
            ),
            Inference(
                bboxes=[],
            ),
        ),
    ],
    "no inferences": [
        (
            TestSample(locator=fake_locator(2, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((3, 3), (4, 4), "b")],
            ),
            Inference(
                bboxes=[],
            ),
        ),
    ],
    "no ground truths": [
        (
            TestSample(locator=fake_locator(3, "OD")),
            GroundTruth(
                bboxes=[],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((1, 1), (2, 2), "b", 1)],
            ),
        ),
    ],
    "iou=1 and different labels and max confidence": [
        (
            TestSample(locator=fake_locator(4, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((1, 1), (2, 2), "a")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((1, 1), (2, 2), "b", 1)],
            ),
        ),
    ],
    "iou=0 and same labels": [
        (
            TestSample(locator=fake_locator(5, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((3, 3), (4, 4), "b")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((1, 1), (2, 2), "b", 1)],
            ),
        ),
    ],
    "iou=0.33 and same labels but 0 confidence": [
        (
            TestSample(locator=fake_locator(6, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((1, 1), (4, 4), "b")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((2, 2), (5, 5), "b", 0)],
            ),
        ),
    ],
    "iou=0.33 and same labels but 0.5 confidence": [
        (
            TestSample(locator=fake_locator(7, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((1, 1), (4, 4), "b")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((2, 2), (5, 5), "b", 0.5)],
            ),
        ),
    ],
    "iou=0.33 and same labels but 0.99 confidence": [
        (
            TestSample(locator=fake_locator(8, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((1, 1), (4, 4), "b")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((2, 2), (5, 5), "b", 0.99)],
            ),
        ),
    ],
    "iou=0.5 and same labels but 0 confidence": [
        (
            TestSample(locator=fake_locator(9, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((1, 1), (4, 4), "b")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((2, 1), (5, 4), "b", 0)],
            ),
        ),
    ],
    "iou=0.5 and same labels but 0.5 confidence": [
        (
            TestSample(locator=fake_locator(10, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((1, 1), (4, 4), "b")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((2, 1), (5, 4), "b", 0.5)],
            ),
        ),
    ],
    "iou=0.5 and same labels but 0.99 confidence": [
        (
            TestSample(locator=fake_locator(11, "OD")),
            GroundTruth(
                bboxes=[LabeledBoundingBox((1, 1), (4, 4), "b")],
            ),
            Inference(
                bboxes=[ScoredLabeledBoundingBox((2, 1), (5, 4), "b", 0.99)],
            ),
        ),
    ],
    "multiple bboxes in an image, perfect match": [
        (
            TestSample(locator=fake_locator(12, "OD")),
            GroundTruth(
                bboxes=[
                    LabeledBoundingBox((1, 1), (2, 2), "a"),
                    LabeledBoundingBox((3, 3), (4, 4), "a"),
                    LabeledBoundingBox((5, 5), (6, 6), "a"),
                    LabeledBoundingBox((7, 7), (8, 8), "a"),
                ],
            ),
            Inference(
                bboxes=[
                    ScoredLabeledBoundingBox((1, 1), (2, 2), "a", 0.9),
                    ScoredLabeledBoundingBox((3, 3), (4, 4), "a", 0.9),
                    ScoredLabeledBoundingBox((5, 5), (6, 6), "a", 0.9),
                    ScoredLabeledBoundingBox((7, 7), (8, 8), "a", 0.9),
                ],
            ),
        ),
    ],
    "multiple bboxes in an image, varied iou": [
        (
            TestSample(locator=fake_locator(13, "OD")),
            GroundTruth(
                bboxes=[
                    LabeledBoundingBox((1, 1), (2, 2), "a"),
                    LabeledBoundingBox((3, 3), (4, 4), "a"),
                    LabeledBoundingBox((5, 5), (6, 6), "a"),
                    LabeledBoundingBox((7, 7), (8, 8), "a"),
                ],
            ),
            Inference(
                bboxes=[
                    ScoredLabeledBoundingBox((1.1, 1), (2.1, 2), "a", 0.9),
                    ScoredLabeledBoundingBox((3.3, 3), (4.3, 4), "a", 0.9),
                    ScoredLabeledBoundingBox((5.5, 5), (6.5, 6), "a", 0.9),
                    ScoredLabeledBoundingBox((7.7, 7), (8.7, 8), "a", 0.9),
                ],
            ),
        ),
    ],
    "multiple bboxes in an image, varied confidence": [
        (
            TestSample(locator=fake_locator(14, "OD")),
            GroundTruth(
                bboxes=[
                    LabeledBoundingBox((1, 1), (2, 2), "a"),
                    LabeledBoundingBox((3, 3), (4, 4), "a"),
                    LabeledBoundingBox((5, 5), (6, 6), "a"),
                    LabeledBoundingBox((7, 7), (8, 8), "a"),
                ],
            ),
            Inference(
                bboxes=[
                    ScoredLabeledBoundingBox((1, 1), (2, 2), "a", 0.6),
                    ScoredLabeledBoundingBox((3, 3), (4, 4), "a", 0.5),
                    ScoredLabeledBoundingBox((5, 5), (6, 6), "a", 0.4),
                    ScoredLabeledBoundingBox((7, 7), (8, 8), "a", 0.01),
                ],
            ),
        ),
    ],
    "multiple bboxes in an image, many inferences": [
        (
            TestSample(locator=fake_locator(15, "OD")),
            GroundTruth(
                bboxes=[
                    LabeledBoundingBox((1, 1), (2, 2), "a"),
                    LabeledBoundingBox((3, 3), (4, 4), "a"),
                    LabeledBoundingBox((5, 5), (6, 6), "a"),
                    LabeledBoundingBox((7, 7), (8, 8), "a"),
                ],
            ),
            Inference(
                bboxes=[
                    ScoredLabeledBoundingBox((0, 0), (1, 1), "a", 0.99),
                    ScoredLabeledBoundingBox((0, 0), (1, 1), "a", 0.99),
                    ScoredLabeledBoundingBox((0, 0), (1, 1), "a", 0.99),
                    ScoredLabeledBoundingBox((0, 0), (1, 1), "a", 0.99),
                    ScoredLabeledBoundingBox((1, 1), (2, 2), "a", 0.99),
                    ScoredLabeledBoundingBox((3, 3), (4, 4), "a", 0.99),
                    ScoredLabeledBoundingBox((5, 5), (6, 6), "a", 0.99),
                    ScoredLabeledBoundingBox((7, 7), (8, 8), "a", 0.99),
                ],
            ),
        ),
    ],
    "multiple bboxes in an image, too few inferences": [
        (
            TestSample(locator=fake_locator(16, "OD")),
            GroundTruth(
                bboxes=[
                    LabeledBoundingBox((1, 1), (2, 2), "a"),
                    LabeledBoundingBox((3, 3), (4, 4), "a"),
                    LabeledBoundingBox((5, 5), (6, 6), "a"),
                    LabeledBoundingBox((7, 7), (8, 8), "a"),
                ],
            ),
            Inference(
                bboxes=[
                    ScoredLabeledBoundingBox((1, 1), (2, 2), "a", 0.99),
                    ScoredLabeledBoundingBox((7, 7), (8, 8), "a", 0.99),
                ],
            ),
        ),
    ],
    "multiple bboxes in an image, suboptimal infs": [
        (
            TestSample(locator=fake_locator(17, "OD")),
            GroundTruth(
                bboxes=[
                    LabeledBoundingBox((1, 1), (2, 2), "a"),
                    LabeledBoundingBox((3, 3), (4, 4), "a"),
                    LabeledBoundingBox((5, 5), (6, 6), "a"),
                    LabeledBoundingBox((7, 7), (8, 8), "a"),
                ],
            ),
            Inference(
                bboxes=[
                    ScoredLabeledBoundingBox((1, 1), (2, 2), "a", 0.001),
                    ScoredLabeledBoundingBox((5, 5), (6, 6), "a", 0.9),
                    ScoredLabeledBoundingBox((7, 7), (9, 9), "a", 0.9),
                ],
            ),
        ),
    ],
    "multiple bboxes in an image, ignored matches": [
        (
            TestSample(locator=fake_locator(18, "OD")),
            GroundTruth(
                bboxes=[
                    LabeledBoundingBox((1, 1), (2, 2), "a"),
                    LabeledBoundingBox((3, 3), (4, 4), "a"),
                ],
                ignored_bboxes=[
                    LabeledBoundingBox((5, 5), (6, 6), "a"),
                    LabeledBoundingBox((7, 7), (8, 8), "a"),
                ],
            ),
            Inference(
                bboxes=[
                    ScoredLabeledBoundingBox((1, 1), (2, 2), "a", 0.9),
                    ScoredLabeledBoundingBox((3, 3), (4, 4), "a", 0.9),
                    ScoredLabeledBoundingBox((5, 5), (6, 6), "a", 0.01),
                    ScoredLabeledBoundingBox((7, 7), (8, 8), "a", 0.9),
                ],
            ),
        ),
    ],
}


TEST_CONFIGURATIONS: Dict[str, ThresholdConfiguration] = {
    "Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0": ThresholdConfiguration(
        threshold_strategy=ThresholdStrategy.FIXED_03,
        iou_threshold=0.3,
        min_confidence_score=0.0,
        with_class_level_metrics=False,
    ),
    "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0": ThresholdConfiguration(
        threshold_strategy=ThresholdStrategy.FIXED_05,
        iou_threshold=0.5,
        min_confidence_score=0.0,
        with_class_level_metrics=False,
    ),
    "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3": ThresholdConfiguration(
        threshold_strategy=ThresholdStrategy.FIXED_05,
        iou_threshold=0.5,
        min_confidence_score=0.3,
        with_class_level_metrics=False,
    ),
    "Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1": ThresholdConfiguration(
        threshold_strategy=ThresholdStrategy.F1_OPTIMAL,
        iou_threshold=0.5,
        min_confidence_score=0.1,
        with_class_level_metrics=False,
    ),
}


# evaluator_configuration -> test_name -> test_sample_metrics
EXPECTED_COMPUTE_TEST_SAMPLE_METRICS: Dict[str, Dict[str, List[Tuple[TestSample, TestSampleMetricsSingleClass]]]] = {
    "Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0": {
        "nothing": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[],
                    count_TP=0,
                    count_FP=0,
                    count_FN=0,
                    has_TP=False,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.3,
                ),
            ),
        ],
        "no inferences": [
            (
                TestSample(locator=fake_locator(2, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.3,
                ),
            ),
        ],
        "no ground truths": [
            (
                TestSample(locator=fake_locator(3, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[],
                    count_TP=0,
                    count_FP=1,
                    count_FN=0,
                    has_TP=False,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=1 and different labels and max confidence": [
            (
                TestSample(locator=fake_locator(4, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=0 and same labels": [
            (
                TestSample(locator=fake_locator(5, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(6, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(7, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.5)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(8, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.99)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(9, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.5)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.3,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.99)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.3,
                ),
            ),
        ],
        "multiple bboxes in an image, perfect match": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=4,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.3,
                ),
            ),
        ],
        "multiple bboxes in an image, varied iou": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.1, 1.0), bottom_right=(2.1, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.3, 3.0), bottom_right=(4.3, 4.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(5.5, 5.0), bottom_right=(6.5, 6.0), label="a", score=0.9),
                    ],
                    FP=[ScoredLabeledBoundingBox(top_left=(7.7, 7.0), bottom_right=(8.7, 8.0), label="a", score=0.9)],
                    FN=[LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a")],
                    count_TP=3,
                    count_FP=1,
                    count_FN=1,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.3,
                ),
            ),
        ],
        "multiple bboxes in an image, varied confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.6),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.5),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.4),
                    ],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a")],
                    count_TP=3,
                    count_FP=0,
                    count_FN=1,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.6,
                    min_confidence_above_t=0.4,
                    thresholds=0.3,
                ),
            ),
        ],
        "multiple bboxes in an image, many inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                    ],
                    FN=[],
                    count_TP=4,
                    count_FP=4,
                    count_FN=0,
                    has_TP=True,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.3,
                ),
            ),
        ],
        "multiple bboxes in an image, too few inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[],
                    FN=[
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=0,
                    count_FN=2,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.3,
                ),
            ),
        ],
        "multiple bboxes in an image, suboptimal infs": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9)],
                    FP=[ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(9.0, 9.0), label="a", score=0.9)],
                    FN=[
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                        LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a"),
                    ],
                    count_TP=1,
                    count_FP=1,
                    count_FN=3,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.3,
                ),
            ),
        ],
        "multiple bboxes in an image, ignored matches": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=2,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.3,
                ),
            ),
        ],
    },
    "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0": {
        "nothing": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[],
                    count_TP=0,
                    count_FP=0,
                    count_FN=0,
                    has_TP=False,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "no inferences": [
            (
                TestSample(locator=fake_locator(2, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "no ground truths": [
            (
                TestSample(locator=fake_locator(3, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[],
                    count_TP=0,
                    count_FP=1,
                    count_FN=0,
                    has_TP=False,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=1 and different labels and max confidence": [
            (
                TestSample(locator=fake_locator(4, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0 and same labels": [
            (
                TestSample(locator=fake_locator(5, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(6, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(7, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.5)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(8, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.99)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(9, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.5)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.99)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, perfect match": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=4,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, varied iou": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.1, 1.0), bottom_right=(2.1, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.3, 3.0), bottom_right=(4.3, 4.0), label="a", score=0.9),
                    ],
                    FP=[
                        ScoredLabeledBoundingBox(top_left=(5.5, 5.0), bottom_right=(6.5, 6.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(7.7, 7.0), bottom_right=(8.7, 8.0), label="a", score=0.9),
                    ],
                    FN=[
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=2,
                    count_FN=2,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, varied confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.6),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.5),
                    ],
                    FP=[],
                    FN=[
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=0,
                    count_FN=2,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.6,
                    min_confidence_above_t=0.5,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, many inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                    ],
                    FN=[],
                    count_TP=4,
                    count_FP=4,
                    count_FN=0,
                    has_TP=True,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, too few inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[],
                    FN=[
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=0,
                    count_FN=2,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, suboptimal infs": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9)],
                    FP=[ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(9.0, 9.0), label="a", score=0.9)],
                    FN=[
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                        LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a"),
                    ],
                    count_TP=1,
                    count_FP=1,
                    count_FN=3,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, ignored matches": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=2,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
    },
    "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3": {
        "nothing": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[],
                    count_TP=0,
                    count_FP=0,
                    count_FN=0,
                    has_TP=False,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "no inferences": [
            (
                TestSample(locator=fake_locator(2, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "no ground truths": [
            (
                TestSample(locator=fake_locator(3, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[],
                    count_TP=0,
                    count_FP=1,
                    count_FN=0,
                    has_TP=False,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=1 and different labels and max confidence": [
            (
                TestSample(locator=fake_locator(4, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0 and same labels": [
            (
                TestSample(locator=fake_locator(5, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(6, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(7, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.5)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(8, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.99)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(9, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.5)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.5,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.99)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, perfect match": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=4,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, varied iou": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.1, 1.0), bottom_right=(2.1, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.3, 3.0), bottom_right=(4.3, 4.0), label="a", score=0.9),
                    ],
                    FP=[
                        ScoredLabeledBoundingBox(top_left=(5.5, 5.0), bottom_right=(6.5, 6.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(7.7, 7.0), bottom_right=(8.7, 8.0), label="a", score=0.9),
                    ],
                    FN=[
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=2,
                    count_FN=2,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, varied confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.6),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.5),
                    ],
                    FP=[],
                    FN=[
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=0,
                    count_FN=2,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.6,
                    min_confidence_above_t=0.5,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, many inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                    ],
                    FN=[],
                    count_TP=4,
                    count_FP=4,
                    count_FN=0,
                    has_TP=True,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, too few inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[],
                    FN=[
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=0,
                    count_FN=2,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, suboptimal infs": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9)],
                    FP=[ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(9.0, 9.0), label="a", score=0.9)],
                    FN=[
                        LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a"),
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                    ],
                    count_TP=1,
                    count_FP=1,
                    count_FN=3,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
        "multiple bboxes in an image, ignored matches": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=2,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.5,
                ),
            ),
        ],
    },
    "Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1": {
        "nothing": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[],
                    count_TP=0,
                    count_FP=0,
                    count_FN=0,
                    has_TP=False,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.1,
                ),
            ),
        ],
        "no inferences": [
            (
                TestSample(locator=fake_locator(2, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.1,
                ),
            ),
        ],
        "no ground truths": [
            (
                TestSample(locator=fake_locator(3, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[],
                    count_TP=0,
                    count_FP=1,
                    count_FN=0,
                    has_TP=False,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=1 and different labels and max confidence": [
            (
                TestSample(locator=fake_locator(4, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=0 and same labels": [
            (
                TestSample(locator=fake_locator(5, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="b", score=1.0)],
                    FN=[LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=1.0,
                    min_confidence_above_t=1.0,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(6, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(7, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.5)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=0.33 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(8, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[ScoredLabeledBoundingBox(top_left=(2.0, 2.0), bottom_right=(5.0, 5.0), label="b", score=0.99)],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=1,
                    count_FN=1,
                    has_TP=False,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0 confidence": [
            (
                TestSample(locator=fake_locator(9, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(4.0, 4.0), label="b")],
                    count_TP=0,
                    count_FP=0,
                    count_FN=1,
                    has_TP=False,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=None,
                    min_confidence_above_t=None,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.5 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.5)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.5,
                    min_confidence_above_t=0.5,
                    thresholds=0.1,
                ),
            ),
        ],
        "iou=0.5 and same labels but 0.99 confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(2.0, 1.0), bottom_right=(5.0, 4.0), label="b", score=0.99)],
                    FP=[],
                    FN=[],
                    count_TP=1,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.1,
                ),
            ),
        ],
        "multiple bboxes in an image, perfect match": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=4,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.1,
                ),
            ),
        ],
        "multiple bboxes in an image, varied iou": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.1, 1.0), bottom_right=(2.1, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.3, 3.0), bottom_right=(4.3, 4.0), label="a", score=0.9),
                    ],
                    FP=[
                        ScoredLabeledBoundingBox(top_left=(5.5, 5.0), bottom_right=(6.5, 6.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(7.7, 7.0), bottom_right=(8.7, 8.0), label="a", score=0.9),
                    ],
                    FN=[
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=2,
                    count_FN=2,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.1,
                ),
            ),
        ],
        "multiple bboxes in an image, varied confidence": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.6),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.5),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.4),
                    ],
                    FP=[],
                    FN=[LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a")],
                    count_TP=3,
                    count_FP=0,
                    count_FN=1,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.6,
                    min_confidence_above_t=0.4,
                    thresholds=0.1,
                ),
            ),
        ],
        "multiple bboxes in an image, many inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(0.0, 0.0), bottom_right=(1.0, 1.0), label="a", score=0.99),
                    ],
                    FN=[],
                    count_TP=4,
                    count_FP=4,
                    count_FN=0,
                    has_TP=True,
                    has_FP=True,
                    has_FN=False,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.1,
                ),
            ),
        ],
        "multiple bboxes in an image, too few inferences": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.99),
                        ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a", score=0.99),
                    ],
                    FP=[],
                    FN=[
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a"),
                    ],
                    count_TP=2,
                    count_FP=0,
                    count_FN=2,
                    has_TP=True,
                    has_FP=False,
                    has_FN=True,
                    max_confidence_above_t=0.99,
                    min_confidence_above_t=0.99,
                    thresholds=0.1,
                ),
            ),
        ],
        "multiple bboxes in an image, suboptimal infs": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[ScoredLabeledBoundingBox(top_left=(5.0, 5.0), bottom_right=(6.0, 6.0), label="a", score=0.9)],
                    FP=[ScoredLabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(9.0, 9.0), label="a", score=0.9)],
                    FN=[
                        LabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a"),
                        LabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a"),
                        LabeledBoundingBox(top_left=(7.0, 7.0), bottom_right=(8.0, 8.0), label="a"),
                    ],
                    count_TP=1,
                    count_FP=1,
                    count_FN=3,
                    has_TP=True,
                    has_FP=True,
                    has_FN=True,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.1,
                ),
            ),
        ],
        "multiple bboxes in an image, ignored matches": [
            (
                TestSample(locator=fake_locator(1, "OD"), metadata={}),
                TestSampleMetricsSingleClass(
                    TP=[
                        ScoredLabeledBoundingBox(top_left=(1.0, 1.0), bottom_right=(2.0, 2.0), label="a", score=0.9),
                        ScoredLabeledBoundingBox(top_left=(3.0, 3.0), bottom_right=(4.0, 4.0), label="a", score=0.9),
                    ],
                    FP=[],
                    FN=[],
                    count_TP=2,
                    count_FP=0,
                    count_FN=0,
                    has_TP=True,
                    has_FP=False,
                    has_FN=False,
                    max_confidence_above_t=0.9,
                    min_confidence_above_t=0.9,
                    thresholds=0.1,
                ),
            ),
        ],
    },
}

# evaluator_configuration -> test_name -> test_case_metrics
EXPECTED_COMPUTE_TEST_CASE_METRICS: Dict[str, Dict[str, TestCaseMetricsSingleClass]] = {
    "Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0": {
        "nothing": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=0,
            TP=0,
            FN=0,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no inferences": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no ground truths": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=1,
            TP=0,
            FN=0,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=1 and different labels and max confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4272074506720952,
        ),
        "iou=0 and same labels": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.3926325080131927,
        ),
        "iou=0.5 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.3988727670279049,
        ),
        "multiple bboxes in an image, perfect match": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=4,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.40951485170235175,
        ),
        "multiple bboxes in an image, varied iou": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=3,
            FN=1,
            FP=1,
            Precision=0.75,
            Recall=0.75,
            F1=0.75,
            AP=0.42852585300501966,
        ),
        "multiple bboxes in an image, varied confidence": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=3,
            TP=3,
            FN=1,
            FP=0,
            Precision=1.0,
            Recall=0.75,
            F1=0.8571428571428571,
            AP=0.4266850829855224,
        ),
        "multiple bboxes in an image, many inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=8,
            TP=4,
            FN=0,
            FP=4,
            Precision=0.5,
            Recall=1.0,
            F1=0.6666666666666666,
            AP=0.4372560077979224,
        ),
        "multiple bboxes in an image, too few inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=2,
            FN=2,
            FP=0,
            Precision=1.0,
            Recall=0.5,
            F1=0.6666666666666666,
            AP=0.4350202624924517,
        ),
        "multiple bboxes in an image, suboptimal infs": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=1,
            FN=3,
            FP=1,
            Precision=0.5,
            Recall=0.25,
            F1=0.3333333333333333,
            AP=0.4338218694169238,
        ),
        "multiple bboxes in an image, ignored matches": TestCaseMetricsSingleClass(
            Objects=2,
            Inferences=2,
            TP=2,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4283473934674454,
        ),
    },
    "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0": {
        "nothing": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=0,
            TP=0,
            FN=0,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no inferences": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no ground truths": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=1,
            TP=0,
            FN=0,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=1 and different labels and max confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.428837349700659,
        ),
        "iou=0 and same labels": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.39420363139782927,
        ),
        "iou=0.5 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.40041454364562773,
        ),
        "multiple bboxes in an image, perfect match": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=4,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.41544159544159553,
        ),
        "multiple bboxes in an image, varied iou": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=2,
            FN=2,
            FP=2,
            Precision=0.5,
            Recall=0.5,
            F1=0.5,
            AP=0.4260551550042678,
        ),
        "multiple bboxes in an image, varied confidence": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=2,
            FN=2,
            FP=0,
            Precision=1.0,
            Recall=0.5,
            F1=0.6666666666666666,
            AP=0.4319672208562497,
        ),
        "multiple bboxes in an image, many inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=8,
            TP=4,
            FN=0,
            FP=4,
            Precision=0.5,
            Recall=1.0,
            F1=0.6666666666666666,
            AP=0.4372395109752708,
        ),
        "multiple bboxes in an image, too few inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=2,
            FN=2,
            FP=0,
            Precision=1.0,
            Recall=0.5,
            F1=0.6666666666666666,
            AP=0.43514419419152073,
        ),
        "multiple bboxes in an image, suboptimal infs": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=1,
            FN=3,
            FP=1,
            Precision=0.5,
            Recall=0.25,
            F1=0.3333333333333333,
            AP=0.4323494849484949,
        ),
        "multiple bboxes in an image, ignored matches": TestCaseMetricsSingleClass(
            Objects=2,
            Inferences=2,
            TP=2,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4305562166793603,
        ),
    },
    "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3": {
        "nothing": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=0,
            TP=0,
            FN=0,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no inferences": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no ground truths": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=1,
            TP=0,
            FN=0,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=1 and different labels and max confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4304586363835988,
        ),
        "iou=0 and same labels": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.39576735369733695,
        ),
        "iou=0.5 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4019490903434968,
        ),
        "multiple bboxes in an image, perfect match": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=4,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.421259302741262,
        ),
        "multiple bboxes in an image, varied iou": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=2,
            FN=2,
            FP=2,
            Precision=0.5,
            Recall=0.5,
            F1=0.5,
            AP=0.4236510358293639,
        ),
        "multiple bboxes in an image, varied confidence": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=2,
            FN=2,
            FP=0,
            Precision=1.0,
            Recall=0.5,
            F1=0.6666666666666666,
            AP=0.4346588861647574,
        ),
        "multiple bboxes in an image, many inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=8,
            TP=4,
            FN=0,
            FP=4,
            Precision=0.5,
            Recall=1.0,
            F1=0.6666666666666666,
            AP=0.43484288861584863,
        ),
        "multiple bboxes in an image, too few inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=2,
            FN=2,
            FP=0,
            Precision=1.0,
            Recall=0.5,
            F1=0.6666666666666666,
            AP=0.4352714273271663,
        ),
        "multiple bboxes in an image, suboptimal infs": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=1,
            FN=3,
            FP=1,
            Precision=0.5,
            Recall=0.25,
            F1=0.3333333333333333,
            AP=0.4292048477682531,
        ),
        "multiple bboxes in an image, ignored matches": TestCaseMetricsSingleClass(
            Objects=2,
            Inferences=2,
            TP=2,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.43277279300115,
        ),
    },
    "Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1": {
        "nothing": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=0,
            TP=0,
            FN=0,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no inferences": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "no ground truths": TestCaseMetricsSingleClass(
            Objects=0,
            Inferences=1,
            TP=0,
            FN=0,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=1 and different labels and max confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.43207137021475245,
        ),
        "iou=0 and same labels": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.33 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=0,
            FN=1,
            FP=1,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=0,
            TP=0,
            FN=1,
            FP=0,
            Precision=0.0,
            Recall=0.0,
            F1=0.0,
            AP=0.0,
        ),
        "iou=0.5 and same labels but 0.5 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.39732371794871796,
        ),
        "iou=0.5 and same labels but 0.99 confidence": TestCaseMetricsSingleClass(
            Objects=1,
            Inferences=1,
            TP=1,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4034764494902942,
        ),
        "multiple bboxes in an image, perfect match": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=4,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4269705445157252,
        ),
        "multiple bboxes in an image, varied iou": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=4,
            TP=2,
            FN=2,
            FP=2,
            Precision=0.5,
            Recall=0.5,
            F1=0.5,
            AP=0.4213108668328975,
        ),
        "multiple bboxes in an image, varied confidence": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=3,
            TP=3,
            FN=1,
            FP=0,
            Precision=1.0,
            Recall=0.75,
            F1=0.8571428571428571,
            AP=0.437306206050923,
        ),
        "multiple bboxes in an image, many inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=8,
            TP=4,
            FN=0,
            FP=4,
            Precision=0.5,
            Recall=1.0,
            F1=0.6666666666666666,
            AP=0.43489983541808785,
        ),
        "multiple bboxes in an image, too few inferences": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=2,
            FN=2,
            FP=0,
            Precision=1.0,
            Recall=0.5,
            F1=0.6666666666666666,
            AP=0.43540176895791877,
        ),
        "multiple bboxes in an image, suboptimal infs": TestCaseMetricsSingleClass(
            Objects=4,
            Inferences=2,
            TP=1,
            FN=3,
            FP=1,
            Precision=0.5,
            Recall=0.25,
            F1=0.3333333333333333,
            AP=0.426122572815534,
        ),
        "multiple bboxes in an image, ignored matches": TestCaseMetricsSingleClass(
            Objects=2,
            Inferences=2,
            TP=2,
            FN=0,
            FP=0,
            Precision=1.0,
            Recall=1.0,
            F1=1.0,
            AP=0.4349967679379444,
        ),
    },
}


@pytest.mark.parametrize(
    "config_name, test_name",
    [
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "nothing"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "nothing"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "nothing"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "nothing"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "no inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "no inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "no inferences"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "no inferences"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "no ground truths"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "no ground truths"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "no ground truths"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "no ground truths"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=1 and different labels and max confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=1 and different labels and max confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=1 and different labels and max confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=1 and different labels and max confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0 and same labels"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0 and same labels"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0 and same labels"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0 and same labels"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, perfect match"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, perfect match"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, perfect match"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, perfect match"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, varied iou"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, varied iou"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, varied iou"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, varied iou"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, varied confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, varied confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, varied confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, varied confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, many inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, many inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, many inferences"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, many inferences"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, too few inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, too few inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, too few inferences"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, too few inferences"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, ignored matches"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, ignored matches"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, ignored matches"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, ignored matches"),
    ],
)
def test__prebuilt__object__detection__single__class__compute__test__sample__metrics(
    config_name: str,
    test_name: str,
) -> None:
    config = TEST_CONFIGURATIONS[config_name]
    eval = ObjectDetectionEvaluator(configurations=[config])

    assert EXPECTED_COMPUTE_TEST_SAMPLE_METRICS[config_name][test_name] == eval.compute_test_sample_metrics(
        test_case=TEST_CASE,
        inferences=TEST_DATA[test_name],
        configuration=config,
    )


def test__prebuilt__object__detection__single__class__compute__test__sample__metrics__all() -> None:
    for config_name, config in TEST_CONFIGURATIONS.items():
        if config_name not in EXPECTED_COMPUTE_TEST_SAMPLE_METRICS:
            continue
        eval = ObjectDetectionEvaluator(configurations=[config])
        assert [
            pair for _, results in EXPECTED_COMPUTE_TEST_SAMPLE_METRICS[config_name].items() for pair in results
        ] == eval.compute_test_sample_metrics(
            test_case=TEST_CASE,
            inferences=[ts_gt_inf for _, data in TEST_DATA.items() for ts_gt_inf in data],
            configuration=config,
        )


@pytest.mark.metrics
@pytest.mark.parametrize(
    "config_name, test_name",
    [
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "nothing"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "nothing"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "nothing"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "nothing"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "no inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "no inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "no inferences"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "no inferences"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "no ground truths"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "no ground truths"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "no ground truths"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "no ground truths"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=1 and different labels and max confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=1 and different labels and max confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=1 and different labels and max confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=1 and different labels and max confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0 and same labels"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0 and same labels"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0 and same labels"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0 and same labels"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.33 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.33 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.33 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.5 and same labels but 0 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.5 and same labels but 0.5 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "iou=0.5 and same labels but 0.99 confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, perfect match"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, perfect match"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, perfect match"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, perfect match"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, varied iou"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, varied iou"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, varied iou"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, varied iou"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, varied confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, varied confidence"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, varied confidence"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, varied confidence"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, many inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, many inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, many inferences"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, many inferences"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, too few inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, too few inferences"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, too few inferences"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, too few inferences"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, suboptimal infs"),
        ("Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0", "multiple bboxes in an image, ignored matches"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0", "multiple bboxes in an image, ignored matches"),
        ("Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3", "multiple bboxes in an image, ignored matches"),
        ("Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1", "multiple bboxes in an image, ignored matches"),
    ],
)
def test__prebuilt__object__detection__single__class__compute__test__case__metrics(
    config_name: str,
    test_name: str,
) -> None:
    config = TEST_CONFIGURATIONS[config_name]

    eval = ObjectDetectionEvaluator(configurations=[config])
    eval.compute_test_sample_metrics(
        test_case=TEST_CASE,
        inferences=TEST_DATA[test_name],
        configuration=config,
    )

    result = eval.compute_test_case_metrics(
        test_case=TEST_CASE,
        inferences=TEST_DATA[test_name],
        metrics=[metric for _, metric in EXPECTED_COMPUTE_TEST_SAMPLE_METRICS[config_name][test_name]],
        configuration=config,
    )

    expected = EXPECTED_COMPUTE_TEST_CASE_METRICS[config_name][test_name]
    assert expected == result


@pytest.mark.metrics
@pytest.mark.parametrize(
    "config_name, expected",
    [
        (
            "Threshold: Fixed(0.3), IoU: 0.3, confidence ≥ 0.0",
            TestCaseMetricsSingleClass(
                Objects=35,
                Inferences=32,
                TP=22,
                FN=13,
                FP=10,
                Precision=0.6875,
                Recall=0.6285714285714286,
                F1=0.6567164179104478,
                AP=0.43971982269854615,
            ),
        ),
        (
            "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0",
            TestCaseMetricsSingleClass(
                Objects=35,
                Inferences=31,
                TP=20,
                FN=15,
                FP=11,
                Precision=0.6451612903225806,
                Recall=0.5714285714285714,
                F1=0.606060606060606,
                AP=0.4409702828499821,
            ),
        ),
        (
            "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.3",
            TestCaseMetricsSingleClass(
                Objects=35,
                Inferences=31,
                TP=20,
                FN=15,
                FP=11,
                Precision=0.6451612903225806,
                Recall=0.5714285714285714,
                F1=0.606060606060606,
                AP=0.437782006245121,
            ),
        ),
        (
            "Threshold: F1-Optimal, IoU: 0.5, confidence ≥ 0.1",
            TestCaseMetricsSingleClass(
                Objects=35,
                Inferences=32,
                TP=21,
                FN=14,
                FP=11,
                Precision=0.65625,
                Recall=0.6,
                F1=0.626865671641791,
                AP=0.4349967679379444,
            ),
        ),
    ],
)
def test__prebuilt__object__detection__single__class__compute__test__case__metrics__all(
    config_name: str,
    expected: TestCaseMetricsSingleClass,
) -> None:
    config = TEST_CONFIGURATIONS[config_name]
    eval = ObjectDetectionEvaluator(configurations=[config])
    eval.compute_test_sample_metrics(
        test_case=TEST_CASE,
        inferences=[ts_gt_inf for _, data in TEST_DATA.items() for ts_gt_inf in data],
        configuration=config,
    )
    result = eval.compute_test_case_metrics(
        test_case=TEST_CASE,
        inferences=[ts_gt_inf for _, data in TEST_DATA.items() for ts_gt_inf in data],
        metrics=[
            metric for _, metrics in EXPECTED_COMPUTE_TEST_SAMPLE_METRICS[config_name].items() for _, metric in metrics
        ],
        configuration=config,
    )

    assert expected == result


@pytest.mark.metrics
@pytest.mark.parametrize(
    "config_name, test_name, test_case_metrics, expected",
    [
        (
            "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0",
            "nothing",
            [
                (
                    TEST_CASE_2,
                    TestCaseMetricsSingleClass(
                        Objects=0,
                        Inferences=0,
                        TP=0,
                        FN=0,
                        FP=0,
                        Precision=0,
                        Recall=0,
                        F1=0,
                        AP=0,
                    ),
                ),
            ],
            TestSuiteMetrics(
                n_images=1,
                mean_AP=0,
            ),
        ),
        (
            "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0",
            "multiple bboxes in an image, perfect match",
            [
                (
                    TEST_CASE,
                    TestCaseMetricsSingleClass(
                        Objects=0,
                        Inferences=0,
                        TP=0,
                        FN=0,
                        FP=0,
                        Precision=0,
                        Recall=0,
                        F1=0,
                        AP=0.123,
                    ),
                ),
            ],
            TestSuiteMetrics(
                n_images=1,
                mean_AP=0.123,
            ),
        ),
    ],
)
def test__prebuilt__object__detection__single__class__compute__test__suite__metrics(
    config_name: str,
    test_name: str,
    test_case_metrics: List[Tuple[TestCase, TestCaseMetricsSingleClass]],
    expected: TestSuiteMetrics,
) -> None:
    config = TEST_CONFIGURATIONS[config_name]
    eval = ObjectDetectionEvaluator(configurations=[config])
    eval.compute_test_sample_metrics(
        test_case=test_case_metrics[0][0],
        inferences=TEST_DATA[test_name],
        configuration=config,
    )

    eval.compute_test_case_metrics(
        test_case=test_case_metrics[0][0],
        inferences=TEST_DATA[test_name],
        metrics=[metric for _, metric in EXPECTED_COMPUTE_TEST_SAMPLE_METRICS[config_name][test_name]],
        configuration=config,
    )

    result = eval.compute_test_suite_metrics(
        test_suite=TEST_SUITE,
        metrics=test_case_metrics,
        configuration=config,
    )

    assert expected == result


@pytest.mark.metrics
def test__prebuilt__object__detection__single__class__compute__multiple__test__suite__metric() -> None:
    config_name = "Threshold: Fixed(0.5), IoU: 0.5, confidence ≥ 0.0"
    test_1 = "multiple bboxes in an image, perfect match"
    test_2 = "nothing"
    config = TEST_CONFIGURATIONS[config_name]
    eval = ObjectDetectionEvaluator(configurations=[config])
    eval.compute_test_sample_metrics(
        test_case=TEST_CASE,
        inferences=TEST_DATA[test_1],
        configuration=config,
    )

    eval.compute_test_case_metrics(
        test_case=TEST_CASE,
        inferences=TEST_DATA[test_1],
        metrics=[EXPECTED_COMPUTE_TEST_SAMPLE_METRICS[config_name][test_1][0][1]],
        configuration=config,
    )

    eval.compute_test_sample_metrics(
        test_case=TEST_CASE_2,
        inferences=TEST_DATA[test_2],
        configuration=config,
    )

    eval.compute_test_case_metrics(
        test_case=TEST_CASE_2,
        inferences=TEST_DATA[test_2],
        metrics=[EXPECTED_COMPUTE_TEST_SAMPLE_METRICS[config_name][test_2][0][1]],
        configuration=config,
    )

    test_case_metrics = [
        (
            TEST_CASE,
            TestCaseMetricsSingleClass(
                Objects=0,
                Inferences=0,
                TP=0,
                FN=0,
                FP=0,
                Precision=0.0,
                Recall=0.0,
                F1=0.0,
                AP=0.3,
            ),
        ),
        (
            TEST_CASE_2,
            TestCaseMetricsSingleClass(
                Objects=0,
                Inferences=0,
                TP=0,
                FN=0,
                FP=0,
                Precision=0.0,
                Recall=0.0,
                F1=0.0,
                AP=0.7,
            ),
        ),
    ]

    result = eval.compute_test_suite_metrics(
        test_suite=TEST_SUITE,
        metrics=test_case_metrics,
        configuration=config,
    )

    assert result == TestSuiteMetrics(
        n_images=2,
        mean_AP=0.5,
    )
