version: 2.1

parameters:
  workflow:
    type: boolean
    default: false
  dataset:
    type: boolean
    default: false
  misc:
    type: boolean
    default: false
  all:
    type: boolean
    default: false
  example:
    type: boolean
    default: false
  token_count:
    type: integer
    default: 8

executors:
  python:
    parameters:
      python-version:
        type: string
        default: "3.9.18"
      resource-class:
        type: string
        default: "small"
    docker:
      - image: cimg/python:<< parameters.python-version >>
    resource_class: << parameters.resource-class >>

jobs:
  ci-base:
    parameters:
      python-version:
        type: string
        default: "3.9.18"
      extras:
        type: string
        default: "none"
    executor:
      name: python
      python-version: << parameters.python-version >>
    steps:
      - checkout
      - run: &install_uv
          name: Install uv
          command: pip install uv
      - run: &setup_env
          name: Setup environment
          command: uv venv --python << parameters.python-version >> && source .venv/bin/activate

  docs:
    parameters:
      python-version:
        type: string
        default: "3.9.18"
      extras:
        type: string
        default: "none"
    executor:
      name: python
      python-version: << parameters.python-version >>
    steps:
      - checkout
      - run: *install_uv
      - run: *setup_env
      - restore_cache:
          name: Restore MkDocs cache
          key: mkdocs-cache
      - run:
          name: Install system-level dependencies
          command: |
            sudo apt-get update
            sudo apt-get install -y libcairo2-dev git-lfs
      - run:
          name: Checkout LFS blobs
          command: git lfs pull
      - run:
          name: Build documentation without 'insiders' packages
          command: uv run mkdocs build
      - add_ssh_keys: # add github.com SSH key fingerprint to access private repo forks
          fingerprints: [ "15:8b:d4:ac:1f:cd:2f:d3:92:a7:4c:aa:46:81:0b:7d" ]
      - run:
          name: Build documentation with 'insiders' packages
          command: |
            ./docs/setup_insiders.sh
            uv run mkdocs build --config-file mkdocs.insiders.yml
      - save_cache:
          key: mkdocs-cache
          paths:
            - .cache

  unit-test:
    parameters:
      python-version:
        type: string
        default: "3.9.18"
      extras:
        type: string
        default: "none"
    executor:
      name: python
      python-version: << parameters.python-version >>
    steps:
      - checkout
      - run: *install_uv
      - run: *setup_env
      - when: &install_without_extras
          condition:
            equal: [ "none", << parameters.extras >> ]
          steps:
            - run:
                name: Install Dependencies
                command: uv pip install -r pyproject.toml --verbose
                env:
                  UV_EXTRA_INDEX_URL: "https://download.pytorch.org/whl/cpu/"
      - when: &install_with_extras
          condition:
            equal: [ "metrics", << parameters.extras >> ]
          steps:
            - run:
                name: Install Dependencies
                command: uv pip install --all-extras -r pyproject.toml --verbose
                env:
                  UV_EXTRA_INDEX_URL: "https://download.pytorch.org/whl/cpu/"
      - run: uv run python3 -c 'import kolena'
      # TODO: fix underlying mypy issues with Python>3.9 rather than skipping
      - when:
          condition:
            not:
              matches:
                pattern: "^3\\.(10|11|12).*$"
                value: << parameters.python-version >>
          steps:
            - run: uvx pre-commit run -a
      - when:
          condition:
            equal: [ "none", << parameters.extras >> ]
          steps:
            - run:
                name: Run unit tests
                command: uv run pytest --cache-clear -m 'not metrics' -vv --cov=kolena --cov-branch tests/unit
      - when:
          condition:
            equal: [ "metrics", << parameters.extras >> ]
          steps:
            - run:
                name: Run metrics unit tests
                command: uv run pytest --cache-clear -m 'metrics' -vv --cov=kolena --cov-branch tests/unit

  integration-test:
    parameters:
      python-version:
        type: string
        default: "3.9"
      pytest-group:
        type: string
        default: workflow
      enabled:
        type: boolean
        default: false
      parallelism:
        type: integer
        default: 1
      extras:
        type: string
        default: "none"
    executor:
      name: python
      python-version: << parameters.python-version >>
    parallelism: << parameters.parallelism >>
    steps:
      - when:
          condition:
            not: << parameters.enabled >>
          steps:
            - run:
                name: skip job
                command: circleci-agent step halt
      - checkout
      - run: *install_uv
      - run: *setup_env
      - when: *install_without_extras
      - when: *install_with_extras
      - run:
          name: Set KOLENA_TOKEN with round robin
          command: |
            token=KOLENA_TOKEN_$((CIRCLE_BUILD_NUM % << pipeline.parameters.token_count >>))
            echo "Using $token"
            export KOLENA_TOKEN=${!token}
      - when:
          condition:
            and:
              - equal: [ "none", << parameters.extras >> ]
              - not_equal: [ "_experimental", << parameters.pytest-group >> ]
          steps:
            - run:
                name: Run << parameters.pytest-group >> integration tests
                no_output_timeout: 30m
                command: |
                  TEST_GROUP="<< parameters.pytest-group >>"
                  if [ "$TEST_GROUP" = "misc" ]; then
                    TESTFILES=$(circleci tests glob tests/integration/test_*.py |
                      circleci tests split --split-by=timings --timings-type=filename)
                  else
                    TESTFILES=$(find tests/integration/$TEST_GROUP -name "test_*.py" |
                      circleci tests split --split-by=timings --timings-type=filename)
                  fi
                  uv run pytest --cache-clear -vv --durations=10 --cov=kolena --cov-branch --ignore=examples \
                    -o junit_family=legacy --junitxml=test-results/result.xml $TESTFILES
      - when:
          condition:
            equal: [ "metrics", << parameters.extras >> ]
          steps:
            - run:
                name: Run experimental metrics integration tests
                command: uv run pytest --cache-clear -m 'metrics' -vv --cov=kolena --cov-branch tests/integration/

  integration-test-dataset:
    parameters:
      python-version:
        type: string
        default: "3.9"
      pytest-group:
        type: string
        default: dataset
      enabled:
        type: boolean
        default: false
      parallelism:
        type: integer
        default: 1
      extras:
        type: string
        default: "none"
    executor:
      name: python
      python-version: << parameters.python-version >>
    parallelism: << parameters.parallelism >>
    steps:
      - when:
          condition:
            or:
              - not: << parameters.enabled >>
              - and:
                - not: << pipeline.parameters.dataset >>
                - not: << pipeline.parameters.all >>
          steps:
            - run:
                name: skip job
                command: circleci-agent step halt
      - checkout
      - run: *install_uv
      - run: *setup_env
      - when: *install_without_extras
      - when: *install_with_extras
      - run:
          name: Set KOLENA_TOKEN with round robin
          command: |
            token=KOLENA_TOKEN_$((CIRCLE_BUILD_NUM % << pipeline.parameters.token_count >>))
            echo "Using $token"
            export KOLENA_TOKEN=${!token}
      - run:
          name: Run dataset integration tests
          command: |
            TESTFILES=$(circleci tests glob tests/integration/dataset/**/test_*.py |
                circleci tests split --split-by=timings --timings-type=filename)
            uv run pytest --cache-clear -vv --durations=10 --cov=kolena --cov-branch --ignore=examples \
              -o junit_family=legacy --junitxml=test-results/result.xml $TESTFILES

  example-test-dataset:
    parameters:
      python-version:
        type: string
        default: "3.9.18"
      subproject:
        type: string
      resource-class:
        type: string
    executor:
      name: python
      python-version: << parameters.python-version >>
      resource-class: << parameters.resource-class >>
    working_directory: ~/project/examples/dataset/<< parameters.subproject >>
    steps:
      - checkout:
          path: ~/project
      - run: *install_uv
      - run: *setup_env
      - run: uv pip install -r pyproject.toml --verbose
      - run:
          name: Run pre-commit checks
          command: uvx pre-commit run -a
      - run:
          name: Run << parameters.subproject >> (Python << parameters.python-version >>) integration tests
          command: |
            token=KOLENA_TOKEN_$((CIRCLE_BUILD_NUM % << pipeline.parameters.token_count >>))
            echo "Using $token"
            export KOLENA_TOKEN=${!token}
            uv run pytest -vv tests

  example-test-workflow:
    parameters:
      python-version:
        type: string
        default: "3.9.18"
      subproject:
        type: string
      resource-class:
        type: string
    executor:
      name: python
      python-version: << parameters.python-version >>
      resource-class: << parameters.resource-class >>
    working_directory: ~/project/examples/workflow/<< parameters.subproject >>
    steps:
      - checkout:
          path: ~/project
      - run: |
          poetry config installer.max-workers 10
          poetry install -vv --no-ansi
      - run:
          name: Run pre-commit checks
          command: poetry run pre-commit run -a
      - run:
          name: Run << parameters.subproject >> (Python << parameters.python-version >>) integration tests
          command: |
            token=KOLENA_TOKEN_$((CIRCLE_BUILD_NUM % << pipeline.parameters.token_count >>))
            echo "Using $token"
            export KOLENA_TOKEN=${!token}
            poetry run pytest --cache-clear -vv tests

  example-evaluator:
    docker:
      - image: cimg/base:2022.06
    steps:
      - checkout:
          path: ~/project
      - setup_remote_docker:
          version: default
      - run:
          name: Build evaluator
          command: examples/workflow/text_summarization/docker/build.sh

workflows:
  ci:
    jobs:
      - ci-base:
          name: ci-base-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.8.18", "3.9.18", "3.10.13", "3.11.6", "3.12.1" ]
              extras: [ "none", "metrics" ]
      - unit-test:
          name: unit-test-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.8.18", "3.9.18", "3.10.13", "3.11.6", "3.12.1" ]
              extras: [ "none", "metrics" ]
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
      - docs:
          name: docs-<< matrix.python-version >>
          matrix:
            parameters:
              python-version: [ "3.9.18" ]
              extras: [ "none" ]
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
      - integration-test:
          name: integration-test-workflow-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.9.18" ]
              extras: [ "none" ]
          pytest-group: workflow
          enabled: true
          parallelism: 3
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
      - integration-test:
          name: integration-test-experimental-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.9.18" ]
              extras: [ "metrics" ]
          pytest-group: _experimental
          enabled: true
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
      - integration-test:
          name: integration-test-experimental-embedding-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.9.18" ]
              extras: [ "none" ]
          pytest-group: _experimental/search
          enabled: true
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
      - integration-test:
          name: integration-test-experimental-dataset-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.9.18" ]
              extras: [ "none" ]
          pytest-group: _experimental/dataset
          enabled: true
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
      - integration-test-dataset:
          name: integration-test-dataset-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.9.18" ]
              extras: [ "none" ]
          pytest-group: dataset
          enabled: true
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
      - integration-test:
          name: integration-test-misc-<< matrix.python-version >>-<< matrix.extras >>
          matrix:
            parameters:
              python-version: [ "3.9.18" ]
              extras: [ "none" ]
          pytest-group: misc
          enabled: << pipeline.parameters.all >>
          parallelism: 1
          requires:
            - ci-base-<< matrix.python-version >>-<< matrix.extras >>
  example:
    when:
      or: [ << pipeline.parameters.workflow >>, << pipeline.parameters.all >>, << pipeline.parameters.example >> ]
    jobs:
      - example-test-dataset:
          matrix:
            parameters:
              subproject: [ text_summarization, image_retrieval_by_text ]
              resource-class: [ large ]
              python-version: [ "3.9.18" ]
      - example-test-dataset:
          context:
            - aws
          matrix:
            parameters:
              subproject: [ age_estimation, automatic_speech_recognition, classification, keypoint_detection, question_answering, rain_forecast, semantic_segmentation, speaker_diarization, object_detection_2d, semantic_textual_similarity, person_detection, crossing_pedestrian_detection, named_entity_recognition ]
              resource-class: [ small ]
              python-version: [ "3.9.18" ]
      - example-test-workflow:
          matrix:
            parameters:
              subproject: [ text_summarization ]
              resource-class: [ large ]
              python-version: [ "3.9.18" ]
      - example-test-workflow:
          context:
            - aws
          matrix:
            parameters:
              subproject: [ age_estimation, automatic_speech_recognition, classification, face_recognition_11, keypoint_detection, object_detection_3d, question_answering, speaker_diarization ]
              resource-class: [ small ]
              python-version: [ "3.9.18" ]
  example-evaluator:
    when:
      condition:
        equal: [ "trunk", << pipeline.git.branch >> ]
    jobs:
      - example-evaluator
