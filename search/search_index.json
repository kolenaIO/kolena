{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Developer Guide","text":"<p>Kolena is a comprehensive machine learning testing and debugging platform to surface hidden model behaviors and take the mystery out of model development. Kolena helps you:</p> <ul> <li>Perform high-resolution model evaluation</li> <li>Understand and track behavioral improvements and regressions</li> <li>Meaningfully communicate model capabilities</li> <li>Automate model testing and deployment workflows</li> </ul> <p>Kolena organizes your test data, stores and visualizes your model evaluations, and provides tooling to craft better tests. You interface with it through the web at app.kolena.io and programmatically via the <code>kolena</code> Python client.</p>"},{"location":"#why-kolena","title":"Why Kolena?","text":"<p>TL;DR</p> <p>Kolena helps you test your ML models more effectively.</p> <p>Jump right in with the   Quickstart guide.</p> <p>Current ML evaluation techniques are falling short. Engineers run inference on arbitrarily split benchmark datasets, spend weeks staring at error graphs to evaluate their models, and ultimately produce a global metric that fails to capture the true behavior of the model.</p> <p>Models exhibit highly variable performance across different subsets of a domain. A global metric gives you a high-level picture of performance but doesn't tell you what you really want to know: what sort of behavior can I expect from my model in production?</p> <p>To answer this question you need a higher-resolution picture of model performance. Not \"how well does my model perform on class X,\" but \"in what scenarios does my model perform well for class X?\"</p> <p> </p> <p>In the above example, looking only at global metric (e.g. F1 score), we'd almost certainly choose to deploy Model B.</p> <p>But what if the \"High Blur\" scenario isn't important for our product? Most of Model A's failures are from that scenario, and it outperforms Model B in more important scenarios like \"Front View.\" Meanwhile, Model B's underperformance in \"Front View,\" a highly important scenario, is masked by improved performance in the unimportant \"High Blur\" scenario.</p> <p>Test data is more important than training data!</p> <p>Everything you know about a new model's behavior is learned from your tests.</p> <p>Fine-grained tests teach you what you need to learn before a model hits production.</p> <p>Now... why Kolena? Two reasons:</p> <ol> <li>Managing fine-grained tests is a tedious data engineering task, especially under changing data circumstances as    your dataset grows and your understanding of your domain develops</li> <li>Creating fine-grained tests is labor-intensive and typically involves manual annotation of countless images, a    costly and time-consuming process</li> </ol> <p>We built Kolena to solve these two problems.</p>"},{"location":"#read-more","title":"Read More","text":"<ul> <li>Best Practices for ML Model Testing (Kolena Blog)</li> <li>Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging (arXiv:1909.12475)</li> <li>No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems (arXiv:2011.12945)</li> </ul>"},{"location":"#developer-guide","title":"Developer Guide","text":"<p>Learn how to use Kolena to test your models effectively:</p> <ul> <li> <p>  Quickstart</p> <p>Run through an example using Kolena to set up rigorous and repeatable model testing in minutes.</p> </li> <li> <p>  Installing <code>kolena</code></p> <p>Install and initialize the <code>kolena</code> Python package, the programmatic interface to Kolena.</p> </li> <li> <p>  Building a Workflow</p> <p>Learn how to use <code>kolena.workflow</code> to test any arbitrary ML problem on Kolena.</p> </li> <li> <p>  Core Concepts</p> <p>Core concepts for testing in Kolena.</p> </li> <li> <p>  Advanced Usage</p> <p>Tutorial documentation for advanced features available in Kolena.</p> </li> <li> <p>  API Reference</p> <p>Developer-focused detailed API reference documentation for <code>kolena</code>.</p> </li> </ul>"},{"location":"building-a-workflow/","title":"Building a Workflow","text":"<p>In this tutorial we'll learn how to use the <code>kolena.workflow</code> workflow builder definitions to test a Keypoint Detection model on the 300-W facial keypoint dataset. This demonstration will show us how we can build a workflow to test any arbitrary ML problem on Kolena.</p>","boost":2},{"location":"building-a-workflow/#getting-started","title":"Getting Started","text":"<p>With the <code>kolena</code> Python client installed, first let's initialize a client session:</p> <pre><code>import kolena\n\nkolena.initialize(verbose=True)\n</code></pre> <p>The data used in this tutorial is publicly available in the <code>kolena-public-datasets</code> S3 bucket in a <code>metadata.csv</code> file:</p> <pre><code>import pandas as pd\n\nDATASET = \"300-W\"\nBUCKET = \"s3://kolena-public-datasets\"\n\ndf = pd.read_csv(f\"{BUCKET}/{DATASET}/meta/metadata.csv\")\n</code></pre> <p>Note: <code>s3fs</code> dependency</p> <p>To load CSVs directly from S3, make sure to install the <code>s3fs</code> Python module: <code>pip3 install s3fs[boto3]</code> and set up AWS credentials.</p> <p>This <code>metadata.csv</code> file describes a keypoint detection dataset with the following columns:</p> <p>Note: Five-point facial keypoints array</p> <p>For brevity, the 300-W dataset has been pared down to only 5 keypoints: outermost corner of each eye, bottom of nose, and corners of the mouth.</p> <p> Example image and five-point facial keypoints array from 300-W. </p> <ul> <li><code>locator</code>: location of the image in S3</li> <li><code>normalization_factor</code>: normalization factor of the image. This is used to normalize the error by providing a     factor for each image. Common techniques for computation include the Euclidean distance between two points or the     diagonal measurement of the image.</li> <li><code>points</code>: stringified list of coordinates corresponding to the <code>(x, y)</code> coordinates of the keypoint ground truths</li> </ul> <p>Each <code>locator</code> is present exactly one time and contains the keypoint ground truth for that image. In this tutorial, we're implementing our workflow with support for only a single keypoint instance per image, but we could easily adapt our ground truth, inference, and metrics types to accommodate a variable number of keypoint arrays per image.</p>","boost":2},{"location":"building-a-workflow/#step-1-defining-data-types","title":"Step 1: Defining Data Types","text":"<p>When building your own workflow you have control over the <code>TestSample</code> (e.g. image), <code>GroundTruth</code> (e.g. 5-element facial keypoint array), and <code>Inference</code> types used in your project.</p>","boost":2},{"location":"building-a-workflow/#test-sample-type","title":"Test Sample Type","text":"<p>For the purposes of this tutorial, let's assume our model takes a single image as input along with an optional bounding box around the face in question, produced by an upstream model in our pipeline. We can import and extend the <code>kolena.workflow.Image</code> test sample type for this purpose:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\n\nfrom kolena.workflow import Image\nfrom kolena.workflow.annotation import BoundingBox\n\n@dataclass(frozen=True)\nclass TestSample(Image):\n    bbox: Optional[BoundingBox] = None\n</code></pre>","boost":2},{"location":"building-a-workflow/#ground-truth-type","title":"Ground Truth Type","text":"<p>Next, let's define our <code>GroundTruth</code> type, typically containing the manually-annotated information necessary to evaluate model inferences:</p> <pre><code>from kolena.workflow import GroundTruth as GT\nfrom kolena.workflow.annotation import Keypoints\n\n@dataclass(frozen=True)\nclass GroundTruth(GT):\n    keypoints: Keypoints\n\n    # In order to compute normalized error, some normalization factor describing\n    # the size of the face in the image is required.\n    normalization_factor: float\n</code></pre>","boost":2},{"location":"building-a-workflow/#inference-type","title":"Inference Type","text":"<p>Lastly, we'll define our <code>Inference</code> type, containing model outputs that are evaluated against a ground truth. Note that our model produces not only a <code>Keypoints</code> array, but also an associated <code>confidence</code> value that we may use to ignore low-confidence predictions:</p> <pre><code>from kolena.workflow import Inference as Inf\n\n@dataclass(frozen=True)\nclass Inference(Inf):\n    keypoints: Keypoints\n    confidence: float\n</code></pre> <p>With our test sample, ground truth, and inference defined, we can now use <code>define_workflow</code> to declare our workflow:</p> <pre><code>from kolena.workflow import define_workflow\n\n# use these TestCase, TestSuite, and Model definitions to create and run tests\n_, TestCase, TestSuite, Model = define_workflow(\n    \"Keypoint Detection\", TestSample, GroundTruth, Inference\n)\n</code></pre>","boost":2},{"location":"building-a-workflow/#step-2-defining-metrics","title":"Step 2: Defining Metrics","text":"<p>With our core data types defined, the next step is to lay out our evaluation criteria: our metrics.</p>","boost":2},{"location":"building-a-workflow/#test-sample-metrics","title":"Test Sample Metrics","text":"<p>Test Sample Metrics (<code>MetricsTestSample</code>) are metrics computed from a single test sample and its associated ground truths and inferences.</p> <p>For the keypoint detection workflow, an example metric may be normalized mean error (NME), the normalized distance between the ground truth and inference keypoints.</p> <pre><code>from kolena.workflow import MetricsTestSample\n\n@dataclass(frozen=True)\nclass TestSampleMetrics(MetricsTestSample):\n    normalized_mean_error: float\n\n    # If the normalized mean error is above some configured threshold, this test\n    # sample is considered an \"alignment failure\".\n    alignment_failure: bool\n</code></pre>","boost":2},{"location":"building-a-workflow/#test-case-metrics","title":"Test Case Metrics","text":"<p>Test case metrics (<code>MetricsTestCase</code>) are aggregate metrics computed across a population. All of your standard evaluation metrics should go here \u2014 things like accuracy, precision, recall, or any other aggregate metrics that apply to your problem.</p> <p>For keypoint detection, we care about the mean NME and alignment failure rate across the different test samples in a test case:</p> <pre><code>from kolena.workflow import MetricsTestCase\n\n@dataclass(frozen=True)\nclass TestCaseMetrics(MetricsTestCase):\n    mean_nme: float\n    alignment_failure_rate: float\n</code></pre> <p>Tip: Plots</p> <p>Evaluators can also compute test-case-level plots using the <code>Plot</code> API. These plots are visualized on the   Results dashboard alongside the metrics reported for each test case.</p> <p>Tip: Test Suite Metrics</p> <p>Metrics can also be computed per test suite by extending <code>MetricsTestSuite</code>.</p> <p>Test suite metrics typically measure variance in performance across different test cases, being used e.g. to measure fairness across demographics for a test suite with test cases stratifying by demographic.</p>","boost":2},{"location":"building-a-workflow/#step-3-creating-tests","title":"Step 3: Creating Tests","text":"<p>With our data already in an S3 bucket and metadata loaded into memory, we can start creating test cases!</p> <p>Let's create a simple test case containing the entire dataset:</p> <pre><code>import json\n\ntest_samples = [TestSample(locator) for locator in df[\"locator\"]]\nground_truths = [\n    GroundTruth(\n        keypoints=Keypoints(points=json.loads(record.points)),\n        normalization_factor=record.normalization_factor,\n    )\n    for record in df.itertuples()\n]\nts_with_gt = list(zip(test_samples, ground_truths))\ntest_case = TestCase(f\"{DATASET} :: basic\", test_samples=ts_with_gt)\n</code></pre> <p>Note: Creating test cases</p> <p>In this tutorial we created only a single simple test case, but more advanced test cases can be generated in a variety of fast and scalable ways, either programmatically with the <code>kolena</code> Python client or visually in the   Studio.</p> <p>Now that we have a basic test case for our entire dataset let's create a test suite for it:</p> <pre><code>test_suite = TestSuite(f\"{DATASET} :: basic\", test_cases=[test_case])\n</code></pre>","boost":2},{"location":"building-a-workflow/#step-4-running-tests","title":"Step 4: Running Tests","text":"<p>With basic tests defined for the 300-W dataset, we're almost ready to start testing our models.</p>","boost":2},{"location":"building-a-workflow/#implementing-an-evaluator","title":"Implementing an Evaluator","text":"<p>Core to the testing process is the <code>Evaluator</code> implementation to compute the metrics defined in step 2. Usually, an evaluator simply plugs your existing metrics computation logic into the class-based or function-based evaluator interface.</p> <p>Evaluators can have arbitrary configuration (<code>EvaluatorConfiguration</code>), allowing you to evaluate model performance under a variety of conditions. For this keypoint detection example, perhaps we want to compute performance at a few different NME threshold values, as this threshold drives the <code>alignment_failure</code> metric.</p> <pre><code>from kolena.workflow import EvaluatorConfiguration\n\n@dataclass(frozen=True)\nclass NmeThreshold(EvaluatorConfiguration):\n    # threshold for NME above which an image is considered an \"alignment failure\"\n    threshold: float\n\n    def display_name(self):\n        return f\"NME threshold: {self.threshold}\"\n</code></pre> <p>Here, we'll mock out an evaluator implementation using the function-based interface:</p> <pre><code>from random import random\nfrom typing import List\n\nfrom kolena.workflow import EvaluationResults, TestCases\n\ndef evaluate_keypoint_detection(\n    test_samples: List[TestSample],\n    ground_truths: List[GroundTruth],\n    inferences: List[Inference],\n    test_cases: TestCases,\n    configuration: NmeThreshold,  # uncomment when configuration is used\n) -&gt; EvaluationResults:\n    # compute per-sample metrics for each test sample\n    per_sample_metrics = [\n        TestSampleMetrics(normalized_mean_error=random(), alignment_failure=bool(random() &gt; 0.5))\n        for gt, inf in zip(ground_truths, inferences)\n    ]\n\n    # compute aggregate metrics across all test cases using `test_cases.iter(...)`\n    aggregate_metrics = []\n    for test_case, *s in test_cases.iter(test_samples, ground_truths, inferences, per_sample_metrics):\n        test_case_metrics = TestCaseMetrics(mean_nme=random(), alignment_failure_rate=random())\n        aggregate_metrics.append((test_case, test_case_metrics))\n\n    return EvaluationResults(\n        metrics_test_sample=list(zip(test_samples, per_sample_metrics)),\n        metrics_test_case=aggregate_metrics,\n    )\n</code></pre>","boost":2},{"location":"building-a-workflow/#running-tests","title":"Running tests","text":"<p>To test our models, we can define an <code>infer</code> function that maps the <code>TestSample</code> object we defined above into an <code>Inference</code>:</p> <pre><code>from random import randint\n\ndef infer(test_sample: TestSample) -&gt; Inference:\n    \"\"\"\n    1. load the image pointed to at `test_sample.locator`\n    2. pass the image to our model and transform its output into an `Inference` object\n    \"\"\"\n\n    # Generate the dummy inference for the demo purpose.\n    return Inference(Keypoints([(randint(100, 400), randint(100, 400)) for _ in range(5)]), random())\n\nmodel = Model(\"example-model-name\", infer=infer, metadata=dict(\n    description=\"Any freeform metadata can go here\",\n    hint=\"It may be helpful to include information about the model's framework, training methodology, dataset, etc.\",\n))\n</code></pre> <p>We now have the pieces in place to run tests on our new workflow using <code>test</code>:</p> <pre><code>from kolena.workflow import test\n\ntest(\n    model,\n    test_suite,\n    evaluate_keypoint_detection,\n    configurations=[NmeThreshold(0.01), NmeThreshold(0.05), NmeThreshold(0.1)],\n)\n</code></pre> <p>That wraps up the testing process! We can now visit   Results to analyze and debug our model's performance on this test suite.</p>","boost":2},{"location":"building-a-workflow/#conclusion","title":"Conclusion","text":"<p>In this tutorial we learned how to build a workflow for an arbitrary ML problem, using a facial keypoint detection model as an example. We created new tests, tested our models on Kolena, and learned how to customize evaluation to fit our exact expectations.</p> <p>This tutorial just scratches the surface of what's possible with Kolena and covered a fraction of the <code>kolena</code> API \u2014 now that we're up and running, we can think about ways to create more detailed tests, improve existing tests, and dive deep into model behaviors.</p>","boost":2},{"location":"installing-kolena/","title":"Installing <code>kolena</code>","text":"<p>Testing on Kolena is conducted using the <code>kolena</code> Python package. You use the client to create and run tests from your infrastructure that can be explored in our web platform.</p> <p><code>kolena</code> is released under the open-source Apache-2.0 license. The package is hosted on PyPI and can be installed using your preferred Python package manager.</p>"},{"location":"installing-kolena/#installation","title":"Installation","text":"<p>The first step to start testing with Kolena is to install <code>kolena</code>. Client builds can be installed directly from PyPI using any Python package manager such as pip or Poetry:</p> <code>pip</code><code>poetry</code> <pre><code>pip install kolena\n</code></pre> <pre><code>poetry add kolena\n</code></pre> <p>Note</p> <p>Kolena uses <code>pydantic</code> for data validation, but is compatible only with <code>pydantic</code> V1. Prior to <code>0.76.0</code>, the <code>kolena</code> client did not set an upper bound on compatible <code>pydantic</code> version, and as a result, fresh installations of older versions of <code>kolena</code> may install incompatible version of <code>pydantic</code>, resulting in errors such as:</p> <pre><code>AttributeError: type object 'SingleProcessResponse' has no attribute '__pydantic_model__'\n</code></pre> <p>If you see the error above, please install the latest version of <code>kolena</code>.</p>"},{"location":"installing-kolena/#extra-dependency-groups","title":"Extra Dependency Groups","text":"<p>Certain metrics computation functionality depends on additional packages like scikit-learn. These extra dependencies can be installed via the <code>metrics</code> group:</p> <code>pip</code><code>poetry</code> <pre><code>pip install 'kolena[metrics]'\n</code></pre> <pre><code>poetry add 'kolena[metrics]'\n</code></pre>"},{"location":"installing-kolena/#initialization","title":"Initialization","text":"<p>Once you have <code>kolena</code> installed, initialize a session with <code>kolena.initialize(api_token=token)</code>.</p> <p>From the   Developer page, generate an API token and set the <code>KOLENA_TOKEN</code> environment variable:</p> <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre> <p>With the <code>KOLENA_TOKEN</code> environment variable set, initialize a client session:</p> <pre><code>import kolena\n\nkolena.initialize(verbose=True)\n</code></pre> <p>By default, sessions have static scope and persist until the interpreter is exited. Additional logging can be configured by specifying <code>initialize(..., verbose=True)</code>.</p> <p>Tip: <code>logging</code></p> <p>Integrate <code>kolena</code> into your existing logging system by filtering for events from the <code>\"kolena\"</code> logger. All log messages are emitted as both Python standard library <code>logging</code> events as well as stdout/stderr messages.</p>"},{"location":"installing-kolena/#supported-python-versions","title":"Supported Python Versions","text":"<p><code>kolena</code> is compatible with all active Python versions.</p>  Python Version  Compatible <code>kolena</code> Versions 3.11 \u22650.69 3.10 All Versions 3.9 All Versions 3.8 All Versions 3.7 All Versions 3.6 (EOL: December 2021) \u22640.46"},{"location":"quickstart/","title":"Quickstart","text":"<p>Install Kolena to set up rigorous and repeatable model testing in minutes.</p> <p>In this quickstart guide, we'll use the <code>object_detection_2d</code> example integration to demonstrate the how to curate test data and test models in Kolena.</p>"},{"location":"quickstart/#install-kolena","title":"Install <code>kolena</code>","text":"<p>Install the <code>kolena</code> Python package to programmatically interact with Kolena:</p> <code>pip</code><code>poetry</code> <pre><code>pip install kolena\n</code></pre> <pre><code>poetry add kolena\n</code></pre>"},{"location":"quickstart/#clone-the-examples","title":"Clone the Examples","text":"<p>The kolenaIO/kolena repository contains a number of example integrations to clone and run directly:</p> <ul> <li> <p>  Example: Age Estimation \u2197</p> <p></p> <p>Age Estimation using the Labeled Faces in the Wild (LFW) dataset</p> </li> <li> <p>  Example: Keypoint Detection \u2197</p> <p></p> <p>Facial Keypoint Detection using the 300 Faces in the Wild (300-W) dataset</p> </li> <li> <p> array-string  Example: Text Summarization \u2197</p> <p></p> <p>Text Summarization using OpenAI GPT-family models and the CNN-DailyMail dataset</p> </li> <li> <p>  Example: Object Detection (2D) \u2197</p> <p></p> <p>2D Object Detection using the COCO dataset</p> </li> <li> <p>  Example: Object Detection (3D) \u2197</p> <p></p> <p>3D Object Detection using the KITTI dataset</p> </li> <li> <p>  Example: Binary Classification \u2197</p> <p></p> <p>Binary Classification of class \"Dog\" using the Dogs vs. Cats dataset</p> </li> <li> <p>  Example: Multiclass Classification \u2197</p> <p></p> <p>Multiclass Classification using the CIFAR-10 dataset</p> </li> <li> <p>  Example: Semantic Textual Similarity \u2197</p> <p></p> <p>Semantic Textual Similarity using the STS benchmark dataset</p> </li> <li> <p>  Example: Question Answering \u2197</p> <p></p> <p>Question Answering using the Conversational Question Answering (CoQA) dataset</p> </li> <li> <p>  Example: Semantic Segmentation \u2197</p> <p></p> <p>Semantic Segmentation on class <code>Person</code> using the COCO-Stuff 10K dataset</p> </li> <li> <p>  Example: Automatic Speech Recognition \u2197</p> <p></p> <p>Automatic speech recognition using the LibriSpeech dataset</p> </li> <li> <p>  Example: Speaker Diarization \u2197</p> <p></p> <p>Speaker Diarization using the ICSI-Corpus dataset</p> </li> </ul> <p>To get started, clone the <code>kolena</code> repository:</p> <pre><code>git clone https://github.com/kolenaIO/kolena.git\n</code></pre> <p>With the repository cloned, let's set up the <code>object_detection_2d</code> example:</p> <pre><code>cd kolena/examples/object_detection_2d\npoetry update &amp;&amp; poetry install\n</code></pre> <p>Now we're up and running and can start creating test suites and testing models.</p>"},{"location":"quickstart/#create-test-suites","title":"Create Test Suites","text":"<p>Each of the example integrations comes with scripts for two flows:</p> <ol> <li><code>seed_test_suite.py</code>: Create test cases and test suite(s) from a source dataset</li> <li><code>seed_test_run.py</code>: Test model(s) on the created test suites</li> </ol> <p>Before running <code>seed_test_suite.py</code>, let's first configure our environment by populating the <code>KOLENA_TOKEN</code> environment variable. Visit the   Developer page to generate an API token and copy and paste the code snippet into your environment:</p> <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre> <p>We can now create test suites using the provided seeding script:</p> <pre><code>poetry run python3 object_detection_2d/seed_test_suite.py\n</code></pre> <p>After this script has completed, we can visit the   Test Suites page to view our newly created test suites.</p> <p>In this <code>object_detection_2d</code> example, we've created test suites stratifying the COCO 2014 validation set (which is stored as a CSV in S3) into test cases by brightness and bounding box size. In this example will be looking at the following labels:</p> <p><code>[\"bicycle\", \"car\", \"motorcycle\", \"bus\", \"train\", \"truck\", \"traffic light\", \"fire hydrant\", \"stop sign\"]</code></p>"},{"location":"quickstart/#test-a-model","title":"Test a Model","text":"<p>After we've created test suites, the final step is to test models on these test suites. The <code>object_detection_2d</code> example provides the following models to choose from <code>{yolo_r, yolo_x, mask_rcnn, faster_rcnn, yolo_v4s, yolo_v3}</code> for this step:</p> <pre><code>poetry run python3 object_detection_2d/seed_test_run.py \"yolo_v4s\"\n</code></pre> <p>Note: Testing additional models</p> <p>In this example, model results have already been extracted and are stored in CSV files in S3. To run a new model, plug it into the <code>infer</code> method in <code>seed_test_run.py</code>.</p> <p>Once this script has completed, click the results link in your console or visit   Results to view the test results for this newly tested model.</p>"},{"location":"quickstart/#conclusion","title":"Conclusion","text":"<p>In this quickstart, we used an example integration from kolenaIO/kolena to create test suites from the COCO dataset and test the open-source <code>yolo_v4s</code> model on these test suites.</p> <p>This example shows us how to define an ML problem as a workflow for testing in Kolena, and can be arbitrarily extended with additional metrics, plots, visualizations, and data.</p>"},{"location":"advanced-usage/","title":"Advanced Usage","text":"<p>This section contains tutorial documentation for advanced features available in Kolena.</p> <ul> <li> <p> Connecting Cloud Storage</p> <p>Establish integrations with cloud storage providers such as  Amazon S3 and  Google Cloud Storage.</p> </li> <li> <p> Packaging for Automated Evaluation</p> <p>Package metrics evaluation logic in a Docker container image to dynamically compute metrics on relevant subsets of your test data.</p> </li> <li> <p>  Nesting Test Case Metrics</p> <p>Report class-level metrics within a test case and test ensembles and pipelines of models by nesting aggregate metrics within your <code>MetricsTestCase</code>.</p> </li> <li> <p>  Uploading Activation Maps</p> <p>Upload and visualize your activation map for each <code>TestSample</code> along with your model results on the   Studio.</p> </li> <li> <p>  Setting Up Natural Language Search</p> <p>Extract and upload embeddings on each <code>Image</code> to set up natural language and similarity search across image data and results in the   Studio.</p> </li> </ul>"},{"location":"advanced-usage/nesting-test-case-metrics/","title":"Nesting Test Case Metrics","text":"<p>When computing test case metrics in an evaluator, in some cases it is desirable to compute multiple sets of aggregate metrics within a given test case.</p> <p> </p> <p>Class-level metrics for the <code>airplane</code>, <code>bear</code>, <code>bench</code>, etc. classes reported for the test case <code>complete :: coco-2014-val [Object Detection]</code></p> <p>Here are a few examples of scenarios where this pattern might be warranted:</p> Use Case Description Multiclass workflows For ML tasks with multiple classes, a given test case may contain samples from more than one class. While it's useful to report metrics aggregated across all classes using an averaging method, it's also useful to see aggregate metrics computed for each of the classes. Ensembles of models When testing an ensemble containing multiple models, it can be useful to see metrics from the output of the complete ensemble as well as metrics computed for each of the constituent models. Model pipelines When testing a pipeline of models, in which one model's output is used as an input for the next model, it can be difficult to understand where along the pipeline performance broke down. Reporting overall metrics as well as per-model metrics for each model in the pipeline (the metrics used can differ from one model to the next!) can help pinpoint the cause of failures within a pipeline. <p>In these cases, Kolena provides the API to nest additional aggregate metrics records within a <code>MetricsTestCase</code> object returned from an evaluator. In this tutorial, we'll learn how to use this API to report class-level or other nested test case metrics for our models.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#example-multiclass-object-detection","title":"Example: Multiclass Object Detection","text":"<p>Let's consider the case of a multiclass object detection task with objects of type <code>Airplane</code>, <code>Boat</code>, and <code>Car</code>. When a test case contains images with each of these three classes, test-case-level metrics are the average (e.g. micro, macro, or weighted) of class-level metrics across each of these three classes.</p> <p>For this workflow, we may consider using macro-averaged precision, recall, and F1 score, and mean average precision score (mAP) across all images as our metrics:</p> Test Case # Images <code>macro_Precision</code> <code>macro_Recall</code> <code>macro_F1</code> <code>mAP</code> Scenario A 2,500 0.91 0.99 0.95 0.97 Scenario B 1,500 0.83 0.96 0.89 0.91 <p>At the API level, these metrics would be defined:</p> <pre><code>from dataclasses import dataclass\n\nfrom kolena.workflow import MetricsTestCase\n\n@dataclass(frozen=True)\nclass AggregateMetrics(MetricsTestCase):\n    # Test Case, # Images are automatically populated\n    macro_Precision: float\n    macro_Recall: float\n    macro_F1: float\n    mAP: float\n</code></pre> <p>These metrics tell us how well the model performs in \"Scenario A\" and \"Scenario B\" across all classes, but they don't tell us anything about per-class model performance. Within each test case, we'd also like to see precision, recall, F1, and AP scores:</p> <code>Class</code> <code>N</code> <code>Precision</code> <code>Recall</code> <code>F1</code> <code>AP</code> <code>Airplane</code> 1,000 0.5 0.5 0.5 0.5 <code>Boat</code> 500 0.5 0.5 0.5 0.5 <code>Car</code> 2,000 0.5 0.5 0.5 0.5 <p>We can report these class-level metrics alongside the macro-averaged overall metrics by nesting <code>MetricsTestCase</code> definitions:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\n\nfrom kolena.workflow import MetricsTestCase\n\n@dataclass(frozen=True)\nclass PerClassMetrics(MetricsTestCase):\n    Class: str  # name of the class corresponding to this record\n    N: int  # number of samples containing this class\n    Precision: float\n    Recall: float\n    F1: float\n    AP: float\n\n@dataclass(frozen=True)\nclass AggregateMetrics(MetricsTestCase):\n    # Test Case, # Images are automatically populated\n    macro_Precision: float\n    macro_Recall: float\n    macro_F1: float\n    mAP: float\n    PerClass: List[PerClassMetrics]\n</code></pre> <p>Now we have the definitions to tell us everything we need to know about model performance within a test case: <code>AggregateMetrics</code> describes overall performance across all classes within the test case, and <code>PerClassMetrics</code> describes performance for each of the given classes within the test case.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#naming-nested-metric-records","title":"Naming Nested Metric Records","text":"<p>When defining nested metrics, e.g. <code>PerClassMetrics</code> in the example above, it's important to identify each row by including at least one <code>str</code>-type column. This column, e.g. <code>Class</code> above, is pinned to the left when displaying nested metrics on the   Results page.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#statistical-significance","title":"Statistical Significance","text":"<p>When comparing models, Kolena highlights performance improvements and regressions that are likely to be statistically significant. The number of samples being evaluated factors into these calculations.</p> <p>For nested metrics, certain fields like <code>N</code> in the above <code>PerClassMetrics</code> example are used as the population size for statistical significance calculations. To ensure that highlighted improvements and regressions in these nested metrics are statistically significant, populate this field for each class reported. In the above example, <code>N</code> can be populated with the number of images containing a certain class (good) or with the number of instances of that class across all images in the test case (better).</p> <p>For a full list of reserved field names for statistical significance calculations, see the API reference documentation for <code>MetricsTestCase</code>.</p>"},{"location":"advanced-usage/nesting-test-case-metrics/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to use the <code>MetricsTestCase</code> API to define class-level metrics within a test case. Nesting test case metrics is desirable for workflows with multiple classes, as well as when testing ensembles of models or testing model pipelines.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/","title":"Packaging for Automated Evaluation","text":""},{"location":"advanced-usage/packaging-for-automated-evaluation/#introduction","title":"Introduction","text":"<p>In addition to analyzing and debugging model performance, we can also use the Kolena platform to create and curate test cases and test suites. Kolena can automatically compute metrics on it for any models that have already uploaded inferences. In this guide, we'll learn how to package our custom metrics engine such that it can be used in this automatic evaluation process.</p> <p>To enable automatic metrics computation when applicable, we need to package the metrics evaluation logic into a Docker image that the Kolena platform can run. The following sections explain how to build this Docker image and link it for metrics computation on the Kolena platform.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#build-evaluator-docker-image","title":"Build Evaluator Docker Image","text":"<p>We will use the keypoint detection workflow we've built in the Building a Workflow guide to illustrate the process. Here is the project structure:</p> <pre><code>.\n\u251c\u2500\u2500 docker/\n\u2502   \u251c\u2500\u2500 build.sh\n\u2502   \u251c\u2500\u2500 publish.sh\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 keypoint_detection/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 evaluator.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 workflow.py\n\u251c\u2500\u2500 poetry.lock\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>The <code>keypoint_detection</code> directory is where our workflow is defined, with evaluator logic in <code>evaluator.py</code> and workflow data objects in <code>workflow.py</code>. The <code>main.py</code> will be the entry point where <code>test</code> is executed.</p> <p>From the workflow building guide, we know that metrics evaluation using <code>test</code> involves a <code>model</code>, a <code>test_suite</code>, an <code>evaluator</code>, and optional <code>configurations</code>:</p> <pre><code>test(model, test_suite, evaluator, configurations=configurations)\n</code></pre> <p>Note: test invocation</p> <p>Ensure that <code>reset=True</code> is NOT used in the <code>test</code> method when you only want to re-evaluate metrics and do not have the model <code>infer</code> logic built in the image. The flag would overwrite existing inference and metrics results of the test suite, therefore requires re-running model <code>infer</code> on the test samples.</p> <p>When executing <code>test</code> locally, the model and test suite can be initiated by user inputs. When Kolena executes <code>test</code> under automation, this information would have to be obtained through environment variables. Kolena sets up following environment variables for evaluator execution:</p> <ul> <li><code>KOLENA_MODEL_NAME</code></li> <li><code>KOLENA_TEST_SUITE_NAME</code></li> <li><code>KOLENA_TEST_SUITE_VERSION</code></li> <li><code>KOLENA_TOKEN</code></li> </ul> <p>The main script would therefore be adjusted like code sample below.</p> keypoint_detection/main.py<pre><code>import os\n\nimport kolena\nfrom kolena.workflow import test\n\nfrom .evaluator import evaluate_keypoint_detection, NmeThreshold\nfrom .workflow import Model, TestSuite\n\n\ndef main() -&gt; None:\n    kolena.initialize(verbose=True)\n\n    model = Model(os.environ[\"KOLENA_MODEL_NAME\"])\n    test_suite = TestSuite.load(\n        os.environ[\"KOLENA_TEST_SUITE_NAME\"],\n        os.environ[\"KOLENA_TEST_SUITE_VERSION\"],\n    )\n\n    test(model, test_suite, evaluate_keypoint_detection, configurations=[NmeThreshold(0.05)])\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Now that we have the main script ready, the next step is to package this script into a Docker image.</p> docker/Dockerfile<pre><code>FROM python:3.9-slim AS base\n\nWORKDIR /opt/keypoint_detection/\n\nFROM base AS builder\n\nARG KOLENA_TOKEN\nENV POETRY_VIRTUALENVS_IN_PROJECT=true \\\n    POETRY_NO_INTERACTION=1\nRUN python3 -m pip install poetry\n\nCOPY pyproject.toml poetry.lock ./\nCOPY keypoint_detection ./keypoint_detection\nRUN poetry install --only main\n\nFROM base\n\nCOPY --from=builder /opt/keypoint_detection /opt/keypoint_detection/\nCOPY --from=builder /opt/keypoint_detection/.venv .venv/\n\nENTRYPOINT [ \"/opt/keypoint_detection/.venv/bin/python\", \"keypoint_detection/main.py\" ]\n</code></pre> docker/build.sh<pre><code>#!/usr/bin/env bash\n\nset -eu\n\nIMAGE_NAME=\"keypoint_detection_evaluator\"\nIMAGE_VERSION=\"v1\"\nIMAGE_TAG=\"$IMAGE_NAME:$IMAGE_VERSION\"\n\necho \"building $IMAGE_TAG...\"\n\nexport DOCKER_BUILDKIT=1\nexport COMPOSE_DOCKER_CLI_BUILD=1\n\ndocker build \\\n    --tag \"$IMAGE_TAG\" \\\n    --file \"docker/Dockerfile\" \\\n    --build-arg KOLENA_TOKEN=${KOLENA_TOKEN} \\\n    .\n</code></pre> <p>This build process installs the <code>kolena</code> package, and as such needs the <code>KOLENA_TOKEN</code> environment variable to be populated with your Kolena API key. Follow the <code>kolena</code> Python client guide to obtain an API key if you have not done so.</p> <pre><code>export KOLENA_TOKEN=\"&lt;kolena-api-token&gt;\"\n./docker/build.sh\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#register-evaluator-for-workflow","title":"Register Evaluator for Workflow","text":"<p>The final step is to publish the Docker image and associate the image with the <code>Keypoint Detection</code> workflow.</p> <p>Kolena supports metrics computation using Docker image hosted on any public Docker registry or Kolena's Docker registry. In this tutorial, we will publish our image to Kolena's Docker registry. However, the steps should be easy to adapt to public Docker registry.</p> <p>The repositories on Kolena Docker registry must be prefixed with the organization name. This is to protect unauthorized access from unintended parties. Replace <code>&lt;organization&gt;</code> in <code>publish.sh</code> script with the actual organization name and run it. This would push our Docker image to the repository and register it for the workflow.</p> docker/publish.sh<pre><code>#!/usr/bin/env bash\n\nset -eu\n\nIMAGE_NAME=\"keypoint_detection_evaluator\"\nIMAGE_VERSION=\"v1\"\nIMAGE_TAG=\"${IMAGE_NAME}:${IMAGE_VERSION}\"\n\nDOCKER_REGISTRY=\"docker.kolena.io\"\nWORKFLOW=\"Keypoint Detection\"\nEVALUATOR_NAME=\"evaluate_keypoint_detection\"\nORGANIZATION=&lt;organization&gt;\n\nTARGET_IMAGE_TAG=\"$DOCKER_REGISTRY/$ORGANIZATION/$IMAGE_TAG\"\n\n# create repository if not exist\npoetry run kolena repository create --name \"$ORGANIZATION/$IMAGE_NAME\"\n\necho $KOLENA_TOKEN | docker login -u \"$ORGANIZATION\" --password-stdin $DOCKER_REGISTRY\n\necho \"publishing $TARGET_IMAGE_TAG...\"\n\ndocker tag $IMAGE_TAG $TARGET_IMAGE_TAG\ndocker push $TARGET_IMAGE_TAG\n\necho \"registering image $TARGET_IMAGE_TAG for evaluator $EVALUATOR_NAME of workflow $WORKFLOW...\"\n\npoetry run kolena evaluator register \\\n  --workflow \"$WORKFLOW\" \\\n  --evaluator-name \"$EVALUATOR_NAME\" \\\n  --image $TARGET_IMAGE_TAG\n</code></pre> <pre><code>./docker/publish.sh\n</code></pre> <p>In <code>publish.sh</code>, we used Kolena client SDK command-line <code>kolena</code> to associate the Docker image to evaluator <code>evaluate_keypoint_detection</code> of workflow <code>Keypoint Detection</code>. You can find out more of its usage with the <code>--help</code> option.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-automatic-metrics-evaluation","title":"Using Automatic Metrics Evaluation","text":"<p>At this point, we are all set to leverage Kolena's automatic metrics evaluation capability. To see it in action, let's first use Kolena's Studio to curate a new test case.</p> <p>Head over to the   Studio and use the \"Explore\" tab to learn more about the test samples from a given test case. Select multiple test samples of interest and then go to the \"Create\" tab to create a new test case with the \"Create Test Case\" button. You will notice there's an option to compute metrics on this new test case for applicable models. Since we have the evaluator image registered for our workflow <code>Keypoint Detection</code>, Kolena will automatically compute metrics for the new case if this option is checked. After the computation completes, metrics of the new test case are immediately ready for us to analyze on the Results page.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to configure Kolena to automatically compute metrics when applicable, and why it brings values to model testing and analyzing process. We can use these tools to continue improving our test cases and our models.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#appendix","title":"Appendix","text":""},{"location":"advanced-usage/packaging-for-automated-evaluation/#evaluator-runtime-limits","title":"Evaluator runtime limits","text":"<p>Currently, the environment evaluator runs in does not support GPU. There is a maximum of 6 hours processing time. The evaluation job would be terminated when the run time reaches the limit.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#testing-evaluator-locally","title":"Testing evaluator locally","text":"<p>You can verify the evaluator Docker image by running it locally:</p> <pre><code>docker run --rm \\\n  -e KOLENA_TEST_SUITE_NAME=\"${EXISTING_TEST_SUITE_NAME}\" \\\n  -e KOLENA_TEST_SUITE_VERSION=3 \\\n  -e KOLENA_MODEL_NAME=\"example keypoint detection model\" \\\n  -e KOLENA_WORKFLOW=\"Keypoint Detection\" \\\n  -e KOLENA_TOKEN=$KOLENA_TOKEN \\\n  &lt;evaluator-docker-image&gt;\n</code></pre> <p>You can find a test suite's version on the   Test Suites page. By default, the latest version is displayed.</p>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-dockerkolenaio","title":"Using docker.kolena.io","text":"<p>In this tutorial, we published an evaluator container image to <code>docker.kolena.io</code>, Kolena's Docker Registry. In this section, we'll explain how to use the Docker CLI to interact with <code>docker.kolena.io</code>.</p> <p>The first step is to use <code>docker login</code> to log into <code>docker.kolena.io</code>. Using your organization's name (e.g. <code>my-organization</code>, the part after <code>app.kolena.io</code> when you visit the app) as a username and your API token as a password, log in with the following command:</p> <pre><code>echo $KOLENA_TOKEN | docker login --username my-organization --password-stdin docker.kolena.io\n</code></pre> <p>Once you've successfully logged in, you can use Docker CLI to perform actions on the Kolena Docker registry. For example, to pull a previously published Docker image, use a command like:</p> <pre><code>docker pull docker.kolena.io/my-organization/&lt;docker-image-tag&gt;\n</code></pre> <p>If you're building Docker images for a new workflow, use the <code>kolena</code> command-line tool to create the repository on <code>docker.kolena.io</code> first. As mentioned in Register Evaluator for Workflow, the repository must be prefixed with your organization's name.</p> <pre><code>poetry run kolena repository create -n my-organization/new-evaluator\n</code></pre> <p>After the repository is created, we can use the Docker CLI to publish a newly built Docker image to <code>docs.kolena.io</code>:</p> <pre><code>docker push docker.kolena.io/my-organization/new-evaluator:v1\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-secrets-in-your-evaluator","title":"Using Secrets in your Evaluator","text":"<p>If secret or sensitive data is used in your evaluation process, Kolena's secret manager can store this securely and pass it as the environment variable <code>KOLENA_EVALUATOR_SECRET</code> at runtime.</p> <p>Update the evaluator register command in <code>docker/publish.sh</code> to pass in sensitive data for the evaluator:</p> <pre><code>poetry run kolena evaluator register --workflow \"$WORKFLOW\" \\\n  --evaluator-name \"$EVALUATOR_NAME\" \\\n  --image $TARGET_IMAGE_TAG \\\n  --secret '&lt;your secret&gt;'\n</code></pre>"},{"location":"advanced-usage/packaging-for-automated-evaluation/#using-aws-apis-in-your-evaluator","title":"Using AWS APIs in your Evaluator","text":"<p>If your evaluator requires access to AWS APIs, specify the full AWS role ARN it should use in the evaluator register command.</p> <pre><code>poetry run kolena evaluator register --workflow \"$WORKFLOW\" \\\n  --evaluator-name \"$EVALUATOR_NAME\" \\\n  --image $TARGET_IMAGE_TAG \\\n  --aws-assume-role &lt;target_role_arn&gt;\n</code></pre> <p>The output of the command would look like:</p> <pre><code>{\n  \"workflow\": \"Keypoint Detection\",\n  \"name\": \"evaluate_keypoint_detection\",\n  \"image\": \"docker.kolena.io/my-organization/keypoint_detection_evaluator:v1\",\n  \"created\": \"2023-04-03 16:18:10.703 -0700\",\n  \"secret\": null,\n  \"aws_role_config\": {\n    \"job_role_arn\": \"&lt;Kolena AWS role ARN&gt;\",\n    \"external_id\": \"&lt;Generated external_id&gt;\",\n    \"assume_role_arn\": \"&lt;target_role_arn&gt;\"\n  }\n}\n</code></pre> <p>The response includes the AWS role ARN that Kolena will use to run the evaluator Docker image, <code>aws_role_config.job_role_arn</code>, and the external_id, <code>aws_role_config.external_id</code>, to verify that requests are made from Kolena.</p> <p>To allow Kolena's AWS role to assume the target role in your AWS account, you need to configure the trust policy of the target role. Here is an example of the trust policy.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sts:AssumeRole\"\n      ],\n      \"Principal\": {\n        \"AWS\": \"&lt;Kolena AWS role ARN&gt;\"\n      },\n      \"Condition\": {\n        \"StringEquals\": {\n          \"sts:ExternalId\": \"&lt;External_id generated by Kolena&gt;\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Please refer to AWS documents for details on Delegate access across AWS accounts using IAM roles.</p> <p>At runtime, Kolena would pass in the target role and the <code>external_id</code> in environment variables <code>KOLENA_EVALUATOR_ASSUME_ROLE_ARN</code> and <code>KOLENA_EVALUATOR_EXTERNAL_ID</code>, respectively. The evaluator would then use AWS assume-role to transit into the intended target role, and use AWS APIs under the new role.</p> <pre><code>import os\nimport boto3\n\nresponse = boto3.client(\"sts\").assume_role(\n    RoleArn=os.environ[\"KOLENA_EVALUATOR_ASSUME_ROLE_ARN\"],\n    ExternalId=os.environ[\"KOLENA_EVALUATOR_EXTERNAL_ID\"],\n    RoleSessionName=\"metrics-evaluator\",\n)\ncredentials = response[\"Credentials\"]\n</code></pre> <p>An example of making AWS API requests under the assumed role is shown below.</p> <pre><code># use credentials to initialize AWS sessions/clients\nclient = boto3.client(\n    \"s3\",\n    aws_access_key_id=credentials[\"AccessKeyId\"],\n    aws_secret_access_key=credentials[\"SecretAccessKey\"],\n    aws_session_token=credentials[\"SessionToken\"],\n)\n</code></pre>"},{"location":"advanced-usage/set-up-natural-language-search/","title":"Natural Language Search Setup","text":"<p>Kolena supports natural language and similar image search across <code>Image</code> data previously registered to the platform. Users may set up this functionality by extracting and uploading the corresponding search embeddings using a Kolena provided package.</p> <p>Note</p> <p>Kolena supports search embedding extraction and upload as an opt-in feature for our customers. Please message your point of contact for the latest relevant extractor package.</p>"},{"location":"advanced-usage/set-up-natural-language-search/#example","title":"Example","text":"<p>The <code>kolena/kolenaIO</code> contains a runnable example of integration code for embeddings extraction and upload. This builds off the data uploaded in the age_estimation example workflow, and is best run after this data has been uploaded to your Kolena platform.</p>"},{"location":"advanced-usage/set-up-natural-language-search/#how-to-set-up-natural-language-search","title":"How to Set Up Natural Language Search","text":"<p>Uploading embeddings to Kolena can be done in three simple steps:</p> <ul> <li>Step 1: installing dependency package</li> <li>Step 2: loading images for input to extraction library</li> <li>Step 3: extracting and uploading search embeddings</li> </ul> <p>Let's take a look at each step with example code snippets.</p>"},{"location":"advanced-usage/set-up-natural-language-search/#step-1-install-kolena_embeddings-package","title":"Step 1: Install <code>kolena_embeddings</code> Package","text":"<p>Copy the <code>kolena_embeddings-*.*.*.tar.gz</code> file (provided by your Kolena contact) to your working directory, and install it as a dependency.</p> <code>pip</code><code>poetry</code> <pre><code>pip install ./kolena_embeddings-*.*.*.tar.gz\n</code></pre> <pre><code>poetry add ./kolena_embeddings-*.*.*.tar.gz\n</code></pre> <p>This package provides the <code>kembed.util.extract_and_upload_embeddings</code> method: <pre><code>def extract_and_upload_embeddings(locators_and_images: Iterable[Tuple[str, Image.Image]], batch_size: int = 50) -&gt; None:\n    \"\"\"\n    Extract and upload a list of search embeddings corresponding to sample locators.\n    Expects to have an exported `KOLENA_TOKEN` environment variable, as per [Kolena client documentation](https://docs.kolena.io/installing-kolena/#initialization).\n\n    :param locators_and_images: An iterator through PIL Image files and their corresponding locators (as provided to\n        the Kolena platform).\n    :param batch_size: Batch size for number of images to extract embeddings for simultaneously. Defaults to 50 to\n        avoid having too many file handlers open at once.\n    \"\"\n</code></pre></p>"},{"location":"advanced-usage/set-up-natural-language-search/#step-2-load-images-for-extraction","title":"Step 2: Load Images for Extraction","text":"<p>In order to extract embeddings on image data, we must load our image files into a <code>PIL.Image.Image</code> object. In this section, we will load these images from an S3 bucket. For other cloud storage services, please refer to your cloud storage's API docs.</p> <pre><code>from typing import Iterator\nfrom typing import List\nfrom typing import Tuple\n\nimport boto3\nfrom urllib.parse import urlparse\nfrom PIL import Image\n\ns3 = boto3.client(\"s3\")\n\ndef load_image_from_locator(locator: str) -&gt; Image.Image:\n    parsed_url = urlparse(locator)\n    bucket_name = parsed_url.netloc\n    key = parsed_url.path.lstrip(\"/\")\n    file_stream = boto3.resource(\"s3\").Bucket(bucket_name).Object(key).get()[\"Body\"]\n    return Image.open(file_stream)\n\ndef iter_image_locators(locators: List[str]) -&gt; Iterator[Tuple[str, Image.Image]]:\n    for locator in locators:\n        image = load_image_from_locator(locator)\n        yield locator, image\n</code></pre> <p>Tip</p> <p>When processing large scales of images, we recommend using an <code>Iterator</code> to limit the number of images loaded into memory at once.</p>"},{"location":"advanced-usage/set-up-natural-language-search/#step-3-extract-and-upload-embeddings","title":"Step 3: Extract and Upload Embeddings","text":"<p>We first retrieve and set our <code>KOLENA_TOKEN</code> environment variable. This is used by the uploader for authentication against your Kolena instance.</p> <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre> <p>We then pass our locators into the <code>extract_and_upload_embeddings</code> function to iteratively upload embeddings for all <code>Image</code> objects in the Kolena platform with matching locators.</p> <pre><code>from kembed.util import extract_and_upload_embeddings\n\nlocators = [\n    \"s3://kolena-public-datasets/labeled-faces-in-the-wild/imgs/AJ_Cook/AJ_Cook_0001.jpg\",\n    \"s3://kolena-public-datasets/labeled-faces-in-the-wild/imgs/AJ_Lamas/AJ_Lamas_0001.jpg\",\n    \"s3://kolena-public-datasets/labeled-faces-in-the-wild/imgs/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\",\n]\nextract_and_upload_embeddings(iter_image_locators(locators))\n</code></pre> <p>Once the upload completes, we can now visit   Studio to search by natural language over the corresponding <code>Image</code> data.</p>"},{"location":"advanced-usage/set-up-natural-language-search/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to extract and upload vector embeddings over your <code>Image</code> data.</p>"},{"location":"advanced-usage/set-up-natural-language-search/#faq","title":"FAQ","text":"Can I share embeddings with Kolena even if I do not share the underlying images? <p>Yes!</p> <p>Embeddings extraction is a unidirectional mapping, and used only for natural language search and similarity comparisons. Uploading these embeddings to Kolena does not allow for any reconstruction of these images, nor does it involve sharing these images with Kolena.</p> Do I need to upload embeddings for every test suite on the Kolena platform? <p>Embeddings are uploaded by locator, but resolved against existing image samples in the platform at upload time. This means that these embeddings are matched against every image with the provided locator across multiple test suites, and subsequent test suites containing the same <code>Image</code> will remain associated with these search embeddings.</p> <p>Please note that if you subsequently register an image sample with different fields (but the same locator), the previously uploaded embeddings may not automatically associate with the image sample. We are working on improving this process so that once embeddings are uploaded once, future image samples linked to the same locator will automatically use these embeddings. Please stay tuned!</p>"},{"location":"advanced-usage/uploading-activation-maps/","title":"Uploading Activation Maps","text":"<p>As models continue to grow larger and more complex, it is increasingly difficult to understand the reasoning behind their decisions or predictions. Understanding why a model produced a specific output is a process called Explainable AI (XAI) and can help data scientists and engineers comprehend and trust model results.</p> <p>There are many explanation methods for different model architectures. Most of the popular techniques used in computer vision workflows output a map that highlights regions in an image that are relevant to the model output. This map is called an activation map.</p> <p> </p> <p>Visualization of an activation map overlaid on an image</p>"},{"location":"advanced-usage/uploading-activation-maps/#popular-interpretation-methods-for-computer-vision","title":"Popular Interpretation Methods for Computer Vision","text":"<p>There are various methodologies that facilitate and aid the interpretation of several computer vision models, and if you are interested in learning more about them, here is a list of some of the popular methods:</p> <ul> <li>Vanilla Gradient (Saliency Maps)</li> <li>Class Activation Mapping (CAM)</li> <li>Gradient-weighted Class Activation Mapping (Grad-CAM) \u2014 PyTorch tutorial</li> </ul>"},{"location":"advanced-usage/uploading-activation-maps/#can-i-visualize-activation-maps-on-kolena","title":"Can I Visualize Activation Maps on Kolena?","text":"<p>Yes! Activation maps can be visualized as an overlay on the corresponding image in   Studio using the <code>BitmapMask</code> annotation type which can help us understand the model\u2019s decision \u2014 what the model \u201csees\u201d when it makes its prediction.</p> <p>In this tutorial, we\u2019ll learn how to upload and visualize activation maps as a part of testing models on Kolena.</p>"},{"location":"advanced-usage/uploading-activation-maps/#how-to-upload-activation-maps-on-kolena","title":"How to Upload Activation Maps on Kolena?","text":"<p>Uploading activation maps to Kolena can be done in three simple steps:</p> <ul> <li>Step 1: creating PNG bitmaps from 2D array activation maps</li> <li>Step 2: uploading PNG bitmaps to cloud storage</li> <li>Step 3: updating inferences and running tests</li> </ul> <p>Let's take a look at each step with example code snippets.</p>"},{"location":"advanced-usage/uploading-activation-maps/#step-1-creating-png-bitmaps","title":"Step 1: Creating PNG Bitmaps","text":"<p>The activation map is a 2D data array ranging from 0 to 1 with <code>(h, w)</code> shape. This array is converted to a PNG bitmap using the following two utility methods:</p> <ul> <li><code>colorize_activation_map</code>: applies color and opacity to the input activation map</li> <li><code>encode_png</code>: encodes the colorized map into an in-memory PNG image represented as binary data</li> </ul> <pre><code>import io\nimport numpy as np\nfrom kolena.workflow.visualization import colorize_activation_map\nfrom kolena.workflow.visualization import encode_png\n\ndef create_bitmap(activation_map: np.ndarray) -&gt; io.BytesIO:\n    bitmap = colorize_activation_map(activation_map)\n    image_buffer = encode_png(bitmap, mode=\"RGBA\")\n    return image_buffer\n</code></pre> <p>Activation Map Scaling</p> <p>The activation map often has the equal dimensions (i.e., width and height) as the input image or sometimes has the scaled-down dimensions with the fixed ratio. Kolena automatically scales the overlay annotations to the images so there is no need to up-scale the map to match the image dimensions.</p>"},{"location":"advanced-usage/uploading-activation-maps/#step-2-uploading-png-bitmaps-to-cloud-storage","title":"Step 2: Uploading PNG Bitmaps to Cloud Storage","text":"<p>In order to visualize the bitmaps on Kolena, these bitmaps must be uploaded to a cloud storage first, and their locators are used to create <code>BitmapMask</code> annotations. In this section, we will learn how to upload the in-memory bitmaps to an S3 bucket. For other cloud storage services, please refer to your cloud storage's API docs.</p> <pre><code>import io\nimport boto3\nfrom urllib.parse import urlparse\n\nBUCKET = \"&lt;YOUR_S3_BUCKET&gt;\"\n\ns3 = boto3.client(\"s3\")\n\ndef bitmap_locator(filename: str) -&gt; str:\n    return f\"{BUCKET}/tutorial/activation_maps/{filename}.png\"\n\ndef upload_bitmap(image_buffer: io.BytesIO, filename: str) -&gt; str:\n    locator = bitmap_locator(filename)\n    parsed_url = urlparse(locator)\n    s3_bucket = parsed_url.netloc\n    s3_key = parsed_url.path.lstrip(\"/\")\n    s3.upload_fileobj(image_buffer, s3_bucket, s3_key)\n    return locator\n</code></pre> <p>With all the building blocks we learned from Step 1 and Step 2, we can now create a <code>BitmapMask</code> with a given activation map.</p> <pre><code>from kolena.workflow.annotation import BitmapMask\n\ndef create_and_upload_bitmap(\n    filename: str,\n    activation_map: np.ndarray,\n) -&gt; BitmapMask:\n    image_buffer = create_bitmap(activation_map)\n    locator = upload_bitmap(image_buffer, filename)\n    return BitmapMask(locator)\n</code></pre>"},{"location":"advanced-usage/uploading-activation-maps/#step-3-updating-inference-and-running-tests","title":"Step 3: Updating <code>Inference</code> and Running Tests","text":"<p>Info</p> <p>If you are not familiar with the workflow concept, please read the   Building a Workflow guide.</p> <p>For the purposes of this tutorial, let's assume we already have a workflow built, and we are going to upload the activation maps as one of the fields in <code>Inference</code>. All we need to do is to update the <code>Inference</code> definition to include a new field for the activation map:</p> <pre><code>from kolena.workflow import Inference as Inf\nfrom kolena.workflow.annotation import BitmapMask\n\n@dataclass(frozen=True)\nclass Inference(Inf):\n    ...\n    activation_map: BitmapMask\n</code></pre> <p>Info</p> <p>If you are not familiar with how to run tests, please read the Step 4: Running Tests from   Building a Workflow guide.</p> <p>Before you run tests, make sure to update your <code>infer</code> function to return an <code>Inference</code> with the corresponding <code>BitmapMask</code> as its <code>activation_map</code> field. You are now ready to run tests! Once the tests complete, we can now visit   Studio to visualize activation maps overlaid on your <code>Image</code> data.</p>"},{"location":"advanced-usage/uploading-activation-maps/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to upload activation maps to Kolena in order to visualize activation maps overlaid on your <code>Image</code> data along with your ground truths and inferences.</p>"},{"location":"advanced-usage/connecting-cloud-storage/","title":"Connecting Cloud Storage","text":"<p>Data such as images, videos, and documents hosted internally or with cloud storage providers can be loaded on Kolena by creating an integration.</p> <p>Integrations can be managed by organization administrators by navigating to the \"Integrations\" tab on the   Organization Settings page.</p> <ul> <li> <p> Amazon S3</p> <p>Integrate with Amazon S3.</p> </li> <li> <p> S3-Compatible APIs</p> <p>Integrate with a third-party system implementing an S3-Compatible API (MinIO, Oracle, Hitachi, etc.).</p> </li> <li> <p> Google Cloud Storage</p> <p>Integrate with Google Cloud Storage.</p> </li> <li> <p> Azure Blob Storage</p> <p>Integrate with Azure Blob Storage.</p> </li> <li> <p>  HTTP Basic</p> <p>Integrate with an internal system using HTTP basic authentication.</p> </li> </ul>"},{"location":"advanced-usage/connecting-cloud-storage/amazon-s3/","title":"Connecting Cloud Storage:  Amazon S3","text":"<p>Kolena connects with Amazon S3 to load files (e.g. images, videos, documents) directly into your browser for visualization. In this tutorial, we'll learn how to establish an integration between Kolena and Amazon S3.</p> <p>To get started, ensure you have administrator access within Kolena. Navigate to the \"Integrations\" tab on the   Organization Settings page and click \"Add Integration\", then \"Amazon S3\".</p>"},{"location":"advanced-usage/connecting-cloud-storage/amazon-s3/#step-1-select-integration-scope","title":"Step 1: Select Integration Scope","text":"<p>Amazon S3 integrations load bucket objects using pre-signed URLs. Kolena generates these URLs by temporarily assuming an IAM role that has access to the specified bucket(s).</p> <p>By default, Kolena will assume this role when loading objects from any permitted bucket. Alternatively, if you wish for Kolena to assume this role only while loading objects from one bucket, uncheck \"Use role by default for all permitted buckets?\" and specify the name of an S3 bucket.</p> <p>Click \"Next\".</p> <p>Note: Scoping Integrations</p> <p>If \"Use role by default for all permitted buckets?\" is selected, Kolena will load any locators beginning with <code>s3://</code> by assuming the role configured for this Integration and generating a presigned URL.</p> <p>Scoping the Integration to one bucket (e.g. <code>my-bucket</code>) means Kolena will only assume the role when generating presigned URLs for locators of the form <code>s3://my-bucket/*</code>.</p>"},{"location":"advanced-usage/connecting-cloud-storage/amazon-s3/#step-2-create-an-access-policy-in-aws","title":"Step 2: Create an Access Policy in AWS","text":"<p>If you selected \"Use role by default for all permitted buckets?\" in the previous step, you must now choose which buckets Kolena is permitted to load objects from. Enter these bucket names.</p> <p>When you have entered the bucket names, you will see an \"Access Policy JSON\" rendered to your page. Copy this JSON.</p> <p>Note: IAM Write Permission Required</p> <p>You will require IAM write permissions within your AWS account to perform the next step.</p> <p>In your AWS console, navigate to the IAM policies page and follow these steps:</p> <ol> <li>Click the \"Create Policy\" button and select the \"JSON\" tab.</li> <li>Paste the \"Access Policy JSON\" copied previously.</li> <li>Click through the \"Next\" buttons, adding the desired name, description, and tags.</li> </ol>"},{"location":"advanced-usage/connecting-cloud-storage/amazon-s3/#step-3-create-a-role-for-kolena-to-assume","title":"Step 3: Create a Role For Kolena to Assume","text":"<p>Return to Kolena and copy the \"Trust Policy JSON\".</p> <p>In your AWS console, navigate to the IAM roles page and follow these steps:</p> <ol> <li>Click the \"Create role\" button and select \"Custom trust policy\".</li> <li>Paste the \"Trust Policy JSON\" you copied above and click \"Next\".</li> <li>Search for and select the access policy created in step 2. Click \"Next\".</li> <li>Provide a role name and review the permissions, then click \"Create role\".</li> </ol> <p>Copy the role's ARN for use in the next step.</p>"},{"location":"advanced-usage/connecting-cloud-storage/amazon-s3/#step-4-save-integration","title":"Step 4: Save Integration","text":"<p>Return to Kolena and fill in the remaining fields for the Integration and then click \"Save\".</p> Field Description Role ARN The ARN of the role created in step 3 Region The region your buckets will be accessed from (e.g. <code>us-east-1</code>)"},{"location":"advanced-usage/connecting-cloud-storage/amazon-s3/#appendix","title":"Appendix","text":""},{"location":"advanced-usage/connecting-cloud-storage/amazon-s3/#allow-cors-access-to-bucket","title":"Allow CORS Access to Bucket","text":"<p>In some scenarios, CORS permissions are required for Kolena to render content from your bucket.</p> <p>To configure CORS access, navigate to your S3 bucket inside your AWS console and follow these steps:</p> <ol> <li>Click on the \"Permissions\" tab and navigate to the \"Cross-origin resource sharing (CORS)\" section.</li> <li>Click \"Edit\" and add the following JSON snippet:</li> </ol> <pre><code>[\n  {\n    \"AllowedHeaders\": [\"*\"],\n    \"AllowedMethods\": [\"GET\"],\n    \"AllowedOrigins\": [\"https://app.kolena.io\"],\n    \"ExposeHeaders\": []\n  }\n]\n</code></pre>"},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/","title":"Connecting Cloud Storage:  Azure Blob Storage","text":"<p>Kolena connects with Azure Blob Storage to load files (e.g. images, videos, documents) directly into your browser for visualization. In this tutorial, we'll learn how to establish an integration between Kolena and Azure Blob Storage.</p> <p>To get started, ensure you have administrator access within Kolena. Navigate to the \"Integrations\" tab on the   Organization Settings page and click \"Add Integration\", then \"Azure Blob Storage\".</p>"},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/#step-1-create-azure-app-registration-for-kolena","title":"Step 1: Create Azure App Registration for Kolena","text":"<p>Azure Blob Storage integrations load resources using shared access signatures. Kolena generates these signatures using delegated access to these resources. We will generate an App registration for Kolena in Azure and then assign roles to this registration:</p> <ol> <li>From the Azure portal, search for \"App registrations\" and navigate to this page</li> <li>Click \"New Registration\"<ol> <li>Under \"Supported account types\", select \"Accounts in any organizational directory\"</li> <li>Click \"Register\" to save the App registration</li> </ol> </li> <li>Click on the App registration you have created</li> <li>Note the \"Tenant ID\" and \"Application (client) ID\"</li> <li>Click \"Certificates &amp; secrets\", then \"New client secret\"<ol> <li>Click \"Add\" to save this secret and note the key value</li> </ol> </li> </ol>"},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/#step-2-assign-roles-to-app-registration","title":"Step 2: Assign Roles to App Registration","text":"<p>We will assign two roles to the App registration created above:</p> <ul> <li>Storage Blob Delegator at the storage account level</li> <li>Storage Blob Data Reader at the container level</li> </ul>"},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/#assign-storage-blob-delegator-role","title":"Assign Storage Blob Delegator Role","text":"<ol> <li>Navigate to the storage account containing your blobs</li> <li>Click \"Access Control (IAM)\"</li> <li>Click the \"Role assignments\" tab<ol> <li>Click \"Add\", then \"Add role assignment\"</li> </ol> </li> <li>Search for and select \"Storage Blob Delegator\"</li> <li>Click on the \"Members\" tab, then click \"Select members\"<ol> <li>Search for the App registration created in step 1</li> <li>Click \"Select\"</li> </ol> </li> <li>Click \"Review + assign\" to save</li> </ol>"},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/#assign-storage-blob-data-reader-role","title":"Assign Storage Blob Data Reader role","text":"<ol> <li>From the storage account, click \"Containers\" under \"Data Storage\" and click on the container containing your blobs</li> <li>Click \"Access Control (IAM)\"</li> <li>Click the \"Role assignments\" tab<ol> <li>Click \"Add\", then \"Add role assignment\"</li> </ol> </li> <li>Search for and select \"Storage Blob Data Reader\"</li> <li>Click on the \"Members\" tab, then click \"Select members\"<ol> <li>Search for the App registration created in step 1</li> <li>Click \"Select\"</li> </ol> </li> <li>Click \"Review + assign\" to save</li> </ol>"},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/#step-3-save-integration-in-kolena","title":"Step 3: Save Integration in Kolena","text":"<p>Return to Kolena and fill in the fields for the Integration and then click \"Save\".</p> Field Description Tenant ID The Tenant ID of the App registration created in step 1 Client ID The Application (client) ID of the App registration created in step 1 Client Secret The secret key for the App registration created in step 1 Storage Account Name The storage account in Azure you wish to connect to Storage Blob EndpointURL The endpoint for accessing the storage account. Can be found in \"Endpoints\" under \"Settings\" for your storage account. Usually of the form <code>https://&lt;storage-account-name&gt;.blob.core.windows.net</code>"},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/#appendix","title":"Appendix","text":""},{"location":"advanced-usage/connecting-cloud-storage/azure-blob-storage/#provide-cors-access-to-kolena","title":"Provide CORS access to Kolena","text":"<p>In some scenarios, CORS permissions are required for Kolena to render content from your bucket.</p> <p>To configure CORS access, navigate to your storage account in the Azure portal and follow these steps:</p> <ol> <li>Click \"Resource Sharing (CORS)\" under \"Settings\"</li> <li>Add <code>https://app.kolena.io</code> as an allowed origin with <code>GET</code> as an allowed method</li> </ol>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/","title":"Connecting Cloud Storage:  Google Cloud Storage","text":"<p>Kolena connects with Google Cloud Storage to load files (e.g. images, videos, documents) directly into your browser for visualization. In this tutorial, we'll learn how to establish an integration between Kolena and Google Cloud Storage.</p> <p>To get started, ensure you have administrator access within Kolena. Navigate to the \"Integrations\" tab on the   Organization Settings page and click \"Add Integration\", then \"Google Cloud Storage\".</p>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/#step-1-save-integration-to-create-a-service-account","title":"Step 1: Save Integration to Create a Service Account","text":"<p>From the Integrations tab, saving a Google Cloud Storage integration will create a service account. Upon creation, the integration's <code>client_email</code> will be used to provide Kolena permission to load data from your Google Cloud Storage buckets.</p>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/#step-2-grant-service-account-read-access","title":"Step 2: Grant Service Account Read Access","text":"<p>Within your Google Cloud Platform console, navigate to the bucket that contains your images. Click on the permissions tab. Click the \"Grant Access\" button and grant the service account created in step 1 the <code>Storage Object Viewer</code> role. The <code>Storage Object Viewer</code> role offers the following permissions:</p> <ul> <li>Grants access to view objects and their metadata, excluding ACLs.</li> <li>Grants access to list the objects in a bucket.</li> </ul>"},{"location":"advanced-usage/connecting-cloud-storage/google-cloud-storage/#step-3-provide-cors-access","title":"Step 3: Provide CORS Access","text":"<p>Create a json file <code>cors.json</code> with the following content:</p> <pre><code>[\n  {\n    \"origin\": [\"https://app.kolena.io\"],\n    \"method\": [\"GET\"],\n    \"responseHeader\": [\"Content-Type\"],\n    \"maxAgeSeconds\": 3600\n  }\n]\n</code></pre> <p>Ensure you have <code>gsutil</code> installed. Then provide CORS access to Kolena for your bucket by running the following command:</p> <pre><code>gsutil cors set example_cors_file.json gs://&lt;my-bucket&gt;\n</code></pre>"},{"location":"advanced-usage/connecting-cloud-storage/http-basic/","title":"Connecting Cloud Storage:            HTTP Basic","text":"<p>Kolena connects with systems that utilize HTTP basic authentication to load files (e.g. images, videos, documents) directly into your browser for visualization. In this tutorial, we'll learn how to establish an integration between Kolena and a file-serving system that utilizes HTTP basic authentication.</p> <p>To get started, ensure you have administrator access within Kolena. Navigate to the \"Integrations\" tab on the   Organization Settings page and click \"Add Integration\", then \"HTTP Basic\".</p>"},{"location":"advanced-usage/connecting-cloud-storage/http-basic/#step-1-save-integration-on-kolena","title":"Step 1: Save Integration on Kolena","text":"<p>On the Integrations tab, fill in the fields for the integration and then click \"Save\".</p> Field Description URL Origin The origin of the domain you wish to load data from. Ensure you omit the protocol (e.g. <code>https://</code>) Username The username for your http basic auth system Password The password (optional) for your http basic auth system <p>Any locators beginning with <code>https://&lt;URL Origin&gt;</code> will be loaded using this integration.</p>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/","title":"Connecting Cloud Storage:  S3-Compatible APIs","text":"<p>Kolena connects with any S3-compatible system to load files (e.g. images, videos, documents) directly into your browser for visualization. Supported systems include:</p> <ul> <li> MinIO</li> <li> Oracle Object Storage</li> <li> Hitachi Content Platform (HCP) for cloud scale</li> </ul> <p>In this tutorial, we'll learn how to establish an integration between Kolena and a storage system implementing an S3-compatible API.</p> <p>To get started, ensure you have administrator access within Kolena. Navigate to the \"Integrations\" tab on the   Organization Settings page and click \"Add Integration\", then \"MinIO\".</p> <p>Steps performed outside of Kolena are shown for a subset of possible S3-compatible systems. You may need to consult documentation for your provider to perform equivalent steps.</p>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/#step-1-create-a-service-user-for-kolena","title":"Step 1: Create a Service User for Kolena","text":"<code>MinIO</code> <pre><code>mc admin user add &lt;deployment_alias&gt; &lt;kolena_user&gt; &lt;secret_access_key&gt;\n</code></pre>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/#step-2-create-an-access-policy","title":"Step 2: Create an Access Policy","text":"<p>Create a policy to allow read access for a bucket or set of buckets.</p> <p>Save the following JSON policy to a file called <code>/tmp/kolena-policy.json</code>, replacing <code>s3://share-with-kolena</code> with the appropriate bucket(s):</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Sid\": \"S3ListBucket\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::share-with-kolena\",\n            \"arn:aws:s3:::share-with-kolena/*\"\n        ]\n    }]\n}\n</code></pre> <p>Note: Bucket names</p> <p>Please note that bucket names must follow S3 naming rules</p> <p>Next, create the policy and attach the policy to the service user created in step 1:</p> <code>MinIO</code> <pre><code>mc admin policy create &lt;deployment_alias&gt; kolenaread /tmp/kolena-policy.json\nmc admin policy attach &lt;deployment_alias&gt; kolenaread --user &lt;kolena_user&gt;\n</code></pre>"},{"location":"advanced-usage/connecting-cloud-storage/s3-compatible/#step-3-save-integration-on-kolena","title":"Step 3: Save Integration on Kolena","text":"<p>Return to the Kolena platform Integrations tab.</p> <p>By default, any locators beginning with <code>s3://</code> will be loaded using this integration.</p> <p>Note: scoping integrations</p> <p>Optionally, each integration can be scoped to a specific bucket such that only locators of the pattern <code>s3://&lt;specific-bucket&gt;/*</code> will be loaded using the integration. This can be necessary if multiple integrations are required. Unchecking \"Apply to all buckets by default?\" and specifying a bucket will enable this behavior.</p> <p>Fill in the fields for the integration and then click \"Save\".</p> Field Description Access Key Id The username (<code>&lt;kolena_user&gt;</code>) of the user created in step 1 Secret Access Key The secret key (<code>&lt;secret_access_key&gt;</code>) of the user created in step 1 Endpoint The hostname or IP address of your S3-compatible service Port The optional port to access your S3-compatible service Region The region your buckets will be accessed from"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>In this section, we'll get acquainted with the core concepts on Kolena, and learn in-depth about the various features offered. For a brief introduction, see the Quickstart Guide or the Building a Workflow tutorial. For code-level API documentation, see the API Reference Documentation for the <code>kolena</code> Python client.</p> <ul> <li> <p>  Workflow</p> <p>Testing in Kolena is broken down by the type of ML problem you're solving, called a workflow. Any ML problem that can be tested can be modeled as a workflow in Kolena.</p> </li> </ul> <ul> <li> <p>  Test Cases &amp; Test Suites</p> <p>Test cases and test suites are used to organize test data in Kolena.</p> </li> </ul> <ul> <li> <p>  Models</p> <p>In Kolena, a model is a deterministic transformation from test samples to inferences.</p> </li> </ul>"},{"location":"core-concepts/model/","title":"Model","text":"<p>In Kolena, a model is a deterministic transformation from test samples to inferences.</p> <p>Kolena only stores metadata associated with your model in its   Models registry. Models themselves \u2014 their code or their weights \u2014 are never uploaded to Kolena, only the inferences from models.</p> <p>Models are considered black boxes, which makes Kolena agnostic to the underlying framework and architecture. It's possible to test any sort of model, from deep learning to rules-based, on Kolena.</p>","boost":2},{"location":"core-concepts/model/#creating-models","title":"Creating Models","text":"<p>The <code>Model</code> class is used to programmatically create models for testing. Rather than importing the class from <code>kolena.workflow</code> directly, use the <code>Model</code> definition returned from <code>define_workflow</code> bound to the test sample and inference types for your workflow:</p> <pre><code>from kolena.workflow import define_workflow\n\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n\n*_, Model = define_workflow(\"My Workflow\", MyTestSample, MyGroundTruth, MyInference)\n</code></pre> <p>With this class, models can be created, loaded, and updated:</p> <pre><code>my_model = Model(\"example-model\")\n</code></pre>","boost":2},{"location":"core-concepts/model/#implementing-infer","title":"Implementing <code>infer</code>","text":"<p>To test a model using the <code>test</code> method, a <code>Model.infer</code> implementation must be provided. <code>infer</code> is where the model itself \u2014 the deterministic transformation from test sample to inference \u2014 lives.</p> <pre><code># in practice, use TestSample and Inference types from your workflow\nfrom kolena.workflow import TestSample, Inference\n\ndef infer(test_sample: TestSample) -&gt; Inference:\n    ...\n</code></pre> <p>When running a model live, this function usually involves loading the image/document/etc. from the <code>TestSample</code>, passing it to your model, and constructing an <code>Inference</code> object from the model outputs. When loading results from e.g. a CSV, this function is often just a lookup.</p> Example: Loading inferences from CSV <p>This example considers a classification workflow using the <code>Image</code> test sample type and the following inference type:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\n\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import ScoredClassificationLabel\n\n@dataclass(frozen=True)\nclass MyInference(Inference):\n    # use Optional to accommodate missing inferences\n    prediction: Optional[ScoredClassificationLabel] = None\n</code></pre> <p>With inferences stored in an <code>inferences.csv</code> with the <code>locator</code>, <code>label</code> and <code>score</code> columns, implementing <code>infer</code> as a lookup is straightforward:</p> <pre><code>import pandas as pd\nfrom kolena.workflow import Image\n\nfrom my_workflow import MyInference\n\ninference_by_locator = {\n    record.locator: MyInference(prediction=ScoredClassificationLabel(\n        label=record.label,\n        score=record.score,\n    )) for record in pd.read_csv(\"inferences.csv\").itertuples()\n}\n\ndef infer(test_sample: Image) -&gt; MyInference:\n    return inference_by_locator.get(test_sample.locator, MyInference())\n</code></pre> <p>Note: Ensure that models are deterministic</p> <p>To preserve reproducibility, ensure that models tested in Kolena are deterministic.</p> <p>This is particularly important for generative models. If your model has a random seed parameter, consider including the random seed value used for testing as a piece of metadata attached to the model.</p>","boost":2},{"location":"core-concepts/model/#metadata","title":"Metadata","text":"<p>When creating a model, you have the option to specify free-form <code>metadata</code> to associate with the model. This metadata can be useful to track relevant information about the model, such as:</p> <ul> <li>Framework (e.g. PyTorch, TensorFlow, custom, etc.) and version used</li> <li>Person who trained the model, e.g. <code>name@company.ai</code></li> <li>GitHub branch, file, or commit hash used to run the model</li> <li>Links to your experimentation tracking system</li> <li>Free-form notes about methodology or observations</li> <li>Location in e.g. S3 where the model's weights are stored</li> <li>Training dataset specifier or URL</li> <li>Hyperparameters applied during training</li> </ul> <p>Metadata can be specified on the command line or edited on the web on the   Models page.</p>","boost":2},{"location":"core-concepts/model/#faq-best-practices","title":"FAQ &amp; Best Practices","text":"How should models be named? <p>Two factors influence model naming:</p> <ol> <li>A model's name is unique, and</li> <li>A model is deterministic.</li> </ol> <p>This means that anything that may change your model's outputs, such as environment or packaging, should be tracked as a new model! We recommend storing a variety of information in the model name, for example:</p> <ul> <li>Model architecture, e.g. <code>YOLOR-D6</code></li> <li>Input size, e.g. <code>1280x1280</code></li> <li>Framework, e.g. <code>pytorch-1.7</code></li> <li>Additional tracking information, such as its name in Weights &amp; Biases, e.g. <code>helpful-meadow-5</code></li> </ul> <p>An example model name may therefore be:</p> <pre><code>helpful-meadow-5 (YOLOR-D6, 1280x1280, pytorch-1.7)\n</code></pre> <p>Model names can be edited on the web on the   Models page.</p>","boost":2},{"location":"core-concepts/test-suite/","title":"Test Case &amp; Test Suite","text":"<p>Test cases and test suites are used to organize test data in Kolena.</p> <p>A test case is a collection of test samples and their associated ground truths. Test cases can be thought of as benchmark datasets, or slices of a benchmark dataset.</p> <p>A test suite is a collection of test cases. Models are tested on test suites.</p> <p>Test cases and test suites are found on the   Test Suites page on Kolena.</p>","boost":2},{"location":"core-concepts/test-suite/#managing-test-cases-test-suites","title":"Managing Test Cases &amp; Test Suites","text":"<p>The <code>TestCase</code> and <code>TestSuite</code> classes are used to programmatically create test cases and test suites. Rather than importing these classes from <code>kolena.workflow</code> directly, Use the definitions returned from <code>define_workflow</code> bound to the test sample and ground truth types for your workflow:</p> <pre><code>from kolena.workflow import define_workflow\n\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n\n_, TestCase, TestSuite, _ = define_workflow(\n    \"My Workflow\",\n    MyTestSample,\n    MyGroundTruth,\n    MyInference,\n)\n</code></pre> <p>These classes can then be used to create, load, and edit test cases and test suites:</p> Test CaseTest Suite <p>Create using <code>TestCase.create</code>:</p> <pre><code># throws if a test case with name 'example-test-case' already exists\ntest_case = TestCase.create(\n    \"example-test-case\",\n    # optionally include list of test samples and ground truths to populate the new test case\n    # test_samples=[(ts0, gt0), (ts1, gt1), (ts2, gt2)],\n)\n</code></pre> <p>Load using <code>TestCase.load</code>:</p> <pre><code># throws if a test case with name 'example-test-case' does not exist\ntest_case = TestCase.load(\"example-test-case\")\n</code></pre> <p>Use the <code>TestCase</code> constructor for idempotent create/load behavior:</p> <pre><code># loads 'example-test-case' or creates it if it does not already exist\ntest_case = TestCase(\"example-test-case\")\n</code></pre> <p>Use <code>TestCase.init_many</code> to initialize multiple test cases at once:</p> <pre><code># loads test cases or creates them if they do not already exist\ntest_cases = TestCase.init_many([\n    (\"test case 1\", [(test_sample_0, ground_truth_0), (test_sample_1, ground_truth_1)]),\n    (\"test case 2\", [(test_sample_2, ground_truth_2), (test_sample_3, ground_truth_3)])\n])\n\n# With 'reset=True', test cases that already exist would be updated with the new test_samples and ground_truths\ntest_cases = TestCase.init_many([\n    (\"test case 1\", [(test_sample_0, ground_truth_0), (test_sample_1, ground_truth_1)]),\n    (\"test case 2\", [(test_sample_2, ground_truth_2), (test_sample_3, ground_truth_3)])\n], reset=True)\n</code></pre> <p>Test cases can be edited using the context-managed <code>Editor</code> interface:</p> <pre><code>with TestCase(\"example-test-case\").edit(reset=True) as editor:\n    # perform desired editing actions within context\n    editor.add(ts0, gt0)\n</code></pre> <p>Create using <code>TestSuite.create</code>:</p> <pre><code># throws if a test suite with name 'example-test-suite' already exists\ntest_suite = TestSuite.create(\n    \"example-test-suite\",\n    # optionally include list of test cases to populate the new test suite\n    # test_cases=[test_case0, test_case1, test_case2],\n)\n</code></pre> <p>Load using <code>TestSuite.load</code>:</p> <pre><code># throws if a test suite with name 'example-test-suite' does not exist\ntest_suite = TestSuite.load(\"example-test-suite\")\n</code></pre> <p>Use the <code>TestSuite</code> constructor for idempotent create/load behavior:</p> <pre><code># loads 'example-test-suite' or creates it if it does not already exist\ntest_suite = TestSuite(\"example-test-suite\")\n</code></pre> <p>Test suites be edited using the context-managed <code>Editor</code> interface:</p> <pre><code>with TestSuite(\"example-test-suite\").edit() as editor:\n    editor.add(test_case_a)\n    editor.remove(test_case_b)\n    # perform desired editing actions within context\n</code></pre>","boost":2},{"location":"core-concepts/test-suite/#versioning","title":"Versioning","text":"<p>All test data on Kolena is versioned and immutable<sup>1</sup>. Previous versions of test cases and test suites are always available and can be visualized on the web and loaded programmatically by specifying a version.</p> <pre><code># load a specific version of a test suite\ntest_suite_v2 = TestSuite.load(\"example-name\", version=2)\n</code></pre>","boost":2},{"location":"core-concepts/test-suite/#faq-best-practices","title":"FAQ &amp; Best Practices","text":"How should I map my existing benchmark into test cases and test suites? <p>To start, create a test suite containing a single test case for the complete benchmark. This single-test-case test suite represents standard, aggregate evaluation on a benchmark dataset.</p> <p>Once this test suite has been created, you can start creating test cases! Use the Studio, the Stratifier, or the Python client to create test cases slicing through (stratifying) this benchmark.</p> How many test cases should a test suite include? <p>While test suites can hold anywhere from one to thousands of test cases, the sweet spot for the signal-to-noise ratio is in the dozens or low hundreds of test cases per test suite.</p> <p>Note that the relationship between benchmark dataset and test suite doesn't need to be 1:1. Often it can be useful to create different test suites for different stratification strategies applied to the same benchmark.</p> How many samples should be included in a test case? <p>While there's no one-size-fits-all answer, we usually recommend including at least 100 samples in each test case. Smaller test cases can be used to provide a very rough signal about the presence or absence of a model beahvior, but shouldn't be relied upon for much more than a directional indication of performance.</p> <p>The multi-model Results comparison view in Kolena takes the number of test samples within a test case into account when highlighting improvements and regressions. The larger the test case, the smaller the \u2206 required to consider a change from one model to another as \"significant.\"</p> How many negative samples should a test case include? <p>Many workflows, such as object detection or binary classification, have a concept of \"negative\" samples. In object detection, a \"negative sample\" is a sample (i.e. image) that does not include any objects to be detected.</p> <p>Negative samples can have a large impact on certain metrics. To continue with the object detection example, the precision metric depends on the number of false positive detections:</p> \\[ \\text{Precision} := \\dfrac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}} \\] <p>Therefore, since each negative sample has some likelihood of yielding false positive detections but no likelihood of yielding true positive detections, adding negative samples to a test case may decrease aggregate precision values computed across the test case.</p> <p>As a general rule of thumb, we recommend including an even balance of positive and negative samples in each test case. This composition minimizes the likelihood of different metrics being heavily skewed in one direction or another.</p> <ol> <li> <p>Immutability caveat: test suites, along with any test cases and test samples they hold, can be deleted on the   Test Suites page.\u00a0\u21a9</p> </li> </ol>","boost":2},{"location":"core-concepts/workflow/","title":"Workflow","text":"<p>Testing in Kolena is broken down by the type of ML problem you're solving, called a workflow. Any ML problem that can be tested can be modeled as a workflow in Kolena.</p> <p>Examples of workflows include:</p> <ul> <li>  Object Detection (2D) using images</li> <li> array-string  Text Summarization using articles/documents</li> <li>  Age Estimation (regression) using images</li> <li>  Video Retrieval using text queries on a corpus of videos</li> </ul> <p>With the <code>kolena.workflow</code> client module, any arbitrary ML problem can be defined as a workflow and tested on Kolena.</p> <p>There are three main components of a workflow:</p> <p>Info</p> <p>These three types can be thought of as the data model, or the schema, of a workflow.</p> <ol> <li>Test Sample: the inputs to a model, e.g. image, video, document</li> <li>Ground Truth: the expected model outputs</li> <li>Inference: the actual model outputs</li> </ol>","boost":2},{"location":"core-concepts/workflow/#test-sample","title":"Test Sample","text":"<p>In Kolena, \"test sample\" is the general term for the input to a model.</p> <p>For standard computer vision (CV) models, the test sample is often a single image. Video-based computer vision models would have a video test sample type, and stereo vision models would use image pairs. For natural language processing models, the test sample may be a document or text snippet.</p> <p>When building a workflow, you can extend and compose these base test sample types as necessary, or use the base types directly if no customization is required.</p>","boost":2},{"location":"core-concepts/workflow/#metadata","title":"Metadata","text":"<p>Any additional information associated with a test sample, e.g. details about how it was collected, can be included as metadata. We recommend uploading any and all metadata that you have available, as metadata can be useful for searching through data in the Studio, interpreting model results, and creating new test cases.</p> <pre><code>from dataclasses import dataclass, field\n\nfrom kolena.workflow import Document, Metadata\n\n@dataclass(frozen=True)\nclass MyDocument(Document):\n    # locator: str  # inherited from parent Document\n    doc_id: int  # example of a field that is explicitly required\n    metadata: Metadata = field(default_factory=dict)  # free-form, optional metadata\n</code></pre> <p>Use <code>pydantic</code> dataclasses</p> <p>When building a workflow, object definitions can us standard library <code>dataclasses</code> or Pydantic <code>dataclasses</code>. Pydantic brings helpful runtime type validation and coercion and can be used as a drop-in replacement for standard library <code>dataclasses</code>.</p>","boost":2},{"location":"core-concepts/workflow/#composite-test-samples","title":"Composite Test Samples","text":"<p>Kolena is not prescriptive about the shape of your ML problem. Test samples can be composed, using the <code>Composite</code> test sample type, to mirror the shape of your problem directly.</p> <p>Consider the example of an autonomous vehicle application that uses four cameras, one for each of the <code>front</code>, <code>right</code>, <code>rear</code>, and <code>left</code> views:</p> <pre><code>from dataclasses import dataclass\n\nfrom kolena.workflow import Composite, Image\n\n@dataclass(frozen=True)\nclass QuadImage(Composite):\n    front: Image\n    right: Image\n    rear: Image\n    left: Image\n</code></pre> How can I specify annotations on <code>Composite</code> test samples? <p>Image-level (or video-level, document-level, etc.) annotations can be specified when using composite test samples. To specify image-level objets in each of the four images, ground truth or inference definitions may look like this:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\n\nfrom kolena.workflow import DataObject, GroundTruth\nfrom kolena.workflow.annotation import BoundingBox\n\n@dataclass(frozen=True)\nclass SingleImageGroundTruth(DataObject):\n    objects: List[BoundingBox]\n\n@dataclass(frozen=True)\nclass QuadImageGroundTruth(GroundTruth):\n    # attribute names matches attribute names in test sample\n    front: SingleImageGroundTruth\n    right: SingleImageGroundTruth\n    rear: SingleImageGroundTruth\n    left: SingleImageGroundTruth\n</code></pre>","boost":2},{"location":"core-concepts/workflow/#ground-truth","title":"Ground Truth","text":"<p>The ground truth represents the expected output from a model when provided with a test sample. Ground truths are often manually annotated and are used to determine the correctness of model predictions.</p> <p>In the   Studio, ground truths are always displayed alongside their paired test samples. Any annotations, such as bounding boxes or polygons, are visualized on top of the test sample.</p> <p>The contents of a ground truth are driven by the requirements of the workflow. Take this example for a multiclass object detection workflow:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\n\nfrom kolena.workflow import GroundTruth\nfrom kolena.workflow.annotation import LabeledBoundingBox\n\n@dataclass(frozen=True)\nclass MyGroundTruth(GroundTruth):\n    objects: List[LabeledBoundingBox]\n</code></pre> Where should additional information that isn't used for model evaluation live? <p>We recommend scoping the ground truth to only the data required for model evaluation. Any additional metadata, annotations, or assets associated with a test sample can be included as a part of the test sample itself or in its free-form metadata.</p> <p>However, it isn't a strict requirement that ground truths only contain information used for model evaluation. Sometimes it makes sense to include additional information as optional fields inside a ground truth definition.</p>","boost":2},{"location":"core-concepts/workflow/#inference","title":"Inference","text":"<p>A workflow's inference type contains the actual output produced by a model when given a test sample. Inferences are also referred to as \"raw inferences,\" as they represent the raw output from a model.</p> <p>The inference type and ground truth type for a workflow will often look very similar to one another.</p>","boost":2},{"location":"core-concepts/workflow/#extending-annotation-types","title":"Extending Annotation Types","text":"<p>Annotation types can be extended to include additional fields, when necessary.</p> <p>Consider the example of a <code>Keypoints</code> detection model that detects anywhere from 0 to N keypoints arrays when provided an image. Each keypoints array has an associated class label and confidence value. This model's inference type could be defined as follows:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\n\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import Keypoints\n\n@dataclass(frozen=True)\nclass ScoredLabeledKeypoints(Keypoints):\n    # points: List[Tuple[float, float]]  # inherited from Keypoints\n    score: float  # confidence score, between 0 and 1\n    label: str  # predicted class\n\n@dataclass(frozen=True)\nclass MyInference(Inference):\n    predictions: List[ScoredLabeledKeypoints]\n</code></pre>","boost":2},{"location":"core-concepts/workflow/#deduplication","title":"Deduplication","text":"<p>Models are considered deterministic inputs from test samples to inferences. This means that, when testing in Kolena, a given model only needs to process a given test sample once. Kolena uses this to speed up the process of running tests, ensuring that compute cycles are not wasted processing a given test sample multiple times when test samples exist in multiple test cases.</p> <p>When calling <code>test</code>, only samples that do not already have inferences uploaded from the given model will be processed. To change this behavior and re-process all test samples, regardless of any uploaded inferences, use the <code>reset</code> flag:</p> <pre><code># all test samples are processed and inferences [re]uploaded when reset=True\ntest(model, test_suite, evaluator, reset=True)\n</code></pre>","boost":2},{"location":"core-concepts/workflow/#defining-a-workflow","title":"Defining a Workflow","text":"<p>With test sample, ground truth, and inference types declared, defining a workflow provides the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> definitions to use when creating tests and testing models with this workflow:</p> <pre><code>from kolena.workflow import define_workflow\n\nfrom my_workflow import MyTestSample, MyGroundTruth, MyInference\n\n_, TestCase, TestSuite, Model = define_workflow(\n    \"My Example Workflow\",\n    MyTestSample,\n    MyGroundTruth,\n    MyInference,\n)\n</code></pre>","boost":2},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This page answers common questions about Kolena and how to use it to test ML models.</p> <p>If you don't see your question here, please reach out to us on Slack or at contact@kolena.io!</p>"},{"location":"faq/#about-kolena","title":"About Kolena","text":"What data types does Kolena support? <p>Testing in Kolena is fully customizable and supports computer vision, natural language processing, and structured data (tabular, time series) machine learning models. This includes images, documents, videos, 3D models and point clouds, and more.</p> <p>See the available data types in <code>kolena.workflow.TestSample</code>, and the available annotation types in <code>kolena.workflow.annotation</code>.</p> <p>We're constantly adding new data types and annotation types \u2014 if you don't see what you're looking for, reach out to us and we'll happily extend our system to support your use case.</p> Do I have to upload my datasets to Kolena? <p>No. Kolena doesn't store your data (images, videos, documents, 3D assetes, etc.) directly, only URLs pointing to the right location in a cloud bucket or internal infrastructure that you own.</p> <p>While onboarding your team, we'll discuss what access restrictions are necessary for your data and select the right integration solution. As one example, as a part of the integration we might restrict access to files registered with Kolena to only users on your corporate VPN.</p> <p>We support a variety of integration patterns depending on your organization's requirements and security stance. Get in touch with us to discuss details!</p> Do I have to upload my models to Kolena? <p>No. Tests are always run in your environment using the <code>kolena</code> Python client, and you never have to package or upload models to Kolena.</p> Where does Kolena fit into the MLOps development life cycle? <p>Kolena is primarily a testing (or \"offline evaluation\") platform, coming after training and before deployment. We believe that increased emphasis on this offline evaluation segment of the model development life cycle can save effort upstream in the data collection and training process as well as prevent headaches downstream in deployment.</p>"},{"location":"faq/#using-kolena","title":"Using Kolena","text":"How do I generate an API token? <p>Generate an API token by visiting the   Developer page, located at the bottom of the lefthand sidebar, then copy/paste the shell snippet to set this token as <code>KOLENA_TOKEN</code> in your environment.</p> How many API tokens can I generate? <p>API tokens are scoped to your username. Each user is limited to one valid token at a time \u2014 generating a new token on the   Developer page invalidates any previous token generated for your user.</p> <p>To retrieve a service user API token that is not scoped to a specific username, please reach out to us on Slack or at contact@kolena.io.</p> How can I add new users to my organization? <p>Administrators for your organization can add new users and grant users administrator privileges by visiting the   Organization Settings page and adding entries to the Authorized Users table.</p> <p>Note that this page is only visible for organization administrators.</p> Who are administrators for my organization? <p>Certain members of each organization have administrator privileges. These administrators can manage users access and privileges, as well as configure Integrations.</p> <p>You may become an administrator by having an existing administrator grant you privileges using the   Organization Settings page.</p> I'm new to Kolena \u2014 how can I learn more about the platform and how to use it? <p>On each page, there is a button with the   icon next to the page title. Click on this button to bring up a detailed tutorial explaining the contents of the current page and how it's used.</p> How can I report a bug? <p>If you encounter a bug when using the <code>kolena</code> Python client or when using app.kolena.io, message us on Slack, email your support representative or contact@kolena.io, or open an issue on the <code>kolena</code> repository for Python-client-related issues.</p> <p>Please include any relevant stacktrace or platform URL when reporting an issue.</p>"},{"location":"faq/#troubleshooting-errors","title":"Troubleshooting Errors","text":"I'm seeing the error: AttributeError: type object XXX has no attribute '__pydantic_model__' <p>If you see error message like this, please check that <code>pydantic &lt; 2.0</code> is installed. Kolena is not currently compatible with <code>pydantic</code> V2. For more information, refer to <code>kolena</code> Installation.</p>"},{"location":"metrics/","title":"Metrics Glossary","text":"<p>This section contains guides for different metrics used to measure model performance.</p> <p>Each ML use case requires different metrics. Using the right metrics is critical for understanding and meaningfully comparing model performance. In each metrics guide, you can learn about the metric with examples, its limitations and biases, and its intended uses.</p> <ul> <li> <p>Accuracy</p> <p>Accuracy measures how well a model predicts correctly. It's a good metric for assessing model performance in simple cases with balanced data.</p> </li> <li> <p>Average Precision (AP)</p> <p>Average precision summarizes a precision-recall (PR) curve into a single threshold-independent value representing model's performance across all thresholds.</p> </li> <li> <p>Averaging Methods: Macro, Micro, Weighted</p> <p>Different averaging methods for aggregating metrics for multiclass workflows, such as classification and object detection.</p> </li> <li> <p>Confusion Matrix</p> <p>Confusion matrix is a structured plot describing classification model performance as a table that highlights counts of objects with predicted classes (columns) against the actual classes (rows), indicating how confused a model is.</p> </li> <li> <p>F<sub>1</sub>-score</p> <p>F<sub>1</sub>-score is a metric that combines two competing metrics, precision and recall with an equal weight. It symmetrically represents both precision and recall as one metric.</p> </li> <li> <p>False Positive Rate (FPR)</p> <p>False positive rate (FPR) measures the proportion of negative ground truths that a model incorrectly predicts as positive, ranging from 0 to 1. It is useful when the objective is to measure and reduce false positive inferences.</p> </li> <li> <p>Precision</p> <p>Precision measures the proportion of positive inferences from a model that are correct. It is useful when the objective is to measure and reduce false positive inferences.</p> </li> <li> <p>Precision-Recall (PR) Curve</p> <p>Precision-recall curve is a plot that gauges machine learning model performance by using precision and recall. It is built with precision on the y-axis and recall on the x-axis computed across many thresholds.</p> </li> <li> <p>Recall (TPR, Sensitivity)</p> <p>Recall, also known as true positive rate (TPR) and sensitivity, measures the proportion of all positive ground truths that a model correctly predicts. It is useful when the objective is to measure and reduce false negative ground truths, i.e. model misses.</p> </li> <li> <p>Receiver Operating Characteristic (ROC) Curve</p> <p>A receiver operating characteristic (ROC) curve is a plot that is used to evaluate the performance of binary classification models by using the true positive rate (TPR) and the false positive rate (FPR).</p> </li> <li> <p>Specificity (TNR)</p> <p>Specificity, also known as true negative rate (TNR), measures the proportion of negative ground truths that a model correctly predicts, ranging from 0 to 1. It is useful when the objective is to measure the model's ability to correctly identify the negative class instances.</p> </li> <li> <p>TP / FP / FN / TN</p> <p>The counts of TP, FP, FN and TN ground truths and inferences are essential for summarizing model performance. They are the building blocks of many other metrics, including accuracy, precision, and recall.</p> </li> </ul>"},{"location":"metrics/#computer-vision","title":"Computer Vision","text":"<ul> <li> <p>Geometry Matching</p> <p>Geometry matching is the process of matching inferences to ground truths for computer vision workflows with a localization component. It is a core building block for metrics such as TP, FP, and FN, and any metrics built on top of these, like precision, recall, and F<sub>1</sub>-score.</p> </li> <li> <p>Intersection over Union (IoU)</p> <p>IoU measures overlap between two geometries, segmentation masks, sets of labels, or time-series snippets. Also known as Jaccard index in classification workflow.</p> </li> </ul>"},{"location":"metrics/#natural-language-processing","title":"Natural Language Processing","text":"<ul> <li> <p>BERTScore</p> <p>BERTScore is a metric used in NLP workflows to measure textual similarity between candidate texts and reference texts.</p> </li> <li> <p>BLEU</p> <p>BLEU is a metric commonly used in a variety of NLP workflows to evaluate the quality of candidate texts. BLEU can be thought of as an analog to precision for text comparisons.</p> </li> <li> <p>METEOR</p> <p>METEOR is a widely recognized and vital metric used in NLP. It is used to measure the quality of candidate texts against reference texts. Though it is an n-gram based metric, it goes beyond traditional methods by factoring in elements such as precision, recall, and order to provide a comprehensive measure of text quality.</p> </li> <li> <p>ROUGE-N</p> <p>ROUGE-N, a metric within the broader ROUGE metric collection, is a vital metric in the field of NLP. It assesses the quality of a candidate text by measuring the overlap of n-grams between the candidate text and reference texts. ROUGE-N can be thought of as an analog to recall for text comparisons.</p> </li> </ul>"},{"location":"metrics/accuracy/","title":"Accuracy","text":"<p>Accuracy is one of the most well-known metrics in machine learning model evaluation because it is simple to understand and straightforward to calculate.</p> <p>Accuracy measures how often a model correctly predicts something (ranging from 0 to 1, with 1 being perfect inferences). It reports the ratio of the number of correct inferences to the total number of inferences, making it a good metric for assessing model performance in simple cases with balanced data. However, accuracy is much less meaningful with imbalanced datasets (e.g. far more negative ground truths than positive ground truths) and should be used with caution.</p> <ul> <li>  API Reference: <code>accuracy</code> \u2197</li> </ul>"},{"location":"metrics/accuracy/#implementation-details","title":"Implementation Details","text":"<p>Accuracy is generally used to evaluate classification models. Aside from classification, accuracy is also often used to evaluate semantic segmentation models by measuring the percent of correctly classified pixels in an image.</p> <p>In a classification workflow, accuracy is the ratio of the number of correct inferences to the total number of inferences.</p> <p>With TP / FP / FN / TN counts computed, accuracy is defined:</p> \\[ \\text{Accuracy} =  \\frac {\\text{TP} + \\text{TN}} {\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}} \\]"},{"location":"metrics/accuracy/#examples","title":"Examples","text":"<p>Perfect inferences:</p> Metric Value TP 10 FP 0 FN 0 TN 10 \\[ \\begin{align} \\text{Accuracy} &amp;= \\frac{10 + 10}{10 + 0 + 0 + 10} \\\\[1em] &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences:</p> Metric Value TP 8 FP 4 FN 2 TN 6 \\[ \\begin{align} \\text{Accuracy} &amp;= \\frac{8 + 6}{8 + 4 + 2 + 6} \\\\[1em] &amp;= 0.7 \\end{align} \\] <p>Highly imbalanced data, with 990 negative ground truths and 10 positive ground truths, with no positive inferences:</p> Metric Value TP 0 FP 0 FN 10 TN 990 \\[ \\begin{align} \\text{Accuracy} &amp;= \\frac{0 + 990}{0 + 0 + 10 + 990} \\\\[1em] &amp;= 0.99 \\end{align} \\] <p>Be careful with imbalanced datasets!</p> <p>This example describes a trivial model that only ever returns negative inferences, yet it has the high accuracy score of 99%.</p>"},{"location":"metrics/accuracy/#limitations-and-biases","title":"Limitations and Biases","text":"<p>While accuracy generally describes a classifier\u2019s performance, it is important to note that the metric can be deceptive, especially when the data is imbalanced.</p> <p>For example, let\u2019s say there are a total of 500 ground truths, with 450 belonging to the positive class and 50 to the negative. If the model correctly predicts all the positive ground truths but misses all the negative ones, its accuracy is <code>450 / 500 = 0.9</code>. An accuracy score of 90% indicates a pretty good model \u2014 but is a model that fails 100% of the time on negative ground truths useful? Using the accuracy metric alone can hide a model\u2019s true performance, so we recommend other metrics that are better suited for imbalanced data, such as:</p> <ul> <li>Balanced accuracy</li> <li>Precision</li> <li>Recall</li> <li>F<sub>1</sub>-score</li> </ul>"},{"location":"metrics/average-precision/","title":"Average Precision","text":"<p>Average precision (AP) summarizes a precision-recall (PR) curve into a single value representing the average of all precisions. It is generally understood as the approximation of the area under the PR curve. AP ranges between 0 and 1, where a perfect model has precision, recall, and AP scores of 1. The larger the metric, the better a model performs across different thresholds.</p> <p>Guides: Precision and Recall</p> <p>Read the precision and the recall guides if you're not familiar with those metrics.</p> <p>Unlike metrics like precision, recall, and F<sub>1</sub>-score, which are threshold-dependent where a confidence threshold value must be defined to compute them, AP is a key performance threshold-independent metric that removes the dependency of selecting one confidence threshold value and measures a model's performance across all thresholds.</p> <p>AP is commonly used to evaluate the performance of object detection and information retrieval workflows. This metric (or an aggregated version of it called mean average precision (mAP)) is the primary metric used across popular object detection benchmarks such as PASCAL VOC 2012, COCO, and Open Images V7.</p>"},{"location":"metrics/average-precision/#implementation-details","title":"Implementation Details","text":"<p>The general definition of AP is finding the approximation of the area under the PR curve. The actual area under the curve, where \\(p(r)\\) is the precision at recall \\(r\\), can be defined:</p> \\[ \\text{AP} = \\int_{0}^{1} p(r)dr \\] <p>The integral above is in practice replaced with a finite sum over every unique recall value (or over a set of evenly spaced recall values) \u2014 different interpolation methods are discussed in the section below. The average precision over a set of recall values or over a range of thresholds at which we are evaluating the model can be defined:</p> \\[ AP = \\sum_{k=0}^{k=n-1}[r(k) - r(k+1)] * p(k) \\] <p>where</p> <ul> <li>\\(p(k)\\) is the precision at threshold \\(k\\)</li> <li>\\(r(k)\\) is the recall at threshold \\(k\\)</li> <li>\\(n\\) is the number of thresholds</li> </ul> <p>Let\u2019s take a closer look at different implementations of the AP metric. Two primary machine learning workflows that use AP as a main evaluation metric are object detection and information retrieval, but AP is implemented slightly differently for both.</p>"},{"location":"metrics/average-precision/#object-detection","title":"Object Detection","text":"<p>Let\u2019s consider the following simple example:</p> <p> </p> <p> </p> <p>The above three images show a total of four ground truth objects, all of which are matched with an inference bounding box based on the Intersection over Union (IoU) scores. Let\u2019s look at each inference bounding box and sort them by their confidence score in descending order.</p> Inference Confidence \u2193 TP/FP cumsum(TP) cumsum(FP) Precision Recall H 0.99 TP 1 0 1.0 0.2 A 0.88 TP 2 0 1.0 0.4 D 0.72 FP 2 1 0.667 0.4 B 0.70 FP 2 2 0.5 0.4 G 0.54 FP 2 3 0.4 0.4 E 0.54 TP 3 3 0.5 0.6 I 0.38 FP 4 3 0.571 0.8 C 0.2 FP 4 4 0.5 0.8 F 0.2 FP 4 5 0.444 0.8 J 0.1 TP 5 5 0.5 1.0 <p>Guides: TP/FP Counts in Object Detection</p> <p>Read the Intersection over Union (IoU), the Geometry Matching, and the TP / FP / FN / TN guides if you're not familiar with those terminologies.</p> <p>In order to compute AP, we first need to define precision and recall at each threshold. In this example, we are going to use every unique confidence score as threshold to calculate precision and recall metrics, so we have the complete list of unique recall values. Starting from the top, each inference is assigned to be either a true positive (TP) or false positive (FP) depending on the matching results \u2014 if the inference is matched with a ground truth, then it's a TP; otherwise, a FP. Notice that all inferences in this table are considered to be positive (either a TP or FP) because we are evaluating them at the thresholds equal to their confidence scores. Then, the cumulative sum of TP and FP counts respectively from the previous rows are computed at each row. Using these accumulated TP and FP counts, the precision and recall metrics can be defined at each threshold. We are using the cumulative sum because once again each row is evaluated at the threshold equal to its confidence score, so only the upper rows and the current row (i.e., inference with score greater than or equal to the threshold) count as positive inferences.</p> <p>Now that we have the precision and recall defined at each threshold, let\u2019s plot the PR curve:</p> <p> </p> <p>Notice the zigzag pattern, often referred to as \u201cwiggles\u201d \u2014 the precision goes down with FPs and goes up again with TPs as the recall increases. It is a common practice to first smooth out the wiggles before calculating the AP metric by taking the maximum precision value to its graphical right side of each recall value. This is why AP is called the approximation of the area under the PR curve. The interpolated precision at each recall is defined:</p> \\[ p_{interp}(r) = \\max_{\\hat{r} \\geq r}p(\\hat{r}) \\] <p>The PR curve is re-plotted using the interpolated precisions (see orange line in the plot below).</p> <p> </p> <p>The precisions (y-values) of the smoothed out curve, the orange line on the plot above, are monotonically decreasing. We\u2019re now ready to calculate AP, which is simply the area under the smoothed out curve:</p> <p>The start and the end of PR curve</p> <p>Notice the above PR curve doesn't start at zero recall. It is because there is no valid threshold that will result in zero recall. In order to ensure that the graph starts on the y-axis, the first point on the curve extends all the way to the y-axis. Similarly, the end of the PR curve doesn't always extend all the way to the recall value of 1. This is because not all the ground truths are matched. Unlike the start of the curve, the tail of the curve doesn't get extended when calculating AP.</p> \\[ \\begin{align} AP &amp;= ((0.4-0.0) \\times 1.0) + ((0.8 - 0.4) \\times 0.571) + ((1.0 - 0.8) \\times 0.5) \\\\[1em] &amp;= 0.7284 \\end{align} \\] <p>Smoothing</p> <p>Although smoothing is considered as the standard implementation of average precision, scikit-learn's average precision implementation does not smooth out the precisions as mentioned above.</p> <p>The example above computes AP at all unique recall values whenever the maximum precision value drops. This is the most precise implementation of the metric, used in popular benchmarks like the PASCAL VOC challenge since 2010. Prior to 2010, the PASCAL VOC challenge had a different implementation for the AP calculation where the 11 linearly spaced recall values from 0.0 to 1.0 were used instead of all unique recall values.</p>"},{"location":"metrics/average-precision/#11-point-interpolation","title":"11-point Interpolation","text":"<p>The 11-point interpolated AP was used in the PASCAL VOC until a new AP calculation, what's considered as the standard now, which was adopted in 2010. This interpolation uses the average of the maximum precision values for 11 linearly spaced recall values from 0.0 to 1.0:</p> <p> </p> <p>When the precisions at certain recall values become extremely small, they are exempted from the AP calculation. The intention of using this 11-point interpolation, according to the original paper, is as follows:</p> <p>The intention in interpolating the precision/recall curve in this way is to reduce the impact of the \u201cwiggles\u201d in the precision/recall curve, caused by small variations in the ranking of examples.</p> <p>However, this linearly interpolated method suffers from being less precise and not being able to measure the difference with low APs due to the approximation mentioned above. The COCO benchmark uses a linear interpolation method but with 101 recall values.</p>"},{"location":"metrics/average-precision/#information-retrieval","title":"Information Retrieval","text":"<p>Information retrieval is a machine learning workflow where the user provides a query, and the model returns a score that measures how similar each data is to the query to find the most relevant information from the database.</p> <p>Average precision is one of the popular metrics used in information retrieval workflow along with object detection workflow. However, the implementation used in information retrieval workflow is slightly different than the one from the section above. The formula of the metric is defined:</p> \\[ \\text{AP}(n) = \\frac 1 {\\text{GTP}} \\sum_k^{n}p(k) \\times rel(k) \\] <p>where</p> <ul> <li>\\(n\\) is the total number of data that you are interested in</li> <li>\\(\\text{GTP}\\) is the total number of positive ground truths</li> <li>\\(p(k)\\) is the precision at rank \\(k\\) data</li> <li>\\(rel(k)\\) is the relevance at rank \\(k\\) data (1 if the data is relevant, 0 otherwise)</li> </ul> <p>Let\u2019s consider the following example of retrieving similar images to the query from a database of images with different shapes and colors:</p> <p> </p> <p>The retrieved images are the complete list of images from the database that are ranked by their similarity scores, which are predicted from the model, where the left-most image is the most similar to the query image.</p> <p>From the retrieved images, the ones with a circle are the TPs, where \\(rel(k) = 1\\), and any other shapes are labeled as FPs, where \\(rel(k) = 0\\). Then by simply accumulating all the counts of the TPs in each rank, \\(p(k) \\times rel(k)\\) can be calculated at each rank.</p> <p>AP is the sum of all the relevant precisions over the total number of positive samples in the database, so in this example, AP becomes:</p> \\[ \\text{AP} = \\frac {(\\frac 1 1 + \\frac 0 2 + \\frac 0 3 + \\frac 2 4 + \\frac 3 5 + \\frac 0 6 + ... + 0)} 3 = 0.7 \\]"},{"location":"metrics/average-precision/#mean-average-precision-map","title":"Mean Average Precision (mAP)","text":"<p>The mean average precision (mAP) is simply the macro-average of the AP calculated across different classes for object detection workflow or across different queries for information retrieval workflow. It is important to note that some papers use AP and mAP interchangeably.</p>"},{"location":"metrics/average-precision/#limitations-and-biases","title":"Limitations and Biases","text":"<p>AP is a great metric that summarizes the PR curve into a single value. Instead of comparing models with a single value of precision and recall at one specific threshold, it lets you compare model performance at every threshold. Although this metric is very popular and commonly used in object detection and information retrieval workflows, it has some limitations. Let's make sure to understand these limitations before using the metric to compare your models.</p> AP is often overestimated. <p>To approximate the area under the curve, it is standard practice to take the maximum precision from the right side of the plot. By doing so, it overestimates the area under the curve.</p> AP cannot distinguish between very different-looking PR curves. <p>Consider the following three plots:</p> <p> </p> <p>These three plots show very different characteristic, but their APs are exactly the same for all of them. Thus, relying solely on the AP metric is not enough. We recommend plotting the PR curve along with the AP metric to better understand the behavior of your model.</p> AP is not confidence score sensitive. <p>AP uses confidence score to sort inferences, and as long as the sorted order is preserved, the distribution of confidence scores does not change the AP score. Therefore, predictions that have confidence scores within a very small range versus ones with scores that are nicely distributed from 0 to 1 can have the same AP as long as the order is preserved.</p> AP uses the interpolated PR curve. <p>As mentioned in the section above, there are many different ways of interpolating the PR curve. Depending on the granularity of the plot, the AP value can be different, so when comparing models using AP, we need to ensure that it is calculated using the same interpolation method.</p> AP is not a fair comparison for thresholded models where the tail part of the PR curve is missing. <p>It is pretty common for object detectors to filter out predictions with very small confidence scores. In such a scenario, the curve will be missing the tail part, but because the metric considers the entire recall domain, any curves that end early will result in a lower average precision score.</p> <p> </p> <p>The plot above shows PR curves of two models: one extending to the recall value of <code>1.0</code> and the other one extending only to <code>0.6</code>. Since a large portion of the area under the curve corresponds to the tail of the curve, model 2 scores a higher AP than model 1.</p>"},{"location":"metrics/averaging-methods/","title":"Averaging Methods","text":"<p>For multiclass workflows like classification or object detection, metrics such as precision, recall, and F<sub>1</sub>-score are computed per class. To compute a single value that represents model performance across all classes, these per-class scores need to be aggregated. There are a few different averaging methods for doing this, most notably:</p> <ul> <li>Macro: unweighted mean of all per-class scores</li> <li>Micro: global average of per-sample TP, FP, FN scores</li> <li>Weighted: mean of all per-class scores, weighted by sample sizes for each class</li> </ul>"},{"location":"metrics/averaging-methods/#example-multiclass-classification","title":"Example: Multiclass Classification","text":"<p>Let\u2019s consider the following multiclass classification metrics, computed across a total of 10 samples:</p> Class # Samples # True Positives # False Positives # False Negatives Precision Recall F1-score <code>Airplane</code> 3 2 1 1 0.67 0.67 0.67 <code>Boat</code> 1 1 3 0 0.25 1.0 0.4 <code>Car</code> 6 3 0 3 1.0 0.5 0.67 Total 10 6 4 4 - - -"},{"location":"metrics/averaging-methods/#macro-average","title":"Macro Average","text":"<p>Macro average is perhaps the most straightforward among the numerous options and is computed by taking an unweighted mean of all the per-class scores:</p> \\[ \\begin{align} \\text{F}_{1 \\, \\text{macro}} &amp;= \\frac{\\text{F}_{1 \\, \\texttt{Airplane}} + \\text{F}_{1 \\, \\texttt{Boat}} + \\text{F}_{1 \\, \\texttt{Car}}}{3} \\\\[1em] &amp;= \\frac{0.67 + 0.4 + 0.67}{3} \\\\[1em] &amp;= 0.58 \\end{align} \\]"},{"location":"metrics/averaging-methods/#micro-average","title":"Micro Average","text":"<p>In contrast to macro, micro average computes a global average by counting the sums of true positive (TP), false negative (FN) and false positive (FP).</p> <p>Micro precision and micro recall are computed with the standard precision and recall formulas, using the total TP/FP/FN counts across all classes:</p> \\[ \\begin{align} \\text{Precision}_\\text{micro} &amp;= \\frac{\\text{TP}_\\text{Total}}{\\text{TP}_\\text{Total} + \\text{FP}_\\text{Total}} \\\\[1em] &amp;= \\frac{6}{6 + 4} \\\\[1em] &amp;= 0.6 \\end{align} \\] \\[ \\begin{align} \\text{Recall}_\\text{micro} &amp;= \\frac{\\text{TP}_\\text{Total}}{\\text{TP}_\\text{Total} + \\text{FN}_\\text{Total}} \\\\[1em] &amp;= \\frac{6}{6 + 4} \\\\[1em] &amp;= 0.6 \\end{align} \\] <p>What about micro F<sub>1</sub>? Plug the micro-averaged values for precision and recall into the standard formula for F<sub>1</sub>-score:</p> \\[ \\begin{align} \\text{F}_{1 \\, \\text{micro}} &amp;= 2 \\times \\frac{\\text{Precision}_\\text{micro} \\times \\text{Recall}_\\text{micro}}{\\text{Precision}_\\text{micro} + \\text{Recall}_\\text{micro}} \\\\[1em] &amp;= 2 \\times \\frac{0.6 \\times 0.6}{0.6 + 0.6} \\\\[1em] &amp;= 0.6 \\end{align} \\] <p>Note that precision, recall, and F<sub>1</sub>-score all have the same value: \\(0.6\\). This is because micro-averaging essentially computes the proportion of correctly classified instances out of all instances, which is the definition of overall accuracy.</p> <p>In the multiclass classification cases where each sample has a single label, we get the following:</p> \\[ \\text{F}_{1 \\, \\text{micro}} = \\text{Precision}_\\text{micro} = \\text{Recall}_\\text{micro} = \\text{Accuracy} \\]"},{"location":"metrics/averaging-methods/#weighted-average","title":"Weighted Average","text":"<p>Weighted average computes the mean of all per-class scores while considering each class\u2019s support. In this case, support is the number of actual instances of the class in the dataset.</p> <p>For example, if there are 3 samples of class <code>Airplane</code>, then the support value of class <code>Airplane</code> is 3. In other words, support is the sum of true positive (TP) and false negative (FN) counts. The weight is the proportion of each class\u2019s support relative to the sum of all support values:</p> \\[ \\begin{align} \\text{F}_{1 \\, \\text{weighted}} &amp;= \\left( \\text{F}_{1 \\, \\texttt{Airplane}} \\times \\tfrac{\\text{#}\\ \\texttt{Airplane}}{\\text{# Total}} \\right) + \\left( \\text{F}_{1 \\, \\texttt{Boat}} \\times \\tfrac{\\text{#}\\ \\texttt{Boat}}{\\text{# Total}} \\right) + \\left( \\text{F}_{1 \\, \\texttt{Car}} \\times \\tfrac{\\text{#}\\ \\texttt{Car}}{\\text{# Total}} \\right) \\\\[1em] &amp;= \\left( 0.67 \\times \\tfrac{3}{10} \\right) + \\left( 0.4 \\times \\tfrac{1}{10} \\right) + \\left( 0.67 \\times \\tfrac{6}{10} \\right) \\\\[1em] &amp;= 0.64 \\end{align} \\]"},{"location":"metrics/averaging-methods/#which-method-should-i-use","title":"Which Method Should I Use?","text":"<p>You would generally use these three methods to aggregate the metrics computed per class. Averaging is most commonly used in multiclass/multi-label classification and object detection tasks.</p> <p>So which average should you use?</p> <p>If you\u2019re looking for an easily understandable metric for overall model performance regardless of class, micro average is probably best.</p> <p>If you want to treat all classes equally, then using macro average would be a good choice.</p> <p>If you have an imbalanced dataset but want to assign more weight to classes with more samples, consider using weighted average instead of macro average.</p>"},{"location":"metrics/bertscore/","title":"BERTScore","text":"<p>BERTScore is a metric used in NLP workflows to measure textual similarity between candidate texts and reference texts. Unlike BLEU, ROUGE, and traditional n-gram similarity measures, it leverages pretrained BERT embeddings to capture the semantic and contextual information of words and phrases in both the candidate and reference texts. This approach makes BERTScore more effective at assessing the quality of candidate text because it considers not only exact word matches but also the overall meaning, fluency, and order of the output.</p> Recall: BERT &amp; Textual Embeddings <p>BERT (Bidirectional Encoder Representations from Transformers) is a popular language model used to generate embeddings from words and phrases. Textual embeddings are learned dense token representations that capture the semantic and contextual information of words and phrases in a continuous vector space. In a perfect embedding space, similar words are grouped together while words that are semantically different are distanced. For a deeper dive into BERT and textual embeddings, feel free to refer to the original paper.  </p>"},{"location":"metrics/bertscore/#implementation-details","title":"Implementation Details","text":"<p>BERTScore is a collection of three metrics \u2014 BERT-Precision, BERT-Recall, and BERT-F1. As the names imply, BERT-Precision measures how well the candidate texts avoid introducing irrelevant content. BERT-Recall measures how well the candidate texts avoid omitting relevant content. BERT-F1 is a combination of both Precision and Recall to measure how well the candidate texts capture and retain relevant information from the reference texts.</p>"},{"location":"metrics/bertscore/#calculating-bertscore","title":"Calculating BERTScore","text":"<p>Given a reference sentence, \\(x = \\langle x_1, x_2, ..., x_n \\rangle\\), and candidate sentence, \\(\\hat{x} = \\langle\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_m\\rangle\\), we first use BERT to generate a sequence of word embeddings for both reference and candidate sentences.</p> \\[ \\begin{align}     &amp; BERT(\\langle x_1, x_2, ..., x_n \\rangle) = \\langle \\mathbf{x_1}, \\mathbf{x_2}, ..., \\mathbf{x_n} \\rangle \\\\     &amp; BERT(\\langle \\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_m \\rangle) = \\langle \\mathbf{\\hat{x}_1}, \\mathbf{\\hat{x}_2}, ..., \\mathbf{\\hat{x}_m} \\rangle \\end{align} \\] <p><p>Note that we will use bold text to indicate vectors, like a word embedding</p></p> <p>To measure the similarity between two individual embeddings, we will use the cosine similarity which is defined as:</p> \\[ \\text{similarity}(\\mathbf{x_i}, \\mathbf{\\hat{x}_j}) = \\frac{\\mathbf{x_i}^\\top \\mathbf{\\hat{x}_j}}{||\\mathbf{x_i}||\\space||\\mathbf{\\hat{x}_j}||} \\] <p>which simply reduces to \\(\\mathbf{x_i}^\\top \\mathbf{\\hat{x}_j}\\) since both \\(\\mathbf{x_i}\\) and \\(\\mathbf{\\hat{x}_j}\\) are pre-normalized. With these definitions, we can now calculate the BERT-precision, BERT-recall, and BERT-F1.</p>"},{"location":"metrics/bertscore/#bert-precision","title":"BERT-Precision","text":"\\[ P_\\text{BERT} = \\frac{1}{|\\hat{x}|} \\sum_{\\mathbf{\\hat{x}_j} \\in \\hat{x}} \\underbrace{\\max_{\\mathbf{x_i} \\in x}\\overbrace{\\mathbf{x_i}^\\top \\mathbf{\\hat{x}_j}}^\\text{cosine similarity}}_\\text{greedy matching} \\] <p>Though the formula may seem intimidating, BERT-precision is conceptually similar to the precision formula, but uses greedy matching to maximize the similarity score between a reference word and the current candidate word. This is because the language domain can have multiple words that are similar in context to the ground truth, and the words of a sentence can be arranged in different ways while preserving the same meaning \u2014 thus, why we use greedy matching.</p>"},{"location":"metrics/bertscore/#bert-recall","title":"BERT-Recall","text":"\\[ R_\\text{BERT} = \\frac{1}{|x|} \\sum_{\\mathbf{x_i} \\in x} \\underbrace{\\max_{\\mathbf{\\hat{x}_j} \\in \\hat{x}}\\overbrace{\\mathbf{x_i}^\\top \\mathbf{\\hat{x}_j}}^\\text{cosine similarity}}_\\text{greedy matching} \\] <p>Once again, the BERT-recall is conceptually similar to the recall formula. Note that we flip \\(\\hat{x}\\) with \\(x\\) when calculating recall.</p>"},{"location":"metrics/bertscore/#bert-f1","title":"BERT-F1","text":"\\[ F_\\text{BERT} = 2 \\times \\frac{P_\\text{BERT} \\times R_\\text{BERT}}{P_\\text{BERT} + R_\\text{BERT}} \\] <p>The formula is the same as the F1-score formula, replacing the precision and recall components with BERT-precision and BERT-recall.</p> <p>In a more advanced implementation of BERTScore, extra steps are taken to finetune the metric. These include:</p> <ol> <li>Applying an \"importance factor\" to rare words so that the score weighs keywords moreso than words like \"it\", \"as\", and \"the\".</li> <li>Rescaling the score such that it lies between 0 and 1 in practical use cases. Although the score already lies between 0 and 1 in theory, it has been observed to lie between a more limited range in practice.</li> </ol> <p> </p>"},{"location":"metrics/bertscore/#python-implementation","title":"Python Implementation","text":"<p>There are many packages that implement the BERTScore metric, making the implementation quick and simple.</p> <ol> <li>HuggingFace - HuggingFace provides a comprehensive BertScore module with 140+ different BERT models to choose from,  allowing you to find the perfect balance between efficiency and accuracy.</li> <li>TorchMetrics - TorchMetrics provides a similar BertScore wrapper with the same functionality as HuggingFace.</li> <li>bert-score - bert-score is another package that can be used. It provides similar functionality as HuggingFace as well.</li> </ol> <p>There are many packages used to calculate BERTScore, and it is up to the user to choose their preferred package based on their existing workflow.</p>"},{"location":"metrics/bertscore/#interpretation","title":"Interpretation","text":"<p>BERTScore (Precision, Recall, F1) scores lie between the range of 0 and 1, with 0 representing no semantic similarity, and 1 representing a perfect semantic match between candidate and reference texts. However, interpreting the metric is completely subjective based on your task. On some tasks, a BERT-F1 of 0.9 may be excellent, whereas a BERT-F1 of 0.8 may be excellent for another. Generally speaking, a higher BERTScore is desirable.</p>"},{"location":"metrics/bertscore/#example","title":"Example","text":"<p>To showcase the value of BERTScore, let's consider the following candidate and reference texts:</p> Semantically Similar Texts Candidate Text Reference Text The sun set behind the mountains, casting a warm, orange glow across the horizon. As the mountains obscured the sun, a warm, orange glow painted the horizon. She sipped her coffee and gazed out of the window, lost in thought on a rainy afternoon. Lost in thought on a rainy afternoon, she sipped her coffee and stared out of the window. The adventurous explorer trekked through the dense jungle, searching for hidden treasures. In search of hidden treasures, the intrepid explorer ventured through the dense jungle. Laughter echoed through the park as children played on the swings and slides. Children's laughter filled the park as they enjoyed the swings and slides. The old bookstore was filled with the scent of well-worn pages, a haven for book lovers. A haven for book lovers, the old bookstore exuded the fragrance of well-read pages. <p>Using the following code and the <code>bert-score</code> package:</p> <pre><code>from bert_score import score\n\ncandidates = [...]\nreferences = [...]\nprecision, recall, f1 = score(c, r, lang='en') # using the default `roberta-large` BERT model\n\nprecision, recall, f1 = precision.mean(), recall.mean(), f1.mean()\n</code></pre> <p>We get the following BertScores: \\(P_\\text{BERT} = 0.9526, R_\\text{BERT} = 0.9480, F_\\text{BERT} = 0.9503\\). This is in-line with human judgement, as the reference and candidate texts are semantically very similar.</p> <p>However, if we were to calculate the BLEU score given these candidates and references, BLEU would yield a sub-optimal score of \\(\\text{BLEU} = 0.2403\\), despite the sentences being the same, semantically. This shows an advantage of embeddings-based metrics over n-gram-based metrics like BLEU.</p> Semantically Different Texts <p>This time, let our candidate and reference texts be:</p> Candidate Text Reference Text The sun was setting behind the mountains. This is a bad example She walked along the beach, feeling the sand between her toes. This has nothing to do with the other The chef prepared a delicious meal with fresh ingredients. Hello, world! The old oak tree stood tall in the middle of the field. Vivaldi is a classical conductor The detective examined the clues carefully. Wrong answer <p>This yields BERTScores of: \\(P_\\text{BERT} = 0.8328, R_\\text{BERT} = 0.8428, F_\\text{BERT} = 0.8377\\). Between different tasks, the baseline for a \"good\" BERTScore may vary based on different factors, like text length and BERT model type.</p>"},{"location":"metrics/bertscore/#advantages-and-limitations","title":"Advantages and Limitations","text":"<p>BERTScore, originally designed to be a replacement to the BLEU score and other n-gram similarity metrics, is a powerful metric that closely aligns with human judgement. However, it comes with limitations.</p> <ol> <li> <p>BERTScore is computationally expensive. The default model (<code>roberta-large</code>) used to calculate BERTScore requires 1.4GB of weights to be stored, and requires a forward pass through the model in order to calculate the score. This may be computationally expensive for large datasets, compared to n-gram-based metrics which are straightforward and easy to compute. However, smaller distilled models like <code>distilbert-base-uncased</code> can be used instead to reduce the computational cost, at the cost of reduced alignment with human judgement.</p> </li> <li> <p>BERTScore is calculated using a black-box pretrained model. The score can not be easily explained, as the embedding space of BERT is a dense and complex representation that is only understood by the model. Though the metric provides a numerical score, it does not explain how or why the particular score was assigned. In contrast, n-gram-based metrics can easily be calculated by inspection.</p> </li> </ol> <p>Limitations aside, BERTScore is still a powerful metric that can be included in NLP workflows to quantify the quality of machine-generated texts. It has been shown to have a high correlation with human judgement in tests, and is overall a better judge of similarity than BLEU, ROUGE, and traditional n-gram-based metrics. Furthermore, unlike traditional metrics, BERTScore's use of embeddings allows it to factor in context, semantics, and order into its score \u2014 which allows it to avoid the pitfalls of the traditional metrics it was designed to replace.</p>"},{"location":"metrics/bleu/","title":"BLEU","text":"<p>BLEU vs. Precision</p> <p>BLEU can be thought of as an analog to precision for text comparisons.</p> <p>The BLEU (BiLingual Evaluation Understudy) score is a metric commonly used in a variety of NLP tasks ranging from Machine Translation to Text Summarization, to evaluate the quality of candidate texts. It quantifies the similarity between the candidate text and reference text as a score between 0 and 1 \u2014 0 meaning there is no overlap with the ground truth text, and 1 meaning there is a perfect overlap. As the name suggests, it was originally developed for evaluating machine-translation models, but has since been adapted to many different tasks within NLP due to its dynamic nature for measuring textual similarity.</p>"},{"location":"metrics/bleu/#implementation-details","title":"Implementation Details","text":""},{"location":"metrics/bleu/#background","title":"Background","text":"<p>The BLEU score consists of two components \u2014 the Brevity Penalty and the n-gram Overlap.</p> What are n-grams? <p>An n-gram is a series of <code>n</code> adjacent tokens or words in a text. For example, all 1-grams (or unigrams) of the sentence <code>The cat chased the squirrel</code> are <code>[\"the\", \"cat\", \"chased\", \"the\", \"squirrel\"]</code>. The bigrams of the same sentence are <code>[\"the cat\", \"cat chased\", \"chased the\", \"the squirrel\"]</code>.</p> <ol> <li>The n-gram Overlap counts the number of 1-grams, 2-grams, 3-grams, and 4-grams of the output text that match the 1-, ..., 4-grams in the reference text \u2014 which is analogous to a precision score for the text. The 1-gram precision ensures that the correct vocabulary is used, whereas the 4-gram precision ensures that the candidate text is coherent.</li> <li>The Brevity Penalty is applied to penalize the score for generating sentences that are less in length than the reference text. This is due to the fact that the n-gram Overlap precision tends to give disproportionately high values to candidate texts that are very short in length, but mostly contain n-grams in the reference text.</li> </ol>"},{"location":"metrics/bleu/#definition","title":"Definition","text":"<p>The BLEU score is mathematically defined as:</p> \\[\\begin{align*} \\text{BLEU} &amp;= \\text{Brevity Penalty} \\times \\text{n-gram Overlap} \\\\ &amp;= \\min\\left(1, \\, \\exp\\left(1 - \\frac{\\text{reference length}}{\\text{output length}}\\right)\\right) \\times \\prod_{i=1}^{4}\\text{i-gram Precision}^{\\frac{1}{4}} \\end{align*}\\] <p>where the i-gram precision is calculated as:</p> \\[ p_i = \\frac{\\text{Clipped} \\text{ count of matching i-grams in candidate text}^1}{\\text{Total number of i-grams in candidate text}} \\] <p> <sup>1</sup> The clipped count of matching i-grams in candidate text is the minimum between the count of         i-grams in the candidate text and the maximum count of i-grams in any of the reference texts for a given i-gram.     </p>"},{"location":"metrics/bleu/#interpretation","title":"Interpretation","text":"<p>A known fact about BLEU scores is that they are not to be compared between different workflows and tasks. An excellent BLEU score for one task may be subpar for another. The score only serves as a general guideline to quantify the performance of your model \u2014 not to replace human judgement. That being said, what do these scores really mean, and how can we decipher them?</p> <ol> <li> <p>Higher is better: Though BLEU scores may vary among tasks, one thing is for sure - higher scores are better. Generally speaking, a commonly accepted guideline is as follows: </p> Score Qualitative Interpretation &lt; 0.1 Useless output 0.1 - 0.4 Varies in quality; May not be acceptable 0.4 - 0.6 High quality generated text &gt; 0.6 Better than human quality </li> <li> <p>Track Trends Over Time: Rising scores signal improvements in models, while drops could hint at issues or changes in your dataset.</p> </li> <li> <p>Combine It With Other Metrics: BLEU primarily measures n-gram overlap, overlooking some nuances like context and understanding. While a high BLEU score is promising, it doesn't guarantee flawless text. A complementary metric like BertScore may help in quantifying your model's performance from other perspectives.</p> </li> </ol>"},{"location":"metrics/bleu/#example","title":"Example","text":"Generated Reference <code>Fall leaves rustled softly beneath our weary feet</code> <code>Crisp autumn leaves rustled softly beneath our weary feet</code> Step 1: Tokenization &amp; n-grams <p>Splitting our sentences up into 1-, ..., 4-grams, we get:</p> <p>Generated Sentence:</p> n n-grams 1 [<code>Fall</code>, <code>leaves</code>, <code>rustled</code>, <code>softly</code>, <code>beneath</code>, <code>our</code>, <code>weary</code>, <code>feet</code>] 2 [<code>Fall leaves</code>, <code>leaves rustled</code>, <code>rustled softly</code>, <code>softly beneath</code>, <code>beneath our</code>, <code>our weary</code>, <code>weary feet</code>] 3 [<code>Fall leaves rustled</code>, <code>leaves rustled softly</code>, <code>rustled softly beneath</code>, <code>softly beneath our</code>, <code>beneath our weary</code>, <code>our weary feet</code>] 4 [<code>Fall leaves rustled softly</code>, <code>leaves rustled softly beneath</code>, <code>rustled softly beneath our</code>, <code>softly beneath our weary</code>, <code>beneath our weary feet</code>] <p>Reference Sentence:</p> n n-grams 1 [<code>Crisp</code>, <code>autumn</code>, <code>leaves</code>, <code>rustled</code>, <code>softly</code>, <code>beneath</code>, <code>our</code>, <code>weary</code>, <code>feet</code>] 2 [<code>Crisp autumn</code>, <code>autumn leaves</code>, <code>leaves rustled</code>, <code>rustled softly</code>, <code>softly beneath</code>, <code>beneath our</code>, <code>our weary</code>, <code>weary feet</code>] 3 [<code>Crisp autumn leaves</code>, <code>autumn leaves rustled</code>, <code>leaves rustled softly</code>, <code>rustled softly beneath</code>, <code>softly beneath our</code>, <code>beneath our weary</code>, <code>our weary feet</code>] 4 [<code>Crisp autumn leaves rustled</code>, <code>autumn leaves rustled softly</code>, <code>leaves rustled softly beneath</code>, <code>rustled softly beneath our</code>, <code>softly beneath our weary</code>, <code>beneath our weary feet</code>] Step 2: Calculate n-gram Overlap <p>Next, let's calculate the clipped precision scores for each of the n-grams. Recall that the precision formula is:</p> \\[ p_i = \\frac{\\text{Clipped} \\text{ count of matching i-grams in machine-generated text}^1}{\\text{Total number of i-grams in machine-generated text}} \\] <p> n Clipped Precision 1 7 / 8 = 0.875 2 6 / 7 = 0.857 3 5 / 6 = 0.833 4 4 / 5 = 0.800 <p></p> <p>So, our n-gram overlap is:</p> \\[ 0.875^{0.25}\\cdot0.857^{0.25}\\cdot0.833^{0.25}\\cdot0.800^{0.25} = 0.841 \\] Step 3: Calculate Brevity Penalty <p>We apply a brevity penalty to prevent the BLEU score from giving undeservingly high scores for short generated texts. Recall that our formula is:</p> \\[ \\min\\left(1, \\exp\\left(1 - \\frac{\\text{reference length}}{\\text{output length}}\\right)\\right) = \\min\\left(1, \\exp\\left(1 - \\frac{\\text{9}}{\\text{8}}\\right)\\right) = 0.882 \\] Step 4: Calculate BLEU <p>Combining our n-gram overlap and Brevity Penalty, our final BLEU score is:</p> \\[ \\text{BLEU} = \\text{Brevity Penalty} \\times \\text{n-gram Overlap} = 0.882 \\times 0.841 = 0.742 \\] <p>Note that in most cases, we may take the average or max of the BLEU score with respect to multiple reference texts \u2014 since multiple interpretations of the same sentences can be allowed. For example, if we calculated the BLEU score with the reference text being <code>\"Crisp autumn leaves rustled softly beneath our exhausted feet\"</code>, our BLEU score would be 0.478 - a much lower score from a small semantic change.</p>"},{"location":"metrics/bleu/#advantages-and-limitations","title":"Advantages and Limitations","text":"<p>Though BLEU is a popular metric in NLP workflows, it comes with its limitations.</p> <ol> <li>BLEU fails to consider the semantics of texts. As seen in the example, simply changing \"take the mystery out of\" to \"demistify\" \u2014 while the text still retains the exact same meaning \u2014 yields a much better score, going from 0.3 to 0.6. In contrast with embeddings-based metrics like BertScore, n-gram-based metrics like BLEU only consider the words in the candidate text, rather than the meaning. However, this is addressed by providing multiple possible reference texts when calculating the BLEU score.</li> <li>BLEU does not consider the order of words. Across a large candidate/reference, BLEU is unable to consider to order and fluency of the sentences due to its short context window of 4-grams. Although BLEU can be extended to include larger n-gram clipped precisions, it would negatively affect shorter texts. Similarly, BLEU does not consider the importance of words in a sentence either. It weighs unimportant words like \"the\", \"an\", \"too\", as much as it would the nouns and verbs in the sentence. Once again, these pitfalls are addressed by embeddings-based metrics like BertScore.</li> </ol> <p>That being said, the metric still has its advantages. It is quick and easy to compute, as opposed to other metrics like BertScore which would take significantly longer to compute and is not easy to justify. Furthermore, it is relatively similar to human judgement, and is commonly used within NLP which allows you to easily benchmark your models with others and identify pain points.</p>"},{"location":"metrics/confusion-matrix/","title":"Confusion Matrix","text":"<p>Guide: True Positive / False Positive / False Negative / True Negative</p> <p>You can find more info on true positive, false positive, false negative, and true negative in the TP / FP / FN / TN guide.</p> <p>A confusion matrix is a structured plot describing classification model performance as a table that highlights counts of objects with predicted classes (columns) against the actual classes (rows). Each cell has a count of the number of objects that have its correct class and predicted class, which indicates how confused a model is. A model is confused when a predicted class does not match the actual class. When they do match, this is considered a true positive (TP). In general, a model resulting in more true positives (TPs) / true negatives (TNs) with fewer false positives (FPs) / false negatives (FNs) is better.</p> <p> </p> <p>Confusion matrices are used in classification workflows with only one class or with multiple classes, which extends to object detection workflows, too. They help evaluate models by counting classification errors and visualizing class imbalances.</p> <ul> <li>  API Reference: <code>ConfusionMatrix</code> \u2197</li> </ul>"},{"location":"metrics/confusion-matrix/#implementation-details","title":"Implementation Details","text":"<p>The implementation of a confusion matrix depends on whether the workflow concerns one or more classes.</p>"},{"location":"metrics/confusion-matrix/#single-class","title":"Single-Class","text":"<p>Single-class confusion matrices are used for binary classification problems. After computing the number of TPs, FPs, FNs, and TNs, a confusion matrix would look like this:</p> <p> Predicted Positive Predicted Negative Actual Positive True Positive (TP) False Negative (FN) Actual Negative False Positive (FP) True Negative (TN) <p></p>"},{"location":"metrics/confusion-matrix/#example-single-class","title":"Example: Single-Class","text":"<p>Let's consider a simple binary classification example and plot a confusion matrix. The table below shows five samples' (three positive and two negative) ground truth labels and inference labels.</p> <p> Sample 1 Sample 2 Sample 3 Sample 4 Sample 5 Ground Truth <code>Cat</code> <code>Cat</code> <code>Cat</code> <code>No Cat</code> <code>No Cat</code> Inference <code>Cat</code> <code>No Cat</code> <code>No Cat</code> <code>No Cat</code> <code>Cat</code> <p></p> <p>A confusion matrix for this example can be plotted:</p> <p> Predicted <code>Cat</code> Predicted <code>No Cat</code> <code>Cat</code> 1 2 <code>No Cat</code> 1 1 <p></p>"},{"location":"metrics/confusion-matrix/#multiclass","title":"Multiclass","text":"<p>Multiclass confusion matrices, used for multiclass classification problems, outline counts of TPs, FPs, FNs, and TNs for every unique pair of actual and predicted labels. A multiclass classification confusion matrix with three classes would have the following format:</p> Predicted <code>Airplane</code> Predicted <code>Boat</code> Predicted <code>Car</code> Actual <code>Airplane</code> Correct Prediction Incorrect Prediction Incorrect Prediction Actual <code>Boat</code> Incorrect Prediction Correct Prediction Incorrect Prediction Actual <code>Car</code> Incorrect Prediction Incorrect Prediction Correct Prediction <p>And for example, if we are trying to calculate the counts of TP, FP, FN, and TN for class <code>Boat</code>:</p> Predicted <code>Airplane</code> Predicted <code>Boat</code> Predicted <code>Car</code> Actual <code>Airplane</code> True Negative False Positive True Negative Actual <code>Boat</code> False Negative True Positive False Negative Actual <code>Car</code> True Negative False Positive True Negative <p>Example: Multiclass</p> <p>Let's take a look at a multiclass classification example and plot a confusion matrix. In this example, we have three classes: <code>Airplane</code>, <code>Boat</code>, and <code>Car</code>. The multiclass classifier outputs the following inferences:</p> <p> Sample 1 Sample 2 Sample 3 Sample 4 Sample 5 Sample 6 Ground Truth <code>Airplane</code> <code>Boat</code> <code>Car</code> <code>Airplane</code> <code>Boat</code> <code>Boat</code> Inference <code>Airplane</code> <code>Boat</code> <code>Airplane</code> <code>Airplane</code> <code>Boat</code> <code>Car</code> <p></p> <p>A confusion matrix for this example can be plotted:</p> <p> Predicted <code>Airplane</code> Predicted <code>Boat</code> Predicted <code>Car</code> <code>Airplane</code> 2 0 0 <code>Boat</code> 0 2 1 <code>Car</code> 1 0 0 <p></p> <p>In a different case, these counts may be much higher:</p> <p> Predicted <code>Airplane</code> Predicted <code>Boat</code> Predicted <code>Car</code> <code>Airplane</code> 200 0 0 <code>Boat</code> 100 8,800 600 <code>Car</code> 100 0 1,000 <p></p> <p>This confusion matrix reveals that a model is very good at identifying the <code>Boat</code> class: 8,800 of 9,500 <code>Boat</code>s were correctly predicted. Of the 700 incorrect <code>Boat</code> predictions, 600 were predicted as <code>Car</code>, and 100 were predicted as <code>Airplane</code>.</p> <p>This confusion matrix indicates that when a model makes a <code>Airplane</code> inference, the model is correct half the time. If it is incorrect, it has labeled the <code>Airplane</code> as a <code>Boat</code> or a <code>Car</code>.</p> <p>Whenever there is an actual <code>Airplane</code> class, the model never predicts that there is a different transportation object.</p>"},{"location":"metrics/confusion-matrix/#normalization","title":"Normalization","text":"<p>Sometimes it is easier to focus on class-level behavior if you are using a normalized confusion matrix. If confusion matrices are color-coded, normalizing can also create a better visual representation:</p> <p> </p> <p>You can normalize a confusion matrix by <code>row</code> (actual classes), <code>column</code> (predicted classes), or <code>all</code> (entire matrix). Each type of normalization surfaces a view sharing different information, which is outlined below.</p> Normalizing by <code>row</code> <p>For an actual class, this normalization allows us to see the proportion of correctly or incorrectly predicted objects for each predicted class. Notice that the diagonal values from this normalization match the recall per class. To normalize by <code>row</code>, divide each entry in that <code>row</code> by the sum of values within it. If we normalize the multiclass example by <code>row</code>, we get:</p> <p> Predicted <code>Airplane</code> Predicted <code>Boat</code> Predicted <code>Car</code> <code>Airplane</code> 1 0 0 <code>Boat</code> 0.01 0.93 0.06 <code>Car</code> 0.09 0 0.91 <p></p> Normalizing by <code>column</code> <p>For a predicted class, this normalization allows us to see the proportion of instances predicted as a certain class that actually belong to each true class. Notice that the diagonal values from this normalization match the precision per class. To normalize by <code>column</code>, divide each entry in a <code>column</code> by the sum of values within that <code>column</code>. If we normalize the multiclass example by <code>column</code>, we get:</p> <p> Predicted <code>Airplane</code> Predicted <code>Boat</code> Predicted <code>Car</code> <code>Airplane</code> 0.5 0 0 <code>Boat</code> 0.25 1 0.375 <code>Car</code> 0.25 0 0.625 <p></p> Normalizing by <code>all</code> <p>For each entry, this normalization allows us to see the overall proportion of instances that fall into a combination of an actual and predicted class. To normalize by <code>all</code>, divide each entry by the total sum of all the values in the matrix. If we normalize the multiclass example by <code>all</code>, we get:</p> <p> Predicted <code>Airplane</code> Predicted <code>Boat</code> Predicted <code>Car</code> <code>Airplane</code> 0.02 0 0 <code>Boat</code> 0.01 0.81 0.06 <code>Car</code> 0.01 0 0.09 <p></p>"},{"location":"metrics/confusion-matrix/#limitations-and-biases","title":"Limitations and Biases","text":"<p>Confusion matrices are great for the evaluation of models that deal with multiple classes. They are structured tables of numbers, which is its strength and weakness.</p> <ol> <li>Class imbalance: Confusion matrices can appear biased when dealing with imbalanced numbers of instances per class, leading to skewed numbers. This can be addressed by normalizing the matrix.</li> <li>Categorical evaluation: Confusion matrices have categorical outputs and do not surface any details for misclassifications. All misclassifications are treated equally, so there may be cases where classes are similar or hierarchically related, but confusion matrices will not account for these details.</li> </ol>"},{"location":"metrics/f1-score/","title":"F<sub>1</sub>-score","text":"<p>The F<sub>1</sub>-score, also known as balanced F-score or F-measure, is a metric that combines two competing metrics, precision and recall, with an equal weight. F<sub>1</sub>-score is the harmonic mean between precision and recall, and symmetrically represents both in one metric.</p> <p>Guides: Precision and Recall</p> <p>Read the precision and the recall guides if you're not familiar with those metrics.</p> <p>Precision and recall offer a trade-off: increasing precision often reduces recall, and vice versa. This is called the precision/recall trade-off.</p> <p>Ideally, we want to maximize both precision and recall to obtain the perfect model. This is where the F<sub>1</sub>-score comes in play. Because the F<sub>1</sub>-score is the harmonic mean of precision and recall, maximizing the F<sub>1</sub>-score implies simultaneously maximizing both precision and recall. Thus, the F<sub>1</sub>-score has become a popular metric for the evaluation of many workflows, such as classification, object detection, semantic segmentation, and information retrieval.</p> <ul> <li>  API Reference: <code>f1_score</code> \u2197</li> </ul>"},{"location":"metrics/f1-score/#implementation-details","title":"Implementation Details","text":"<p>Using TP / FP / FN / TN, we can define precision and recall. The F<sub>1</sub>-score is computed by taking the harmonic mean of precision and recall.</p> <p>The F<sub>1</sub>-score is defined:</p> \\[ \\begin{align} \\text{F}_1 &amp;= \\frac {2} {\\frac {1} {\\text{Precision}} + \\frac {1} {\\text{Recall}}} \\\\[1em] &amp;= \\frac {2 \\times \\text{Precision} \\times \\text{Recall}} {\\text{Precision} + \\text{Recall}} \\end{align} \\] <p>It can also be calculated directly from true positive (TP) / false positive (FP) / false negative (FN) counts:</p> \\[ \\text{F}_1 = \\frac {\\text{TP}} {\\text{TP} + \\frac 1 2 \\left( \\text{FP} + \\text{FN} \\right)} \\]"},{"location":"metrics/f1-score/#examples","title":"Examples","text":"<p>Perfect inferences:</p> Metric Value TP 20 FP 0 FN 0 \\[ \\begin{align} \\text{Precision} = \\frac{20}{20 + 0} &amp;= 1.0 \\\\[1em] \\text{Recall} = \\frac{20}{20 + 0} &amp;= 1.0 \\\\[1em] \\text{F}_1 = \\frac{20}{20 + \\frac 1 2 \\left( 0 + 0 \\right)} &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences, where every ground truth is recalled by an inference:</p> Metric Value TP 25 FP 75 FN 0 \\[ \\begin{align} \\text{Precision} = \\frac{25}{25 + 75} &amp;= 0.25 \\\\[1em] \\text{Recall} = \\frac{25}{25 + 0} &amp;= 1.0 \\\\[1em] \\text{F}_1 = \\frac{25}{25 + \\frac 1 2 \\left( 75 + 0 \\right)} &amp;= 0.4 \\end{align} \\] <p>Perfect inferences but some ground truths are missed:</p> Metric Value TP 25 FP 0 FN 75 \\[ \\begin{align} \\text{Precision} = \\frac{25}{25 + 0} &amp;= 1.0 \\\\[1em] \\text{Recall} = \\frac{25}{25 + 75} &amp;= 0.25 \\\\[1em] \\text{F}_1 = \\frac{25}{25 + \\frac 1 2 \\left( 0 + 75 \\right)} &amp;= 0.4 \\end{align} \\] <p>Zero correct inferences with non-zero false positive and false negative:</p> Metric Value TP 0 FP 15 FN 10 \\[ \\begin{align} \\text{Precision} = \\frac{0}{0 + 15} &amp;= 0.0 \\\\[1em] \\text{Recall} = \\frac{0}{0 + 10} &amp;= 0.0 \\\\[1em] \\text{F}_1 = \\frac{0}{0 + \\frac 1 2 \\left( 15 + 10\\right)} &amp;= 0.0 \\end{align} \\] <p>Zero correct inferences with zero false positive and false negative:</p> Metric Value TP 0 FP 0 FN 0 <p>Undefined F<sub>1</sub></p> <p>This example shows an edge case where both precision and recall are <code>undefined</code>. When either metric is <code>undefined</code>, F<sub>1</sub> is also <code>undefind</code>. In such cases, it's often interpreted as <code>0.0</code> instead.</p> \\[ \\begin{align} \\text{Precision} &amp;= \\frac{0}{0 + 0} \\\\[1em] &amp;= \\text{undefined} \\\\[1em] \\text{Recall} &amp;= \\frac{0}{0 + 0} \\\\[1em] &amp;= \\text{undefined} \\\\[1em] \\text{F}_1 &amp;= \\frac{0}{0 + \\frac 1 2 \\left( 0 + 0\\right)} \\\\[1em] &amp;= \\text{undefined} \\\\[1em] \\end{align} \\]"},{"location":"metrics/f1-score/#multiple-classes","title":"Multiple Classes","text":"<p>In workflows with multiple classes, the F<sub>1</sub>-score can be computed per class. In the TP / FP / FN / TN guide, we learned how to compute per-class metrics when there are multiple classes, using the one-vs-rest (OvR) strategy. Once you have TP, FP, and FN counts computed for each class, you can compute precision, recall, and F<sub>1</sub>-score for each class by treating each as a single-class problem.</p>"},{"location":"metrics/f1-score/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single F<sub>1</sub>-score that summarizes model performance across all classes, there are different ways to aggregate per-class F<sub>1</sub>-scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/f1-score/#f_beta-score","title":"F\\(_\\beta\\)-score","text":"<p>The F\\(_\\beta\\)-score is a generic form of the F<sub>1</sub>-score with a weight parameter, \\(\\beta\\), where recall is considered \\(\\beta\\) times more important than precision:</p> \\[ \\text{F}_{\\beta} = \\frac {(1 + \\beta^2) \\times \\text{precision} \\times \\text{recall}} {(\\beta^2 \\times \\text{precision}) + \\text{recall}} \\] <p>The three most common values for the beta parameter are as follows:</p> <ul> <li>F<sub>0.5</sub>-score \\(\\left(\\beta = 0.5\\right)\\), where precision is more important than recall, it focuses more on minimizing FPs than minimizing FNs</li> <li>F<sub>1</sub>-score \\(\\left(\\beta = 1\\right)\\), the true harmonic mean of precision and recall</li> <li>F<sub>2</sub>-score \\(\\left(\\beta = 2\\right)\\), where recall is more important than precision, it focuses more on minimizing FNs than minimizing FPs</li> </ul>"},{"location":"metrics/f1-score/#limitations-and-biases","title":"Limitations and Biases","text":"<p>While the F<sub>1</sub>-score can be used to evaluate classification/object detection models with a single metric, this metric is not adequate to use for all applications. In some applications, such as identifying pedestrians from an autonomous vehicle, any false negatives can be life-threatening. In these scenarios, having a few more false positives as a trade-off for reducing the chance of any life-threatening events happening is preferred. Here, recall should be weighted much more than the precision as it minimizes false negatives. To address the significance of recall, \\(\\text{F}_\\beta\\) score can be used as an alternative.</p>"},{"location":"metrics/f1-score/#threshold-dependence","title":"Threshold-Dependence","text":"<p>Precision, recall, and F<sub>1</sub>-score are all threshold-dependent metrics. Threshold-dependent means that, before computing these metrics, a confidence score threshold must be applied to inferences to decide which should be used for metrics computation and which should be ignored.</p> <p>A small change to this confidence score threshold can have a large impact on threshold-dependent metrics. To evaluate a model across all thresholds, rather than at a single-threshold, use threshold-independent metrics, like average precision.</p>"},{"location":"metrics/fpr/","title":"False Positive Rate (FPR)","text":"<p>False positive rate (FPR) measures the proportion of negative ground truths that a model incorrectly predicts as positive, ranging from 0 to 1. A low false positive rate indicates that the model is good at avoiding false alarms, where a high false positive rate suggests that the model is incorrectly classifying a significant number of negative cases as positive.</p> <p>As shown in this diagram, false positive rate is the fraction of all negative ground truths that are incorrectly predicted:</p> \\[\\text{FPR} = \\frac{\\text{FP}}{\\text{TN} + \\text{FP}}\\] <p>In the above formula, \\(\\text{TN}\\) is the number of true negative inferences and \\(\\text{FP}\\) is the number of false positive inferences.</p> <p>Guide: True Negative / False Positive</p> <p>Read the TP / FP / FN / TN guide if you're not familiar with \"TN\" and \"FP\" terminology.</p> <p> </p> <p>FPR is often used in conjuction with TPR (recall). By measuring both FPR and TPR, a more complete picture of a model's performance can be drawn.</p> <ul> <li>  API Reference: <code>FPR</code> \u2197</li> </ul>"},{"location":"metrics/fpr/#implementation-details","title":"Implementation Details","text":"<p>FPR is used to evaluate the performance of a classification model, particularly in tasks like binary classification, where the goal is to classify data into one of two possible classes. FPR is closely related to specificity. While specificity measure the model's ability to correctly identify the negative class, the FPR focuses on the negative class instances that are incorrectly classified as positive.</p> <p>Here is how FPR is calculated:</p> \\[ \\text{FPR} = \\frac {\\text{# False Positives}} {\\text{# True Negatives} + \\text{# False Positives}} \\]"},{"location":"metrics/fpr/#examples","title":"Examples","text":"<p>Perfect model inferences, where every negative ground truth is recalled by an inference:</p> Metric Value TN 20 FP 0 \\[ \\begin{align} \\text{FPR} &amp;= \\frac{0}{20 + 0} \\\\[1em] &amp;= 0.0 \\end{align} \\] <p>Partially correct inferences, where some negative ground truths are correctly recalled (TN) and others are missed (FP):</p> Metric Value TN 85 FP 15 \\[ \\begin{align} \\text{FPR} &amp;= \\frac{15}{85 + 15} \\\\[1em] &amp;= 0.15 \\end{align} \\] <p>Zero correct inferences \u2014 no negative ground truths are recalled:</p> Metric Value TN 0 FP 20 \\[ \\begin{align} \\text{FPR} &amp;= \\frac{20}{0 + 20} \\\\[1em] &amp;= 1.0 \\end{align} \\]"},{"location":"metrics/fpr/#multiple-classes","title":"Multiple Classes","text":"<p>So far, we have only looked at binary classification cases, but in multiclass or multi-label cases, FPR is computed per class. In the TP / FP / FN / TN guide, we went over multiple-class cases and how these metrics are computed. Once you have these four metrics computed per class, you can compute FPR for each class by treating each as a single-class problem.</p>"},{"location":"metrics/fpr/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single FPR score that summarizes model performance across all classes, there are different ways to aggregate per-class FPR scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/fpr/#limitations-and-biases","title":"Limitations and Biases","text":"<p>While FPR is a valuable metric for evaluating the performance of classification models, it does have limitations and potential biases that should be considered when interpreting results:</p> <ol> <li> <p>Sensitivity to Class Imbalance: FPR is sensitive to class imbalance in dataset, just like specificity. If one class significantly outnumbers the other, a low FPR can be achieved simply by predicting the majority class most of the time. This can lead to a misleadingly low FPR score while neglecting the model's ability to correctly classify the minority class.</p> </li> <li> <p>Ignoring False Negatives: FPR focuses exclusively on the true negatives (correctly classified negative cases) and false positives (negative cases incorrectly classified as positive), but it doesn't account for false negatives (positive cases incorrectly classified as negative). Ignoring false negatives can be problematic in applications where missing positive cases is costly or has severe consequences.</p> </li> <li> <p>Incomplete Context: FPR alone does not provide a complete picture of a model's performance. It is often used in conjunction with other metrics like sensitivity (recall), precision, and F<sub>1</sub>-score to provide a more comprehensive assessment. Depending solely on FPR might hide issues related to other aspects of classification, such as the models' ability to identify true positives.</p> </li> <li> <p>Threshold Dependence: FPR is a binary metric that doesn't take into account the probability or confidence levels associated with predictions. Models with different probability distributions might achieve the same FPR score, but their operational characteristics can vary significantly. To address this limitation, consider using threshold-independent metrics like the area under the receiver operating characteristic curve (AUC-ROC) which can provide a more comprehensive understanding of model performance.</p> </li> </ol>"},{"location":"metrics/geometry-matching/","title":"Geometry Matching","text":"<p>Geometry matching is the process of matching inferences to ground truths for computer vision workflows with a localization component, such as 2D and 3D object detection and instance segmentation. It is a building block for metrics like TP / FP / FN counts and any metrics derived from these, such as precision and recall.</p> <p>While it may sound simple, geometry matching is surprisingly challenging and full of edge cases! In this guide, we'll focus on 2D object detection\u2014specifically 2D bounding box matching\u2014to learn about geometry matching algorithms.</p> <ul> <li>  API Reference: <code>match_inferences</code>,   <code>match_inferences_multiclass</code> \u2197</li> </ul>"},{"location":"metrics/geometry-matching/#algorithm-overview","title":"Algorithm Overview","text":"<p>In a geometry matching algorithm, the following criteria must be met for a valid match:</p> <ol> <li>The IoU between the inference and ground truth must be greater than or equal to a threshold</li> <li>For multiclass workflows, inference label must match the ground truth label</li> </ol> Pseudocode: Geometry Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all labels;</li> <li>Get inferences and ground truths with the current label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than or equal to the IoU threshold;</li> </ol> </li> <li>Repeat 5-6 on the next inference;</li> </ol>"},{"location":"metrics/geometry-matching/#examples-matching-2d-bounding-boxes","title":"Examples: Matching 2D Bounding Boxes","text":"<p>Let's apply the algorithm above to the following examples of 2D object detection. Bounding boxes (see: <code>BoundingBox</code>) in the diagrams below use the following colors based on their type and the matching result:</p> <p> </p> <p>This example contains two ground truth and two inference bounding boxes, each with the same label. The pair \\((\\text{A}, \\text{a})\\) has high overlap (IoU of 0.9) and the pair \\((\\text{B}, \\text{b})\\) has low overlap (IoU of 0.13). Let's find out what the matched results look like in this example with a IoU threshold of 0.5:</p> <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.98 0.9 0.0 \\(\\text{b}\\) 0.6 0.0 0.13 <p></p> <p>Because inference \\(\\text{a}\\) has a higher confidence score than inference \\(\\text{b}\\), it gets matched first. It is pretty clear that ground truth \\(\\text{A}\\) scores the highest IoU with inference \\(\\text{a}\\), and IoU is greater than IoU threshold, so \\(\\text{a}\\) and \\(\\text{A}\\) are matched.</p> <p>Next, inference \\(\\text{b}\\) gets compared against all ground truth bounding boxes. Once again, it is clear that ground truth \\(\\text{B}\\) scores the maximum IoU with inference \\(\\text{b}\\), but this time IoU is less than the IoU threshold, so \\(\\text{b}\\) becomes an unmatched inference.</p> <p>Now that we have checked all inferences, any ground truth bounding boxes that are not matched yet are marked as unmatched. In this case, ground truth \\(\\text{B}\\) is the only unmatched ground truth.</p> <p> Bounding Box(es) Match Type \\((\\text{A}, \\text{a})\\) Matched Pair \\(\\text{B}\\) Unmatched Ground Truth \\(\\text{b}\\) Unmatched Inference <p></p> <p>Let's take a look at another example with multiple classes, <code>Apple</code> and <code>Banana</code>:</p> <p> </p> <p> Bounding Box Class Score IoU(\\(\\text{A}\\)) \\(\\text{A}\\) <code>Apple</code> \u2014 \u2014 \\(\\text{a}\\) <code>Apple</code> 0.3 0.0 \\(\\text{b}\\) <code>Banana</code> 0.5 0.8 <p></p> <p>Each class is evaluated independently. Starting with <code>Apple</code>, there is one ground truth \\(\\text{A}\\) and one inference \\(\\text{a}\\), but these two do not overlap at all (IoU of 0.0). Because IoU is less than the IoU threshold, there is no match for class <code>Apple</code>.</p> <p>For class <code>Banana</code>, there is only one inference and no ground truths. Therefore, there is also no match for class <code>Banana</code>.</p> <p> Bounding Box(es) Match Type \\(\\text{A}\\) Unmatched Ground Truth \\(\\text{a}\\) Unmatched Inference \\(\\text{b}\\) Unmatched Inference <p></p> <p>Here is another example with multiple inferences overlapping with the same ground truth:</p> <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) \\(\\text{a}\\) 0.5 0.8 \\(\\text{b}\\) 0.8 0.5 <p></p> <p>Among the two inferences \\(\\text{a}\\) and \\(\\text{b}\\), \\(\\text{b}\\) has a higher confidence score, so \\(\\text{b}\\) gets matched first. IoU between ground truth \\(\\text{A}\\) and \\(\\text{b}\\) is greater than the IoU threshold, so they become a match.</p> <p>Inference \\(\\text{a}\\) is compared with ground truth \\(\\text{A}\\), but even though IoU is greater than the IoU threshold, they cannot become a match because \\(\\text{A}\\) is already matched with \\(\\text{b}\\), so inference \\(\\text{a}\\) remains unmatched.</p> <p> Bounding Box(es) Match Type \\((\\text{A}, \\text{b})\\) Matched Pair \\(\\text{a}\\) Unmatched Inference <p></p> <p>Finally, let's consider another scenario where there are multiple ground truths overlapping with the same inference:</p> <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.8 0.6 0.9 <p></p> <p>Inference \\(\\text{a}\\) has a higher IoU with ground truth \\(\\text{B}\\), so \\(\\text{a}\\) and \\(\\text{B}\\) become matched.</p> <p> Bounding Box(es) Match Type \\((\\text{B}, \\text{a})\\) Matched Pair \\(\\text{A}\\) Unmatched Ground Truth <p></p>"},{"location":"metrics/geometry-matching/#comparison-of-matching-algorithms-from-popular-benchmarks","title":"Comparison of Matching Algorithms from Popular Benchmarks","text":"<p>Geometry matching is a fundamental part of evaluation for workflows with localization. Metrics such as precision, recall, and average precision are built on top of these matches. The matching algorithm we've covered above is standard across various popular object detection benchmarks.</p> <p>In this section, we'll examine the differences in matching algorithm from a few popular benchmarks:</p> <ul> <li>PASCAL VOC 2012</li> <li>COCO</li> <li>Open Images V7</li> </ul>"},{"location":"metrics/geometry-matching/#pascal-voc-2012","title":"PASCAL VOC 2012","text":"<p>The PASCAL VOC 2012 benchmark includes a <code>difficult</code> boolean annotation for each ground truth, used to differentiate objects that are difficult to recognize from an image. Any ground truth with the <code>difficult</code> flag and any inferences that are matched with a <code>difficult</code> ground truth will be ignored in the matching process. In other words, these ground truths and the inferences that are matched with them are excluded in the matched results. Hence, models will not be penalized for failing to detect these <code>difficult</code> objects, nor rewarded for detecting them.</p> <p>Another difference that is noteworthy is how PASCAL VOC outlines the IoU criteria for a valid match. According to the evaluation section (4.4) in development kit doc, IoU must exceed the IoU threshold to be considered as a valid match.</p> Pseudocode: PASCAL VOC Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all labels;</li> <li>Get inferences and ground truths with the evaluating label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than the IoU threshold;</li> </ol> </li> <li>If matched with a <code>difficult</code> ground truth, ignore;</li> <li>Repeat 5-7 on the next inference;</li> </ol>"},{"location":"metrics/geometry-matching/#coco","title":"COCO","text":"<p>COCO (Common Objects in Context) labels its ground truth annotations with an <code>iscrowd</code> field to specify when a ground truth includes multiple objects. Similarly to how <code>difficult</code> ground truths are treated in PASCAL VOC, any inferences matched with these <code>iscrowd</code> ground truths, are excluded from the matched results. This <code>iscrowd</code> flag is intended to avoid penalizing models for failing to detect objects in a crowded scene.</p> Pseudocode: COCO Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all labels;</li> <li>Get inferences and ground truths with the evaluating label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than or equal to the IoU threshold;</li> </ol> </li> <li>If matched with a <code>iscrowd</code> ground truth, ignore;</li> <li>Repeat 5-7 on the next inference;</li> </ol>"},{"location":"metrics/geometry-matching/#open-images-v7","title":"Open Images V7","text":"<p>The Open Images V7 Challenge evaluation introduces two key differences in its matching algorithm.</p> <p>The first is with the way that the images are annotated in this dataset. Images are annotated with positive image-level labels, indicating certain object classes are present, and with negative image-level labels, indicating certain classes are absent. Therefore, for fair evaluation, all unannotated classes are excluded from evaluation in that image, so if an inference has a class label that is unannotated on that image, this inference is excluded in the matching results.</p> <p> </p> <p>An example of non-exhaustive image-level labeling from Open Images V7</p> <p>The second difference is with handling <code>group-of</code> boxes, which is similar to <code>iscrowd</code> annotation from COCO but is not just simply ignored. If at least one inference is inside the <code>group-of</code> box, then it is considered to be a match. Otherwise, the <code>group-of</code> box is considered as an unmatched ground truth. Also, multiple correct inferences inside the same <code>group-of</code> box still count as a single match:</p> <p> </p> <p>An example of <code>group-of</code> boxes from Open Images V7</p> Pseudocode: Open Images V7 Matching <ol> <li>Loop through all images in your dataset;</li> <li>Loop through all positive image-level labels;</li> <li>Get inferences and ground truths with the evaluating label;</li> <li>Sort inferences by descending confidence score;</li> <li>Check against all non-<code>ground-of</code> ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the following criteria for a valid match:<ol> <li>This ground truth is not matched yet AND</li> <li>The IoU is greater than or equal to the IoU threshold;</li> </ol> </li> <li>If matched with a <code>difficult</code> ground truth, ignore;</li> <li>Repeat 5-7 on the next inference;</li> <li>Loop through all unmatched inferences;</li> <li>Check against all <code>group-of</code> ground truths and find a ground truth that results in maximum IoU;</li> <li>Check for the matching criteria (6);</li> <li>Repeat 10-11 on the next unmatched inference;</li> </ol>"},{"location":"metrics/geometry-matching/#limitations-and-biases","title":"Limitations and Biases","text":"<p>The standard matching algorithm appears to have an undesirable behavior when there are many overlapping ground truths and inferences with high confidence scores due to its greedy matching. Because it optimizes for higher confidence score and maximum IoU, it can potentially miss valid matches by matching a non-optimal pair, resulting in a poorer matching performance.</p> Example: Greedy Matching <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.7 0.0 0.6 \\(\\text{b}\\) 0.8 0.5 0.7 <p></p> <p>When there are two ground truths and two inferences, one inference \\(\\text{b}\\) with a higher score overlaps well with both ground truths \\(\\text{A}\\) and \\(\\text{B}\\), and the other one, \\(\\text{a}\\), with a lower score overlaps well with just one ground truth \\(\\text{B}\\). Because the IoU between \\(\\text{B}\\) and \\(\\text{b}\\) is greater than IoU between \\(\\text{A}\\) and \\(\\text{b}\\), inference \\(\\text{b}\\) is matched with ground truth \\(\\text{B}\\), causing inference \\(\\text{a}\\) to fail to match.</p> <p>This greedy matching behavior results in a higher false positive count in this type of scenario. Ideally, inference \\(\\text{a}\\) matches with ground truth \\(\\text{B}\\), and inference \\(\\text{b}\\) matches with ground truth \\(\\text{A}\\), resulting in no FPs.</p> <p>Another behavior to note here is that it is possible to get different matching results depending on the ground truth order when there are multiple ground truths overlapping with an inference with the equal IoU or depending on the inference order when there are multiple inferences overlapping with a ground truth with the equal confidence score.</p> Example: Different Matching Results When Ground Truth Order Changes <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.7 0.0 0.5 \\(\\text{b}\\) 0.7 0.5 0.5 <p></p> <p>The three pairs of ground truth and inference have same IoU and both inferences have same confidence score.</p> <p>If the ground truths are ordered as \\([\\text{A}, \\text{B}]\\) and the inferences as \\([\\text{a}, \\text{b}]\\), inference \\(\\text{a}\\) is matched with \\(\\text{B}\\) first, so inference \\(\\text{b}\\) gets matched with \\(\\text{A}\\).</p> <p>If the inference order changes to \\([\\text{b}, \\text{a}]\\), now inference \\(\\text{a}\\) may or may not be matched with any ground truth. The matched result can change depending on the ground truth order. If \\(\\text{A}\\) is evaluated before \\(\\text{B}\\), inference \\(\\text{b}\\) is matched with \\(\\text{A}\\), and \\(\\text{a}\\) can be matched with \\(\\text{B}\\). However, if \\(\\text{B}\\) comes before \\(\\text{A}\\), inference \\(\\text{b}\\) is matched with \\(\\text{B}\\) instead, leaving inference \\(\\text{a}\\) with no match.</p> <p>As discussed earlier, the standard matching algorithm compares model inferences with annotated ground truths in two fundamental aspects: localization and classification. The comparison generates results, which entail matched pairs, unmatched ground truths, and unmatched inferences; however, these results do not reveal why certain matches were unsuccessful. A myriad of reasons can lead to a failed match, such as poor localization due to insufficient overlap (IoU), or good localization but poor classification. Surfacing these types of errors is profoundly useful during model debugging. For instance, confused matches where localization succeeded (i.e. IoU above the IoU threshold) but classification failed (i.e. mismatching label values) can be identified by matching unmatched inferences with unmatched ground truths once more after the initial matching. Confused matches are useful for creating a confusion matrix to focus on a detection model's classification performance.</p>"},{"location":"metrics/iou/","title":"Intersection over Union (IoU)","text":"<p>Intersection over Union (IoU) measures the ratio of the intersection and the union between ground truth and inference, ranging from 0 to 1 where 1 indicates a perfect match.  The objective of this metric is to compare inferences to ground truths by measuring similarity between them.</p> <p>As the name suggests, the IoU of two instances (\\(\\text{A}\\) and \\(\\text{B}\\)) is defined as:</p> \\[\\text{IoU} \\left( \\text{A}, \\text{B} \\right) = \\frac {\\text{A} \\cap \\text{B}} {\\text{A} \\cup \\text{B}}\\] <ul> <li>  API Reference: <code>iou</code> \u2197</li> </ul>"},{"location":"metrics/iou/#when-do-i-use-iou","title":"When Do I Use IoU?","text":"<p>It is often used to compare two geometries (e.g., <code>BoundingBox</code>, <code>Polygon</code> or <code>SegmentationMask</code>) in object detection, instance segmentation, or semantic segmentation workflows. In multi-label classification, IoU, more likely known as the Jaccard index, is used to compare set of inference labels for a sample to the corresponding set of ground truth labels. Moreover, there are workflows such as action detection and video moment retrieval where IoU measures the temporal overlap between two time-series snippets.</p> <p>Because IoU can be used on various types of data, let's look at how the metric is defined for some of these data types:</p> <ul> <li>2D Axis-Aligned Bounding Box</li> <li>Segmentation Mask</li> <li>Set of Labels</li> </ul>"},{"location":"metrics/iou/#2d-axis-aligned-bounding-box","title":"2D Axis-Aligned Bounding Box","text":"<p>Let's consider two 2D axis-aligned bounding boxes, \\(\\text{A}\\) and \\(\\text{B}\\), with the origin of the coordinates being the top-left corner of the image, and to the right and down are the positive directions of the \\(x\\) and \\(y\\) axes, respectively. This is the most common coordinate system in computer vision.</p> <p> </p> <p>Guides: Commonly Used Bounding Box Representations</p> <p>A bounding box is often defined by the \\(x\\) and \\(y\\) coordinates of the top-left and bottom-right corners. This is the format used in this guide and in the <code>kolena</code> package.</p> <p>Another commonly used representation is the \\(x\\) and \\(y\\) coordinates of bounding box center, along with the width and height of the box.</p> <p>In order to compute IoU for two 2D bounding boxes, the first step is identifying the area of the intersection box, \\((\\text{A} \\cap \\text{B})\\).  This is the highlighted overlap region in the image above. The two coordinates of the intersection box, top-left and bottom-right corners, can be defined as:</p> \\[ \\text{A} \\cap \\text{B}\\,_{\\text{top-left}} = (\\max \\left( x_{a1}, \\, x_{b1} \\right), \\, \\max \\left( y_{a1}, \\, y_{b1} \\right)) \\] \\[ \\text{A} \\cap \\text{B}\\,_{\\text{bottom-right}} = (\\min \\left( x_{a2}, \\, x_{b2} \\right), \\, \\min \\left(y_{a2}, \\, y_{b2} \\right)) \\] <p>Once the intersection box \\((\\text{A} \\cap \\text{B})\\) is identified, the area of the union, \\((\\text{A} \\cup \\text{B})\\), is simply a sum of the area of \\(\\text{A}\\) and \\({\\text{B}}\\) minus the area of the intersection box.</p> \\[ \\text{area} \\left( \\text{A} \\cup \\text{B} \\right) = \\text{area} \\left( \\text{A} \\right) + \\text{area} \\left( \\text{B} \\right) - \\text{area} \\left( \\text{A} \\cap \\text{B} \\right) \\] <p>Finally, IoU is calculated by taking the ratio of the area of intersection box and the area of the union region.</p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {\\text{area} \\left( \\text{A} \\cap \\text{B} \\right)} {\\text{area} \\left( \\text{A} \\cup \\text{B} \\right)} \\\\[1em] &amp;= \\frac {\\text{area} \\left( \\text{A} \\cap \\text{B} \\right)} {\\text{area} \\left( \\text{A} \\right) + \\text{area} \\left( \\text{B} \\right) - \\text{area} \\left( \\text{A} \\cap \\text{B} \\right)} \\end{align} \\]"},{"location":"metrics/iou/#examples-iou-of-2d-bounding-boxes","title":"Examples: IoU of 2D Bounding Boxes","text":"<p>The following examples show what IoU values look like in different scenarios with 2D bounding boxes:</p> <p>Example 1: overlapping bounding boxes</p> <p> </p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {(10 - 5) \\times (10 - 2)} {10 \\times 10 + (15 - 5) \\times (12 - 2) - (10 - 5) \\times (10 - 2)} \\\\[1em] &amp;= \\frac {40} {100 + 100 - 40} \\\\[1em] &amp;= 0.25 \\end{align} \\] <p>Example 2: non-overlapping bounding boxes</p> <p> </p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {0} {10 \\times 10 + (15 - 10) \\times (15 - 10) - 0} \\\\[1em] &amp;= \\frac {0} {100 + 25 - 0} \\\\[1em] &amp;= 0.0 \\end{align} \\] <p>Example 3: completely matching bounding boxes</p> <p> </p> \\[ \\begin{align} \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) &amp;= \\frac {10 \\times 10} {10 \\times 10 + 10 \\times 10 - 10 \\times 10} \\\\[1em] &amp;= \\frac {100} {100 + 100 - 100} \\\\[1em] &amp;= 1.0 \\end{align} \\]"},{"location":"metrics/iou/#segmentation-mask","title":"Segmentation Mask","text":"<p>A segmentation mask is a 2D image where each pixel is a class label commonly used in semantic segmentation workflow. The inference shape matches the ground truth shape (width and height), with a channel depth equivalent to the number of class labels to be predicted. Each channel is a binary mask that labels areas where a specific class is present:</p> <p> </p> From left to right: the original RGB image, the ground truth segmentation mask, and the inference segmentation mask <p>The IoU metric measures the intersection (the number of pixels common between the ground truth and inference masks, true positive (TP)) divided by the union (the total number of pixels present across both masks, TP + false negative (FN) + false positive (FP)). And here is the formula for the IoU metric for a segmentation mask:</p> \\[ \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) = \\frac {\\text{TP}} {\\text{TP} + \\text{FN} + \\text{FP}} \\] <p>Let\u2019s look at what TP, FP, and FN look like on a segmentation mask:</p> <p> </p> From left to right: the ground truth segmentation mask, the inference segmentation mask, and the overlay with TP, FP, and FN labeled <p>From the cat image shown above, when you overlay the ground truth and inference masks, the pixels that belong to both masks are TP. The pixels that only exist in the ground truth mask are FNs, and the pixels that only exist in the inference mask are FPs. Let's consider the following pixel counts for each category:</p> <p> # True Positives # False Positives # False Negatives 100 25 75 <p></p> <p>Then the IoU becomes:</p> \\[ \\begin{align} \\text{IoU} &amp;= \\frac {100} {100 + 25 + 75} \\\\[1em] &amp;= 0.5 \\end{align} \\]"},{"location":"metrics/iou/#set-of-labels","title":"Set of Labels","text":"<p>The set of labels used in multi-label classification workflow is often a binarized list with a number of label elements, for example, let\u2019s say there are three classes, <code>Airplane</code>, <code>Boat</code>, and <code>Car</code>, and a sample is labeled as <code>Boat</code> and <code>Car</code>. The binary set of labels would then be \\([0, 1, 1]\\), where each element represents each class, respectively.</p> <p>Similar to the segmentation mask, the IoU or Jaccard index metric for the ground truth/inference labels would be the size of the intersection of the two sets (the number of labels common between two sets, or TP) divided by the size of the union of the two sets (the total number of labels present in both sets, TP + FN + FP):</p> \\[ \\text{IoU} \\left( \\text{A}, \\, \\text{B} \\right) = \\frac {\\text{TP}} {\\text{TP} + \\text{FN} + \\text{FP}} \\] <p>The IoU for multi-label classification is defined per class. This technique, also known as one-vs-the-rest (OvR), evaluates each class as a binary classification problem. Per-class IoU values can then be aggregated using different averaging methods. The popular choice for this workflow is macro, so let\u2019s take a look at examples of different averaged IoU/Jaccard index metrics for multiclass multi-label classification:</p>"},{"location":"metrics/iou/#example-macro-iou-of-multi-label-classification","title":"Example: Macro IoU of Multi-label Classification","text":"<p>Consider the case of multi-label classification with classes <code>Airplane</code>, <code>Boat</code>, <code>Car</code>:</p> <p> Set Sample #1 Sample #2 Sample #3 Sample #4 ground truth <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Car</code> <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> inference <code>Boat</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <code>Airplane</code>, <code>Boat</code>, <code>Car</code> <p></p> \\[ \\text{ground truth} = [[1, 1, 1], \\, [1, 0, 1], \\, [0, 1, 1], \\, [1, 1, 1]] \\] \\[ \\text{inference} = [[0, 1, 0], \\, [1, 1, 1], \\, [1, 1, 1], \\, [1, 1, 1]] \\] \\[ \\begin{align} \\text{IoU}_\\text{macro} &amp;= \\frac {\\text{IoU}_\\texttt{Airplane} + \\text{IoU}_\\texttt{Boat} + \\text{IoU}_\\texttt{Car}} {3} \\\\[1em] &amp;= \\frac {\\frac 2 4 + \\frac 3 4 + \\frac 3 4} {3} \\\\[1em] &amp;= \\frac 2 3 \\end{align} \\]"},{"location":"metrics/iou/#limitations-and-biases","title":"Limitations and Biases","text":"<p>IoU works well to measure the overlap between two sets, whether they are types of geometry or a list of labels. However, this metric cannot be directly used to measure the overlap of an inference and <code>iscrowd</code> ground truth, which is an annotation from COCO Detection Challenge Evaluation used to label a\u00a0large groups of objects (e.g., a crowd of people). Therefore, the inference is expected to take up a small portion of the ground truth region, resulting in a low IoU score and a pair not being a valid match. In this scenario, a variation of IoU, called Intersection over Foreground (IoF), is preferred. This variation is used when there are ground truth regions you want to ignore in evaluation, such as <code>iscrowd</code>.</p> <p>The second limitation of IoU is measuring the localization performance of non-overlaps. IoU ranges from 0 (no overlap) to 1 (complete overlap), so when two bounding boxes have zero overlap, it\u2019s hard to tell how bad the localization performance is solely based on IoU. There are variations of IoU, such as signed IoU (SIoU) and generalized IoU (GIoU), that aim to measure the localization error even when there is no overlap. These metrics can replace IoU metric if the objective is to measure the localization performance of non-overlaps.</p>"},{"location":"metrics/meteor/","title":"METEOR","text":"<p>METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a widely recognized and vital metric used in natural language processing. Originally developed for machine translation workflows, it is used to measure the quality of candidate texts against reference texts for many different workflows. Though it is an n-gram based metric, it goes beyond traditional methods by factoring in elements such as precision, recall, and order to provide a comprehensive measure of text quality. For an in-depth justification behind METEOR's design choices, feel free to check out the original paper.</p>"},{"location":"metrics/meteor/#implementation-details","title":"Implementation Details","text":"<p>We define METEOR as the product of two components - the Unigram Precision / Recall Harmonic Mean, and Word Order Penalty. That is,</p> \\[ \\text{METEOR} = \\underbrace{\\text{FMean}}_{\\text{Harmonic Mean of Unigram Precision/Recall}} * \\underbrace{(1 - \\text{Penalty})}_{\\text{Word Order Penalty}} \\] <p>To understand the formula, let's break down each component into their respective parts.</p> FMean: Harmonic Mean of the Unigram Precision / Recall <p>This is defined as</p> \\[ FMean = \\frac{10PR}{R + 9P} \\] <p>where P represents the unigram precision, and R represents the unigram recall. Here's a recap of precision and recall. Notice that most of the weight is placed on the recall component by design \u2013 this allows METEOR to prioritize the coverage of essential keywords in the candidate text.</p> Penalty: Word Order Penalty <p>Since the FMean is based on unigram precision and recall, to take into account longer sequences, METEOR has a penalty factor to alleviate this weakness and enforce an order on the candidate sentence.</p> <p>First, the  unigrams in the candidate text that are mapped to unigrams in the reference text are grouped in such a way that there exists the fewest number of chunks, where each chunk consists of adjacent unigrams. Our penalty factor is then defined as:</p> \\[ Penalty = 0.5 \\times \\frac{\\text{# of Chunks}}{\\text{# of Unigrams Matched}} \\] <p>For example, if our candidate sentence was \"the president spoke to the audience\" and our reference sentence was \"the president then spoke to the audience\", there would be two chunks \u2013 \"the president\" and \"spoke to the audience\" \u2013 and 6 unigrams matched. Notice that as the chunks decrease, so does the penalty, which results in a higher METEOR score. This is quite intuitive as a lower number of chunks translates to an enforced order and better alignment with the reference text.</p>"},{"location":"metrics/meteor/#examples","title":"Examples","text":"Candidate Reference <code>Under the starry night, we danced with glee.</code> <code>We danced with joy under the starry night.</code> Step 1: Calculate FMean <p>Upon analysis, our precision is \\(\\frac{7}{8} = 0.875\\) and our recall is also \\(\\frac{7}{8} = 0.875\\). As a result, our FMean is</p> \\[ \\frac{10 \\times 0.875 \\times 0.875}{0.875 + 9 \\times 0.875} = 0.875 \\] Step 2: Calculate Word Order Penalty <p>We can break up our candidate sentence into two chunks to map it to our reference sentence.</p> <p>Candidate: \\(\\underbrace{\\text{Under the starry night}}_{\\text{Chunk 2}} \\space \\underbrace{\\text{we danced with}}_{\\text{Chunk 1}} \\space\\text{glee}\\)  Reference: \\(\\underbrace{\\text{We danced with}}_{\\text{Chunk 1}} \\space\\text{joy}\\space \\underbrace{\\text{under the starry night}}_{\\text{Chunk 2}}\\)</p> <p>Between the two chunks, we have matched 7 unigrams. This gives us a penalty score of \\(0.5 \\times \\frac{2}{7} = 0.143\\).</p> Step 3: Calculate METEOR <p>With our Penalty and FMean calculated, we can proceed with calculating the METEOR score.</p> \\[ \\text{METEOR} = 0.875 * (1 - 0.143) = 0.750. \\] <p>Not bad! We have a pretty high score for two sentences that are semantically very similar but have different orders.</p> <p>Lets try the same reference example with a slightly different candidate.</p> Candidate Reference <code>Danced we with under joy the night starry.</code> <code>We danced with joy under the starry night.</code> Step 1: Calculate FMean <p>Our first step is trivial. Since both sentences contain the same words, our unigram precision and recall are both 1.0. As a result, our FMean is \\(\\frac{10 \\times 1.0 \\times 1.0}{1.0 + 9 \\times 1.0} = 1\\)</p> Step 2: Calculate Word Order Penalty <p>Our penalty is different from the first example, due to the jumbled up order. We split our candidate sentence into 8 chunks, since no adjacent words can be mapped to the reference sentence.</p> <p>Candidate: \\(\\underbrace{\\text{Danced}}_\\text{Chunk 2}\\space\\underbrace{\\text{we}}_\\text{Chunk 1}\\space\\underbrace{\\text{with}}_\\text{Chunk 3}\\space\\underbrace{\\text{under}}_\\text{Chunk 5}\\space\\underbrace{\\text{joy}}_\\text{Chunk 4}\\space\\underbrace{\\text{the}}_\\text{Chunk 6}\\space\\underbrace{\\text{night}}_\\text{Chunk 8}\\space\\underbrace{\\text{starry}}_\\text{Chunk 7}\\space\\)</p> <p>Reference: \\(\\underbrace{\\text{We}}_\\text{Chunk 1}\\space\\underbrace{\\text{danced}}_\\text{Chunk 2}\\space\\underbrace{\\text{with}}_\\text{Chunk 3}\\space\\underbrace{\\text{joy}}_\\text{Chunk 4}\\space\\underbrace{\\text{under}}_\\text{Chunk 5}\\space\\underbrace{\\text{the}}_\\text{Chunk 6}\\space\\underbrace{\\text{starry}}_\\text{Chunk 7}\\space\\underbrace{\\text{night}}_\\text{Chunk 8}\\space\\)</p> <p>Between the eight chunks, we have matched 8 unigrams. This gives us a penalty score of \\(0.5 \\times \\frac{8}{8} = 0.5\\).</p> Step 3: Calculate METEOR <p>With our Penalty and FMean calculated, we can proceed with calculating the METEOR score.</p> \\[ \\text{METEOR} = 1 * (1 - 0.5) = 0.5. \\] <p>Despite having all the keywords of the reference sentence, our candidate had the wrong order and meaning! This is a massive improvement over something like ROUGE-1 which would not have considered the orders of the sentences, and given a perfect score of 1.0.</p>"},{"location":"metrics/meteor/#limitations-and-advantages","title":"Limitations and Advantages","text":"<p>Although METEOR was created to address some of the major limitations of BLEU, it still comes with its own limitations.</p> <ol> <li> <p>METEOR does not consider synonyms. Unlike embeddings-based metrics like BERTScore, it does not have a mechanism to quantify the similarity of words within the candidate and reference sentences. Thus, having two sentences like \"She looked extremely happy at the surprise party.\" and \"She appeared exceptionally joyful during the unexpected celebration.\" would yield a subobtimal score despite being very similar in meaning. That being said, METEOR has shown to have a higher correlation with human judgement than both BLEU and ROUGE, making it generally better than the two.</p> </li> <li> <p>METEOR can fail on context. If we have two sentences \"I am a big fan of Taylor Swift\" (Reference) and \"Fan of Taylor Swift I am big\" (Candidate), METEOR would yield a good score. However, the candidate sentence makes little sense and intuitively shouldn't be given a good score. This is a limitation with all n-gram metrics, and not specific to METEOR.</p> </li> </ol> <p>Limitations aside, METEOR is still a great metric to include in NLP workflows for measuring text similarity. Like other n-gram metrics, it is easy to compute and doesn't require extra hardware for inference. Furthermore, it is a noticeable improvement over BLEU, and even ROUGE, in many ways \u2013 it places weight on both precision and recall, factors in word order, and generally does a better job at filtering out bad candidate texts, as seen in example 2. It is also a better judge of global coherence than BLEU and ROUGE, since it greedily looks for the largest chunks to calculate its penalty factor, rather than using a sliding context window of n-grams. METEOR is a powerful metric, and should be included in every NLP toolbox to give a holistic view of model performance.</p>"},{"location":"metrics/pr-curve/","title":"Precision-Recall (PR) Curve","text":"<p>Guides: Precision and Recall</p> <p>Read the precision and the recall guides if you're not familiar with those metrics.</p> <p>A precision-recall (PR) curve is a plot that gauges machine learning model performance by using precision and recall, which are performance metrics that evaluate the quality of a classification model. The curve is built with precision on the y-axis and recall on the x-axis computed across many thresholds, showing a trade-off of how precision and recall values change when a classification threshold changes.</p> <ul> <li>  API Reference: <code>CurvePlot</code> \u2197</li> </ul>"},{"location":"metrics/pr-curve/#implementation-details","title":"Implementation Details","text":"<p>The curve\u2019s points (precisions and recalls) are calculated with a varying threshold, and made into points (precision values on the y-axis and recall values on the x-axis). Precision and recall are threshold-dependent metrics where a threshold value must be defined to compute them, and by computing and plotting these two metrics across many thresholds we can check how these metrics change depending on the threshold.</p> Thresholds Selection <p>Threshold ranges are very customizable. Typically, a uniformly spaced range of values from 0 to 1 can model a PR curve, where users pick the number of thresholds to include. Another common approach to picking thresholds is collecting and sorting the unique confidences of every prediction.</p>"},{"location":"metrics/pr-curve/#example-binary-classification","title":"Example: Binary Classification","text":"<p>Let's consider a simple binary classification example and plot a PR curve at a uniformly spaced range of thresholds. The table below shows six samples (four positive and two negative) sorted by their confidence score. Each inference is evaluated at each threshold: 0.25, 0.5, and 0.75. It's a negative prediction if its confidence score is below the evaluating threshold; otherwise, it's positive.</p> <p> Sample Confidence \u2193 Inference @ 0.25 Inference @ 0.5 Inference @ 0.75 Positive 0.9 Positive Positive Positive Positive 0.8 Positive Positive Positive Positive 0.7 Positive Positive Negative Negative 0.4 Positive Positive Negative Positive 0.35 Positive Negative Negative Negative 0.3 Positive Negative Negative <p></p> <p>As the threshold increases, there are fewer false positives and more false negatives, most likely yielding high precision and low recall. Conversely, decreasing the threshold may improve recall at the cost of precision. Let's compute the precision and recall values at each threhold.</p> <p> Threshold TP FP FN Precision Recall 0.25 4 2 0 \\(\\frac{4}{6}\\) \\(\\frac{4}{4}\\) 0.5 3 1 1 \\(\\frac{3}{4}\\) \\(\\frac{3}{4}\\) 0.75 2 0 2 \\(\\frac{2}{2}\\) \\(\\frac{2}{4}\\) <p></p> <p>Using these precision and recall values, a PR curve can be plotted:</p> <p> </p>"},{"location":"metrics/pr-curve/#example-multiclass-classification","title":"Example: Multiclass Classification","text":"<p>For multiple classes, it is common practice to plot a curve per class by treating each class as a binary classification problem. This technique is known as one-vs-rest (OvR). With this strategy, we can have <code>n</code> PR curves for <code>n</code> unique classes.</p> <p>Let's take a look at a multiclass classification example and plot per class PR curves for the same three thresholds that we used in the example above: 0.25, 0.5, and 0.75. In this example, we have three classes: <code>Airplane</code>, <code>Boat</code>, and <code>Car</code>. The multiclass classifier outputs a confidence score for each class:</p> <p> Label <code>Airplane</code> Confidence <code>Boat</code> Confidence <code>Car</code> Confidence <code>Airplane</code> 0.9 0.05 0.05 <code>Airplane</code> 0.7 0.1 0.2 <code>Airplane</code> 0.4 0.25 0.35 <code>Boat</code> 0.6 0.25 0.15 <code>Boat</code> 0.4 0.5 0.1 <code>Car</code> 0.25 0.25 0.5 <code>Car</code> 0.3 0.4 0.3 <p></p> <p>Just like the binary classification example, we are going to determine whether each inference is positive or negative depending on the evaluating threshold, so for class <code>Airplane</code>:</p> <p> Sample <code>Airplane</code> Confidence Inference @ 0.25 Inference @ 0.5 Inference @ 0.75 Positive 0.9 Positive Positive Positive Positive 0.7 Positive Positive Negative Positive 0.4 Positive Negative Negative Negative 0.6 Positive Positive Negative Negative 0.4 Positive Negative Negative Negative 0.25 Positive Negative Negative Negative 0.3 Positive Negative Negative <p></p> <p>And the precision and recall values for class <code>Airplane</code> can be computed:  Threshold TP FP FN <code>Airplane</code> Precision <code>Airplane</code> Recall 0.25 3 4 0 \\(\\frac{3}{7}\\) \\(\\frac{3}{3}\\) 0.5 2 1 1 \\(\\frac{2}{3}\\) \\(\\frac{2}{3}\\) 0.75 1 0 2 \\(\\frac{1}{1}\\) \\(\\frac{1}{3}\\) <p></p> <p>We are going to repeat this step to compute precision and recall for class <code>Boat</code> and <code>Car</code>.</p> <p> Threshold <code>Airplane</code> Precision <code>Airplane</code> Recall <code>Boat</code> Precision <code>Boat</code> Recall <code>Car</code> Precision <code>Car</code>  Recall 0.25 \\(\\frac{3}{7}\\) \\(\\frac{3}{3}\\) \\(\\frac{2}{4}\\) \\(\\frac{2}{2}\\) \\(\\frac{2}{3}\\) \\(\\frac{2}{2}\\) 0.5 \\(\\frac{2}{3}\\) \\(\\frac{2}{3}\\) \\(\\frac{1}{1}\\) \\(\\frac{1}{2}\\) \\(\\frac{1}{1}\\) \\(\\frac{1}{2}\\) 0.75 \\(\\frac{1}{1}\\) \\(\\frac{1}{3}\\) \\(\\frac{0}{0}\\) \\(\\frac{0}{2}\\) \\(\\frac{0}{0}\\) \\(\\frac{0}{2}\\) <p></p> <p>Using these precision and recall values, per class PR curves can be plotted:</p> <p> </p>"},{"location":"metrics/pr-curve/#area-under-the-pr-curve-auprc","title":"Area Under the PR Curve (AUPRC)","text":"<p>The area under the PR curve (AUPRC), also known as AUC-PR or PR-AUC, is a threshold-independent metric that summarizes the performance of a model depicted by a PR curve. The greater the area, the better a model performs. The average precision is one particular method for calculating the AUPRC. With PR curves, we can visually conclude which curves indicate that a certain class or model has a better performance.</p> <p> </p> <p>In the plot above, we see that the cyan curve has a higher precision than the purple curve for almost every recall value. This means that the model behind the cyan curve performs better.</p>"},{"location":"metrics/pr-curve/#limitations-and-biases","title":"Limitations and Biases","text":"<p>PR curves are a very common plot used in practice to evaluate model performance in terms of precision and recall. There are some pitfalls that might be overlooked: class imbalance, source of error, and poor threshold choices.</p> <ol> <li>Classes with too few data points may have PR curves that are poor representations of actual performance or overall performance. The performance of minority classes may be less accurate compared to a majority class.</li> <li>PR curves only gauge precision and recall based on classifications, they do not surface misclassification patterns or reasons for different types of errors.</li> <li>The values of the thresholds affect the shape of PR curves, which can affect how they are interpreted. Having a different number of thresholds, or having different threshold values, make PR curve comparisons difficult.</li> </ol>"},{"location":"metrics/precision/","title":"Precision","text":"<p>Precision measures the proportion of positive inferences from a model that are correct, ranging from 0 to 1 (where 1 is best).</p> <p>As shown in this diagram, precision is the fraction of all inferences that are correct:</p> \\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\] <p>In the above formula, \\(\\text{TP}\\) is the number of true positive inferences and \\(\\text{FP}\\) is the number of false positive inferences.</p> <p>Guide: True Positive / False Positive</p> <p>Read the TP / FP / FN / TN guide if you're not familiar with \"TP\" and \"FP\" terminology.</p> <p> </p> <ul> <li>  API Reference: <code>precision</code> \u2197</li> </ul>"},{"location":"metrics/precision/#implementation-details","title":"Implementation Details","text":"<p>Precision is used across a wide range of workflows, including classification, object detection, instance segmentation, semantic segmentation, and information retrieval. It is especially useful when the objective is to measure and reduce false positive inferences.</p> <p>For most workflows, precision is the ratio of the number of correct positive inferences to the total number of positive inferences:</p> \\[\\text{Precision} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}}\\] <p>For workflows with a localization component, such as object detection and instance segmentation, see the Geometry Matching guide to learn how to compute true positive and false positive counts.</p>"},{"location":"metrics/precision/#examples","title":"Examples","text":"<p>Perfect inferences:</p> Metric Value TP 20 FP 0 \\[ \\begin{align} \\text{Precision} &amp;= \\frac{20}{20 + 0} \\\\[1em] &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences, where some inferences are correct (TP) and others are incorrect (FP):</p> Metric Value TP 90 FP 10 \\[ \\begin{align} \\text{Precision} &amp;= \\frac{90}{90 + 10} \\\\[1em] &amp;= 0.9 \\end{align} \\] <p>Zero correct inferences \u2014 all positive predictions are incorrect:</p> Metric Value TP 0 FP 20 \\[ \\begin{align} \\text{Precision} &amp;= \\frac{0}{0 + 20} \\\\[1em] &amp;= 0.0 \\end{align} \\]"},{"location":"metrics/precision/#multiple-classes","title":"Multiple Classes","text":"<p>So far, we have only looked at binary classification/object detection cases, but in multiclass or multi-label cases, precision is computed per class. In the TP / FP / FN / TN guide, we went over multiple-class cases and how these metrics are computed. Once you have these four metrics computed per class, you can compute precision for each class by treating each as a single-class problem.</p>"},{"location":"metrics/precision/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single precision score that summarizes model performance across all classes, there are different ways to aggregate per-class precision scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/precision/#limitations-and-biases","title":"Limitations and Biases","text":"<p>As seen in its formula, precision only takes positive inferences (TP and FP) into account; negative inferences (TN and FN) are not considered. Thus, precision only provides one half of the picture, and should always be used in tandem with recall: recall penalizes false negatives (FN), whereas precision does not.</p> <p>For a single metric that takes both precision and recall into account, use F<sub>1</sub>-score, which is the harmonic mean between precision and recall.</p>"},{"location":"metrics/recall/","title":"Recall (TPR, Sensitivity)","text":"<p>Recall, also known as true positive rate (TPR) and sensitivity, measures the proportion of all positive ground truths that a model correctly predicts, ranging from 0 to 1 (where 1 is best).</p> <p>As shown in this diagram, recall is the fraction of all positive ground truths that are correctly predicted:</p> \\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\] <p>In the above formula, \\(\\text{TP}\\) is the number of true positive inferences and \\(\\text{FN}\\) is the number of false negative ground truths.</p> <p>Guide: True Positive / False Negative</p> <p>Read the TP / FP / FN / TN guide if you're not familiar with \"TP\" and \"FN\" terminology.</p> <p> </p> <ul> <li>  API Reference: <code>recall</code> \u2197</li> </ul>"},{"location":"metrics/recall/#implementation-details","title":"Implementation Details","text":"<p>Recall is used across a wide range of workflows, including classification, object detection, instance segmentation, semantic segmentation, and information retrieval. It is especially useful when the objective is to measure and reduce false negative ground truths, i.e. model misses.</p> <p>For most tasks, recall is the ratio of the number of correct positive inferences to the total number of positive ground truths.</p> \\[ \\text{Recall} = \\frac {\\text{# True Positives}} {\\text{# True Positives} + \\text{# False Negatives}} \\] <p>For workflows with a localization component, such as object detection and instance segmentation, see the Geometry Matching guide to learn how to compute true positive and false negative counts.</p>"},{"location":"metrics/recall/#examples","title":"Examples","text":"<p>Perfect model inferences, where every ground truth is recalled by an inference:</p> Metric Value TP 20 FN 0 \\[ \\begin{align} \\text{Recall} &amp;= \\frac{20}{20 + 0} \\\\[1em] &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences, where some ground truths are correctly recalled (TP) and others are missed (FN):</p> Metric Value TP 85 FN 15 \\[ \\begin{align} \\text{Recall} &amp;= \\frac{85}{85 + 15} \\\\[1em] &amp;= 0.85 \\end{align} \\] <p>Zero correct inferences \u2014 no positive ground truths are recalled:</p> Metric Value TP 0 FN 20 \\[ \\begin{align} \\text{Recall} &amp;= \\frac{0}{0 + 20} \\\\[1em] &amp;= 0.0 \\end{align} \\]"},{"location":"metrics/recall/#multiple-classes","title":"Multiple Classes","text":"<p>So far, we have only looked at binary classification/object detection cases, but in multiclass or multi-label cases, recall is computed per class. In the TP / FP / FN / TN guide, we went over multiple-class cases and how these metrics are computed. Once you have these four metrics computed per class, you can compute recall for each class by treating each as a single-class problem.</p>"},{"location":"metrics/recall/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single recall score that summarizes model performance across all classes, there are different ways to aggregate per-class recall scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/recall/#limitations-and-biases","title":"Limitations and Biases","text":"<p>As seen in its formula, recall only takes positive ground truths (TP and FN) into account; negative ground truths (TN and FP) are not considered. Thus, recall only provides one half of the picture, and should always be used in tandem with precision: precision penalizes false positives (FP), whereas recall does not.</p> <p>For a single metric that takes both precision and recall into account, use F<sub>1</sub>-score, which is the harmonic mean between precision and recall.</p>"},{"location":"metrics/roc-curve/","title":"ROC Curve","text":"<p>A receiver operating characteristic (ROC) curve is a plot that is used to evaluate the performance of binary classification models by using the true positive rate (TPR) and the false positive rate (FPR). The curve is built with the TPR on the y-axis and the FPR on the x-axis computed across many thresholds, showing a trade-off of how TPR and FPR values change when a classification threshold changes.</p> <p>Guides: TPR and FPR</p> <p>The TPR is also known as sensitivity or recall, and it represents the proportion of true positive inferences (correctly predicted positive instances) among all actual positive instances. The FPR is the proportion of false positive inferences (incorrectly predicted positive instances) among all actual negative instances. Read the true positive rate (TPR) and the false positive rate (FPR) guides to learn more about these metrics.</p> <ul> <li>  API Reference: <code>CurvePlot</code> \u2197</li> </ul>"},{"location":"metrics/roc-curve/#implementation-details","title":"Implementation Details","text":"<p>The curve\u2019s points (TPRs and FPRs) are calculated with a varying threshold, and made into points (TPR values on the y-axis and FPR values on the x-axis). TPR and FPR metrics are threshold-dependent where a threshold value must be defined to compute them, and by computing and plotting these two metrics across many thresholds we can check how these metrics change depending on the threshold.</p> Thresholds Selection <p>Threshold ranges are very customizable. Typically, a uniformly spaced range of values from 0 to 1 can model a ROC curve, where users pick the number of thresholds to include. Another common approach to picking thresholds is collecting and sorting the unique confidences of every prediction.</p>"},{"location":"metrics/roc-curve/#example-binary-classification","title":"Example: Binary Classification","text":"<p>Let's consider a simple binary classification example and plot a ROC curve at a uniformly spaced range of thresholds. The table below shows eight samples (four positive and four negative) sorted by their confidence score. Each inference is evaluated at each threshold: 0.25, 0.5, and 0.75. It's a negative prediction if its confidence score is below the evaluating threshold; otherwise, it's positive.</p> <p> Sample Confidence \u2193 Inference @ 0.25 Inference @ 0.5 Inference @ 0.75 Positive 0.9 Positive Positive Positive Positive 0.8 Positive Positive Positive Negative 0.75 Positive Positive Positive Positive 0.7 Positive Positive Negative Negative 0.5 Positive Positive Negative Positive 0.35 Positive Negative Negative Negative 0.3 Positive Negative Negative Negative 0.2 Negative Negative Negative <p></p> <p>As the threshold increases, there are fewer true positives and fewer false positives, most likely yielding lower TPR and FPR. Conversely, decreasing the threshold may increase both TPR and FPR. Let's compute the TPR and FPR values at each threhold using the following formulas:</p> \\[\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\] \\[\\text{FPR} = \\frac{\\text{FP}}{\\text{TN} + \\text{FP}}\\] <p> Threshold TP FP FN TN TPR FPR 0.25 4 3 0 1 \\(\\frac{4}{4}\\) \\(\\frac{3}{4}\\) 0.5 3 2 1 2 \\(\\frac{3}{4}\\) \\(\\frac{2}{4}\\) 0.75 2 1 2 3 \\(\\frac{2}{4}\\) \\(\\frac{1}{4}\\) <p></p> <p>Using these TPR and FPR values, a ROC curve can be plotted:</p> <p> </p>"},{"location":"metrics/roc-curve/#example-multiclass-classification","title":"Example: Multiclass Classification","text":"<p>For multiple classes, a curve is plotted per class by treating each class as a binary classification problem. This technique is known as one-vs-rest (OvR). With this strategy, we can have <code>n</code> ROC curves for <code>n</code> unique classes.</p> <p>Let's take a look at a multiclass classification example and plot per class ROC curves for the same three thresholds that we used in the example above: 0.25, 0.5, and 0.75. In this example, we have three classes: <code>Airplane</code>, <code>Boat</code>, and <code>Car</code>. The multiclass classifier outputs a confidence score for each class:</p> <p> Sample # Label <code>Airplane</code> Confidence <code>Boat</code> Confidence <code>Car</code> Confidence 1 <code>Airplane</code> 0.9 0.05 0.05 2 <code>Airplane</code> 0.7 0.05 0.25 3 <code>Airplane</code> 0.25 0.25 0.5 4 <code>Boat</code> 0.6 0.25 0.15 5 <code>Boat</code> 0.4 0.5 0.1 6 <code>Car</code> 0.25 0.25 0.5 7 <code>Car</code> 0.05 0.7 0.25 <p></p> <p>Just like the binary classification example, we are going to determine whether each inference is positive or negative depending on the evaluating threshold, so for class <code>Airplane</code>:</p> <p> Sample # Sample <code>Airplane</code> Confidence \u2193 Inference @ 0.25 Inference @ 0.5 Inference @ 0.75 1 Positive 0.9 Positive Positive Positive 2 Positive 0.7 Positive Positive Negative 4 Negative 0.6 Positive Positive Negative 5 Negative 0.4 Positive Negative Negative 3 Positive 0.25 Positive Negative Negative 6 Negative 0.25 Positive Negative Negative 7 Negative 0.05 Negative Negative Negative <p></p> <p>And the TPR and FPR values for class <code>Airplane</code> can be computed:  Threshold TP FP FN TN <code>Airplane</code> TPR <code>Airplane</code> FPR 0.25 3 3 0 1 \\(\\frac{3}{3}\\) \\(\\frac{3}{4}\\) 0.5 2 1 1 3 \\(\\frac{2}{3}\\) \\(\\frac{1}{4}\\) 0.75 1 0 2 4 \\(\\frac{1}{3}\\) \\(\\frac{0}{4}\\) <p></p> <p>We are going to repeat this step to compute TPR and FPR for class <code>Boat</code> and <code>Car</code>.</p> <p> Threshold <code>Airplane</code> TPR <code>Airplane</code> FPR <code>Boat</code> TPR <code>Boat</code> FPR <code>Car</code> TPR <code>Car</code> FPR 0.25 \\(\\frac{3}{3}\\) \\(\\frac{3}{4}\\) \\(\\frac{2}{2}\\) \\(\\frac{3}{5}\\) \\(\\frac{2}{2}\\) \\(\\frac{2}{5}\\) 0.5 \\(\\frac{2}{3}\\) \\(\\frac{1}{4}\\) \\(\\frac{1}{2}\\) \\(\\frac{1}{5}\\) \\(\\frac{1}{2}\\) \\(\\frac{1}{5}\\) 0.75 \\(\\frac{1}{3}\\) \\(\\frac{0}{4}\\) \\(\\frac{0}{2}\\) \\(\\frac{0}{5}\\) \\(\\frac{0}{2}\\) \\(\\frac{0}{5}\\) <p></p> <p>Using these TPR and FPR values, per class ROC curves can be plotted:</p> <p> </p>"},{"location":"metrics/roc-curve/#area-under-the-roc-curve-auc-roc","title":"Area Under the ROC Curve (AUC ROC)","text":"<p>The area under the ROC curve (AUC ROC) is a threshold-independent metric that summarizes the performance of a model depicted by a ROC curve. A perfect classifier would have an AUC ROC value of 1. The greater the area, the better a model performs at classifying the positive and negative instances. Using AUC ROC metric alongside other evaluation metrics, we can assess and compare the performance of different models and choose the one that best suits their specific problem.</p> <p>Let's take a look at the binary classification example again. Given the following TPR and FPR values, the AUC ROC can be computed:</p> <p> Threshold TP FP FN TN TPR FPR 0.25 4 3 0 1 \\(\\frac{4}{4}\\) \\(\\frac{3}{4}\\) 0.5 3 2 1 2 \\(\\frac{3}{4}\\) \\(\\frac{2}{4}\\) 0.75 2 1 2 3 \\(\\frac{2}{4}\\) \\(\\frac{1}{4}\\) <p></p> <p>The area under the curve can computed by taking an integral along the x-axis using the composite trapezoidal rule:</p> \\[ \\begin{align} \\text{AUC ROC} &amp;= \\int y(x) \\, dx \\\\[1em] &amp;\\approx \\sum_{k=1}^{N} \\frac{y(x_{k-1}) + y(x_{k})}{2} \\Delta x_{k} \\\\[1em] &amp;= \\frac{\\frac{1}{4}(\\frac{4}{4} + \\frac{3}{4})}{2} + \\frac{\\frac{1}{4}(\\frac{3}{4} + \\frac{2}{4})}{2} \\\\[1em] &amp;= \\frac{7}{32} + \\frac{5}{32} \\\\[1em] &amp;= \\frac{3}{8} \\\\[1em] \\end{align} \\] <p>For Python implementation, we recommend using NumPy's <code>np.trapz(y, x)</code> or scikit-learn's <code>sklearn.metrics.auc</code>.</p>"},{"location":"metrics/roc-curve/#limitations-and-biases","title":"Limitations and Biases","text":"<p>While ROC curves and AUC ROC are widely used in machine learning for evaluating classification models, they do have limitations and potential biases that should be considered:</p> <ol> <li>Sensitive to Class Imbalance: Classes with too few data points may have ROC curves that are poor representations of actual performance or overall performance. The performance of minority classes may be less accurate compared to a majority class.</li> <li>Partial Insight to Model Performance: ROC curves only gauge TPR and FPR based on classifications, they do not surface misclassification patterns or reasons for different types of errors. AUC ROC treats false positives and false negatives equally which may not be appropriate in situations where one type of error is more costly or impactful than the other. In such cases, other evaluation metrics like precision or PR curve can be used.</li> <li>Dependence on Threshold: The values of the thresholds affect the shape of ROC curves, which can affect how they are interpreted. Having a different number of thresholds, or having different threshold values, make ROC curve comparisons difficult.</li> </ol>"},{"location":"metrics/rouge-n/","title":"ROUGE-N","text":"<p>ROUGE vs. Recall</p> <p>Complimentary to BLEU, ROUGE-N can be thought of as an analog to recall for text comparisons.</p> <p>ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation), a metric within the broader ROUGE metric collection, is a vital metric in the field of natural language processing and text evaluation. It assesses the quality of a candidate text by measuring the overlap of n-grams between the candidate text and reference texts. ROUGE-N provides insights into the ability of a system to capture essential content and linguistic nuances, making it an important and versatile tool used in many NLP workflows. As the name implies, it is a recall-based metric \u2014 a complement to the precision-based BLEU score.</p>"},{"location":"metrics/rouge-n/#implementation-details","title":"Implementation Details","text":"<p>Formally, ROUGE-N is an n-gram recall between a candidate and a set of reference texts. That is, ROUGE-N calculates the number of overlapping n-grams between the generated and reference texts divided by the total number of n-grams in the reference texts.</p> <p>Mathematically, we define ROUGE-N as follows:</p> \\[ \\text{ROUGE-N} = \\frac{\\sum_{S \\in \\text{Reference Texts}} \\sum_{\\text{n-gram} \\in S} \\text{Match}(\\text{n-gram})}{\\sum_{S \\in \\text{Reference Texts}} \\sum_{\\text{n-gram} \\in S} \\text{Count}(\\text{n-gram})} \\] <p>where \\(\\text{Match(n-gram)}\\) is the maximum number of n-grams co-occuring in a candidate text and set of reference texts.</p> <p>It is clear that ROUGE-N is analogous to a recall-based measure, since the denominator of the equation is the sum of the number of n-grams on the reference-side.</p>"},{"location":"metrics/rouge-n/#multiple-references-taking-the-max","title":"Multiple References - Taking the Max","text":"<p>We usually want to compare a candidate text against multiple reference texts, as there is no single correct reference text that can capture all semantic nuances. Though the above formula is sufficient for calculating ROUGE-N across multiple references, another proposed way of calculating ROUGE-N across multiple references, as highlighted in the original ROUGE paper, is as follows:</p> <p>We compute pairwise ROUGE-N between a candidate text s, and every reference, \\(r_i\\), in the reference set, then take the maximum of pairwise ROUGE-N scores as the final multiple reference ROUGE-N score. That is,</p> \\[ \\text{ROUGE-N}_\\text{multi} = argmax_i \\space \\text{ROUGE-N}(r_i, s) \\] <p>The decision to use classic \\(\\text{ROUGE-N}\\) or \\(\\text{ROUGE-N}_\\text{multi}\\) is up to the user.</p>"},{"location":"metrics/rouge-n/#interpretation","title":"Interpretation","text":"<p>When using ROUGE-N, it is important to consider the metric for multiple values of N (n-gram length). For smaller values of N, like ROUGE-1, it places more focus on capturing the presence of keywords or content terms in the candidate text. For larger values of N, like ROUGE-3, it places focus on syntactic structure and replicating linguistic patterns. In other words, ROUGE-1 and small values of N are suitable for tasks where the primary concern is to assess whether the candidate text contains essential vocabulary, while ROUGE-3 and larger values of N are used in tasks where sentence structure and fluency are important.</p> <p>Generally speaking, a higher ROUGE-N score is desirable. However, the score varies among different tasks and values of N, so it is a good idea to benchmark your model's ROUGE-N score against other models trained on the same data, or previous iterations of your model.</p>"},{"location":"metrics/rouge-n/#examples","title":"Examples","text":""},{"location":"metrics/rouge-n/#rouge-1-unigrams","title":"ROUGE-1 (Unigrams)","text":"<p>Assume we have the following candidate and reference texts:</p> Reference #1 <code>A fast brown dog jumps over a sleeping fox</code> Reference #2 <code>A quick brown dog jumps over the fox</code> Candidate <code>The quick brown fox jumps over the lazy dog</code> Step 1: Tokenization &amp; n-Grams <p>Splitting our candidate and reference texts into unigrams, we get the following:</p> Reference #1 [<code>A</code>, <code>fast</code>, <code>brown</code>, <code>dog</code>, <code>jumps</code>, <code>over</code>, <code>a</code>, <code>sleeping</code>, <code>fox</code>] Reference #2 [<code>A</code>, <code>quick</code>, <code>brown</code>, <code>dog</code>, <code>jumps</code>, <code>over</code>, <code>the</code>, <code>fox</code>] Candidate [<code>The</code>, <code>quick</code>, <code>brown</code>, <code>fox</code>, <code>jumps</code>, <code>over</code>, <code>the</code>, <code>lazy</code>, <code>dog</code>] Step 2: Calculate ROUGE <p>Recall that our ROUGE-N formula is: \\(\\frac{\\text{# of overlapping n-grams}}{\\text{# of unigrams in reference}}\\)</p> <p>There are 5 overlapping unigrams in the first reference and 7 in the second reference, and 9 total unigrams in the first reference and 8 in the second. Thus our calculated ROUGE-1 score is \\(\\frac{12}{17} \\approx 0.706\\)</p>"},{"location":"metrics/rouge-n/#rouge-2-bigrams","title":"ROUGE-2 (Bigrams)","text":"<p>Assume we have the same following candidate and reference texts:</p> Reference #1 <code>A fast brown dog jumps over a sleeping fox</code> Reference #2 <code>A quick brown dog jumps over the fox</code> Candidate <code>The quick brown fox jumps over the lazy dog</code> Step 1: Tokenization &amp; n-Grams <p>Splitting our candidate and reference texts into bigrams, we get the following:</p> Reference #1 [<code>A fast</code>, <code>fast brown</code>, <code>brown dog</code>, <code>dog jumps</code>, <code>jumps over</code>, <code>over a</code>, <code>a sleeping</code>, <code>sleeping fox</code>] Reference #2 [<code>A quick</code>, <code>quick brown</code>, <code>brown dog</code>, <code>dog jumps</code>, <code>jumps over</code>, <code>over the</code>, <code>the fox</code>] Candidate [<code>The quick</code>, <code>quick brown</code>, <code>brown fox</code>, <code>fox jumps</code>, <code>jumps over</code>, <code>over the</code>, <code>the lazy</code>, <code>lazy dog</code>] Step 2: Calculate ROUGE <p>Recall that our ROUGE-N formula is: \\(\\frac{\\text{# of overlapping n-grams}}{\\text{# of unigrams in reference}}\\)</p> <p>There is 1 overlapping bigram in the first reference and 3 in the second reference, and 8 total bigrams in the first reference and 7 in the second. Thus our calculated ROUGE-2 score is \\(\\frac{4}{15} = 0.267\\)</p> <p>Note that our ROUGE-2 score is significantly lower than our ROUGE-1 score on the given candidate and reference texts. It is always important to consider multiple n-grams when using ROUGE-N, as one value of N does not give a holistic view of candidate text quality.</p>"},{"location":"metrics/rouge-n/#limitations-and-advantages","title":"Limitations and Advantages","text":"<p>ROUGE-N, like any other n-gram based metric, suffers from the following limitations:</p> <ol> <li> <p>Unlike BERTScore, ROUGE-N is not able to consider order, context, or semantics when calculating a score. Since it only relies on overlapping n-grams, it can not tell when a synonym is being used or if the placement of two matching n-grams have any meaning on the overall sentence. As a result, the metric may not be a perfect representation of the quality of the text, but rather the \"likeness\" of the n-grams in two sentences. Take for example, the ROUGE-2 score of \"This is an example of text\" and \"Is an example of text this\". Both ROUGE-1 and ROUGE-2 would give this a (nearly) perfect score, but the second sentence makes absolutely no sense!</p> </li> <li> <p>ROUGE-N can not capture global coherence. Given a long paragraph, realistically, having too large of a value for N would not return a meaningful score for two sentences, but having a reasonable number like N = 3 wouldn't be able to capture the flow of the text. The score might yield good results, but the entire paragraph might not flow smoothly at all. This is a weakness of n-gram based metrics, as they are limited to short context windows.</p> </li> </ol> <p>That being said, ROUGE-N has some advantages over embeddings-based metrics. First of all, it is very simple and easy to compute \u2014 it is able to calculate scores for large corpuses efficiently with no specialized hardware. ROUGE-N is also relatively easy to interpret. The N value can be adjusted to measure the granularity of measurements, and higher scores indicate greater overlap with the reference text. In fact, ROUGE is very widely used in NLP which allows engineers to benchmark their models against others on most open source NLP datasets. Lastly, it can be used in complement with other n-gram based metrics like BLEU to provide a more holistic view of test results \u2014 since BLEU provides a precision-related score, and ROUGE provides a recall-related score, it makes it easier to pinpoint potential failure cases.</p>"},{"location":"metrics/specificity/","title":"Specificity (TNR)","text":"<p>Specificity, also known as true negative rate (TNR), measures the proportion of negative ground truths that a model correctly predicts, ranging from 0 to 1. A high specificity indicates that the model is effective at correctly identifying negative cases, where a low specificity suggests that the model is misclassifying many negative cases as positive.</p> <p>As shown in this diagram, specificty is the fraction of all negative ground truths that are correctly predicted:</p> \\[\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\\] <p>In the above formula, \\(\\text{TN}\\) is the number of true negative inferences and \\(\\text{FP}\\) is the number of false positive inferences.</p> <p>Guide: True Negative / False Positive</p> <p>Read the TP / FP / FN / TN guide if you're not familiar with \"TN\" and \"FP\" terminology.</p> <p> </p> <p>Specificity is often used in conjuction with sensitivity (recall), also known true positive rate (TPR). By measuring both specificity and sensitivity, a more complete picture of a model's performance can be drawn.</p> <ul> <li>  API Reference: <code>Specificity</code> \u2197</li> </ul>"},{"location":"metrics/specificity/#implementation-details","title":"Implementation Details","text":"<p>Specificity is used to evaluate the performance of a classification model, particularly in tasks like binary classification, where the goal is to classify data into one of two possible classes. It is especially useful when the objective is to measure the model's ability to correctly identify the negative class instances. For example, in medical diagnostics, a high specificity is essential to avoid unnecessary treatments or alarm.</p> <p>Here is how specificity is calculated:</p> \\[ \\text{Specificity} = \\frac {\\text{# True Negatives}} {\\text{# True Negatives} + \\text{# False Positives}} \\]"},{"location":"metrics/specificity/#examples","title":"Examples","text":"<p>Perfect model inferences, where every negative ground truth is recalled by an inference:</p> Metric Value TN 20 FP 0 \\[ \\begin{align} \\text{Specificity} &amp;= \\frac{20}{20 + 0} \\\\[1em] &amp;= 1.0 \\end{align} \\] <p>Partially correct inferences, where some negative ground truths are correctly recalled (TN) and others are missed (FP):</p> Metric Value TN 85 FP 15 \\[ \\begin{align} \\text{Specificity} &amp;= \\frac{85}{85 + 15} \\\\[1em] &amp;= 0.85 \\end{align} \\] <p>Zero correct inferences \u2014 no negative ground truths are recalled:</p> Metric Value TN 0 FP 20 \\[ \\begin{align} \\text{Specificity} &amp;= \\frac{0}{0 + 20} \\\\[1em] &amp;= 0.0 \\end{align} \\]"},{"location":"metrics/specificity/#multiple-classes","title":"Multiple Classes","text":"<p>So far, we have only looked at binary classification cases, but in multiclass or multi-label cases, specificity is computed per class. In the TP / FP / FN / TN guide, we went over multiple-class cases and how these metrics are computed. Once you have these four metrics computed per class, you can compute specificity for each class by treating each as a single-class problem.</p>"},{"location":"metrics/specificity/#aggregating-per-class-metrics","title":"Aggregating Per-class Metrics","text":"<p>If you are looking for a single specificity score that summarizes model performance across all classes, there are different ways to aggregate per-class specificity scores: macro, micro, and weighted. Read more about these methods in the Averaging Methods guide.</p>"},{"location":"metrics/specificity/#limitations-and-biases","title":"Limitations and Biases","text":"<p>While specificity is a valuable metric for evaluating the performance of classification models, it does have limitations and potential biases that should be considered when interpreting results:</p> <ol> <li> <p>Sensitivity to Class Imbalance: Specificity is sensitive to class imbalance in dataset. If one class significantly outnumbers the other, a high specificity can be achieved simply by predicting the majority class most of the time. This can lead to a misleadingly high specificity score while neglecting the model's ability to correctly classify the minority class.</p> </li> <li> <p>Ignoring False Negatives: Specificity focuses exclusively on the true negatives (correctly classified negative cases) and false positives (negative cases incorrectly classified as positive), but it doesn't account for false negatives (positive cases incorrectly classified as negative). Ignoring false negatives can be problematic in applications where missing positive cases is costly or has severe consequences.</p> </li> <li> <p>Incomplete Context: Specificity alone does not provide a complete picture of a model's performance. It is often used in conjunction with other metrics like sensitivity (recall), precision, and F<sub>1</sub>-score to provide a more comprehensive assessment. Depending solely on specificity might hide issues related to other aspects of classification, such as the models' ability to identify true positives.</p> </li> <li> <p>Threshold Dependence: Specificity is a binary metric that doesn't take into account the probability or confidence levels associated with predictions. Models with different probability distributions might achieve the same specificity score, but their operational characteristics can vary significantly. To address this limitation, consider using threshold-independent metrics like the area under the receiver operating characteristic curve (AUC-ROC) which can provide a more comprehensive understanding of model performance.</p> </li> </ol>"},{"location":"metrics/tp-fp-fn-tn/","title":"TP / FP / FN / TN","text":"<p>The counts of true positive (TP), false positive (FP), false negative (FN), and true negative (TN) ground truths and inferences are essential for summarizing model performance. These metrics are the building blocks of many other metrics, including accuracy, precision, and recall.</p> Metric Description True Positive TP An instance for which both predicted and actual values are positive False Positive FP An instance for which predicted value is positive but actual value is negative False Negative FN An instance for which predicted value is negative but actual value is positive True Negative TN An instance for which both predicted and actual values are negative <p>To compute these metrics, each inference is compared to a ground truth and categorized into one of the four groups. Let\u2019s say we\u2019re building a dog classifier that predicts whether an image has a dog or not:</p> <p> Positive Inference (Dog) Negative Inference (No Dog) Positive Ground Truth (Dog) True Positive (TP) False Negative (FN) Negative Ground Truth (No Dog) False Positive (FP) True Negative (TN) <p></p> <p>Images of a dog are positive samples, and images without a dog are negative samples.</p> <p>If a classifier predicts that there is a dog on a positive sample, that inference is a true positive (TP). If that classifier predicts that there isn\u2019t a dog on a positive sample, that inference is a false negative (FN).</p> <p>Similarly, if that classifier predicts that there is a dog on a negative sample, that inference is a false positive (FP). A negative inference on a negative sample is a true negative (TN).</p>"},{"location":"metrics/tp-fp-fn-tn/#implementation-details","title":"Implementation Details","text":"<p>The TP / FP / FN / TN metrics have been around for a long time and are mainly used to evaluate classification, detection, and segmentation models.</p> <p>The implementation of these metrics is simple and straightforward. That said, there are different guidelines and edge cases to be aware of for binary and multiclass problems as well as object detection and other workflows.</p>"},{"location":"metrics/tp-fp-fn-tn/#classification","title":"Classification","text":"<p>There are three types of classification workflows: binary, multiclass, and multi-label.</p>"},{"location":"metrics/tp-fp-fn-tn/#binary","title":"Binary","text":"<p>In binary classification workflow, TP, FN, FP, and TN are implemented as follows:</p> Variable Type Description <code>ground_truths</code> <code>List[bool]</code> Ground truth labels, where <code>True</code> indicates a positive sample <code>inferences</code> <code>List[float]</code> Predicted confidence scores, where a higher score indicates a higher confidence of the sample being positive <code>T</code> <code>float</code> Threshold value to compare against the inference\u2019s confidence score, where <code>score &gt;= T</code> is positive <p>Should Threshold Be Inclusive or Exclusive?</p> <p>A confidence threshold is defined as \"the minimum score that the model will consider the inference to be positive (i.e. true)\". Therefore, it is a standard practice to consider inferences with confidence score greater than or equal to the confidence threshold as positive.</p> <p>With these inputs, TP / FP/ FN / TN metrics are defined:</p> <pre><code>TP = sum(    gt and inf &gt;= T for gt, inf in zip(ground_truths, inferences))\nFP = sum(not gt and inf &gt;= T for gt, inf in zip(ground_truths, inferences))\nFN = sum(    gt and inf &lt;  T for gt, inf in zip(ground_truths, inferences))\nTN = sum(not gt and inf &lt;  T for gt, inf in zip(ground_truths, inferences))\n</code></pre> Example: Binary Classification <p>This example considers five samples with the following ground truths, inferences, and threshold:</p> <pre><code>ground_truths = [False, True, False, False, True]\ninferences = [0.3, 0.2, 0.9, 0.4, 0.5]\nT = 0.5\n</code></pre> <p>Using the above formula for TP, FP, FN, and TN yields the following metrics:</p> <pre><code>print(f\"TP={TP}, FP={FP}, FN={FN}, TN={TN}\")\n# TP=1, FN=1, FP=1, TN=2\n</code></pre>"},{"location":"metrics/tp-fp-fn-tn/#multiclass","title":"Multiclass","text":"<p>TP / FP / FN / TN metrics are computed a little differently in a multiclass classification workflow.</p> <p>For a multiclass classification workflow, these four metrics are defined per class. This technique, also known as one-vs-rest (OvR), essentially evaluates each class as a binary classification problem.</p> <p>Consider a classification problem where a given image belongs to either the <code>Airplane</code>, <code>Boat</code>, or <code>Car</code> class. Each of these TP / FP / FN / TN metrics is computed for each class. For class <code>Airplane</code>, the metrics are defined as follows:</p> Metric Example True Positive Any image predicted as an <code>Airplane</code> that is labeled as an <code>Airplane</code> False Positive Any image predicted as an <code>Airplane</code> that is not labeled as an <code>Airplane</code> (e.g. labeled as <code>Boat</code> but predicted as <code>Airplane</code>) False Negative Any image not predicted as an <code>Airplane</code> that is labeled as an <code>Airplane</code> (e.g. labeled as <code>Airplane</code> but predicted as <code>Car</code>) True Negative Any image not predicted as an <code>Airplane</code> that is not labeled as an <code>Airplane</code> (e.g. labeled as <code>Boat</code> but predicted as <code>Boat</code> or <code>Car</code>)"},{"location":"metrics/tp-fp-fn-tn/#multi-label","title":"Multi-label","text":"<p>In a multi-label classification workflow, TP / FP / FN / TN are computed per class, like in multiclass classification.</p> <p>A sample is considered to be a positive one if the ground truth includes the evaluating class; otherwise, it\u2019s a negative sample. The same logic can be applied to the inferences, so, for example, if a classifier predicts that this sample belongs to class <code>Airplane</code> and <code>Boat</code>, and the ground truth for the same sample is only class <code>Airplane</code>, then this sample is considered to be a TP for class <code>Airplane</code>, and FP for class <code>Boat</code>.</p> <p>Multi-label classification workflow can alternately be thought of as a collection of binary classification workflows.</p>"},{"location":"metrics/tp-fp-fn-tn/#object-detection","title":"Object Detection","text":"<p>There are some differences in how these four metrics work for a detection workflow compared to a classification workflow. Rather than being computed at the sample level (e.g. per image), they're computed at the instance level (i.e. per object) for instances that the model is detecting. When given an image with multiple objects, each inference and each ground truth is assigned to one group, and the definitions of the terms are slightly altered:</p> Metric Description True Positive TP Positive inference (<code>score &gt;= T</code>) that is matched with a ground truth False Positive FP Positive inference (<code>score &gt;= T</code>) that is not matched with a ground truth False Negative FN Ground truth that is not matched with an inference or that is matched with a negative inference (<code>score &lt; T</code>) True Negative TN <p> Poorly defined for object detection! </p><p>In object detection workflow, a true negative is any non-object that isn't detected as an object. This isn't well defined and as such true negative isn't a commonly used metric in object detection.</p>Occasionally, for object detection workflow \"true negative\" is used to refer to any image that does not have any true positive or false positive inferences. <p>In object detection workflow, checking for detection correctness requires a couple of other metrics (e.g., Intersection over Union (IoU) and Geometry Matching).</p>"},{"location":"metrics/tp-fp-fn-tn/#single-class","title":"Single-class","text":"<p>Let\u2019s assume that a matching algorithm has already been run on all inferences and that the matched pairs and unmatched ground truths and inferences are given. Consider the following variables, adapted from <code>match_inferences</code>:</p> Variable Type Description <code>matched</code> <code>List[Tuple[GT, Inf]]</code> List of matched ground truth and inference bounding box pairs <code>unmatched_gt</code> <code>List[GT]</code> List of unmatched ground truth bounding boxes <code>unmatched_inf</code> <code>List[Inf]</code> List of unmatched inference bounding boxes <code>T</code> <code>float</code> Threshold used to filter valid inference bounding boxes based on their confidence scores <p>Then these metrics are defined:</p> <pre><code>TP = len([inf.score &gt;= T for _, inf in matched])\nFN = len([inf.score &lt;  T for _, inf in matched]) + len(unmatched_gt)\nFP = len([inf.score &gt;= T for inf in unmatched_inf])\n</code></pre> Example: Single-class Object Detection <p> </p> <p> Bounding Box Score IoU(\\(\\text{A}\\)) IoU(\\(\\text{B}\\)) \\(\\text{a}\\) 0.98 0.9 0.0 \\(\\text{b}\\) 0.6 0.0 0.13 <p></p> <p>This example includes two ground truths and two inferences, and when computed with an IoU threshold of 0.5 and confidence score threshold of 0.5 yields:</p> <p> TP FP FN 1 1 1 <p></p>"},{"location":"metrics/tp-fp-fn-tn/#multiclass_1","title":"Multiclass","text":"<p>Like classification, multiclass object detection workflow compute TP / FP / FN per class.</p> Example: Multiclass Object Detection <p> </p> <p> Bounding Box Class Score IoU(\\(\\text{A}\\)) \\(\\text{A}\\) <code>Apple</code> \u2014 \u2014 \\(\\text{a}\\) <code>Apple</code> 0.3 0.0 \\(\\text{b}\\) <code>Banana</code> 0.5 0.8 <p></p> <p>Similar to multiclass classification, TP / FP / FN are computed for class <code>Apple</code> and class <code>Banana</code> separately.</p> <p>Using an IoU threshold of 0.5 and a confidence score threshold of 0.5, this example yields:</p> <p> Class TP FP FN <code>Apple</code> 0 0 1 <code>Banana</code> 0 1 0 <p></p>"},{"location":"metrics/tp-fp-fn-tn/#averaging-per-class-metrics","title":"Averaging Per-class Metrics","text":"<p>For problems with multiple classes, these TP / FP / FN / TN metrics are computed for each class. If you are looking for a single score that summarizes model performance across all classes, there are a few different ways to aggregate per-class metrics: macro, micro, and weighted.</p> <p>Read more about these different averaging methods in the Averaging Methods guide.</p>"},{"location":"metrics/tp-fp-fn-tn/#limitations-and-biases","title":"Limitations and Biases","text":"<p>TP, FP, FN, and TN are four metrics based on the assumption that each sample/instance can be classified as a positive or a negative, thus they can only be applied to single-class applications. The workaround for multiple-class applications is to compute these metrics for each label using the one-vs-rest (OvR) strategy and then treat it as a single-class problem.</p> <p>Additionally, these four metrics don't take model confidence score into account. All inferences above the confidence score threshold are treated the same! For example, when using a confidence score threshold of 0.5, an inference with a confidence score barely above the threshold (e.g. 0.55) is treated the same as an inference with a very high confidence score (e.g. 0.99). In other words, any inference above the confidence threshold is considered as a positive inference. To examine performance taking confidence score into account, consider plotting a histogram of the distribution of confidence scores.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section contains detailed API reference documentation for <code>kolena</code>.</p> <ul> <li> <p><code>kolena.initialize</code></p> <p>Initialize client sessions</p> </li> <li> <p><code>kolena.errors</code></p> <p>Custom error definitions</p> </li> </ul> <ul> <li> <p> <code>kolena.workflow</code></p> <p>Building blocks to test any ML problem in Kolena</p> </li> </ul> <ul> <li> <p>  Pre-built Workflows</p> <p>Ready-to-use workflows built with <code>kolena.workflow</code>.</p> </li> </ul> <ul> <li> <p>  Legacy Definitions</p> <p>Built-in <code>kolena.classification</code>, <code>kolena.detection</code>, and <code>kolena.fr</code> workflows</p> </li> </ul>"},{"location":"reference/errors/","title":"<code>kolena.errors</code>","text":"<p>Reference for various exceptions raised from <code>kolena</code>. All custom exceptions extend the base <code>KolenaError</code>.</p>"},{"location":"reference/errors/#kolena.errors.KolenaError","title":"<code>KolenaError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Base error for all Kolena errors to extend. Allows consumers to catch Kolena specific errors.</p>"},{"location":"reference/errors/#kolena.errors.InputValidationError","title":"<code>InputValidationError</code>","text":"<p>             Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided input data failed validation.</p>"},{"location":"reference/errors/#kolena.errors.IncorrectUsageError","title":"<code>IncorrectUsageError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user performed a disallowed action with the client.</p>"},{"location":"reference/errors/#kolena.errors.InvalidTokenError","title":"<code>InvalidTokenError</code>","text":"<p>             Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided token value was invalid.</p>"},{"location":"reference/errors/#kolena.errors.InvalidClientStateError","title":"<code>InvalidClientStateError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that client state was invalid.</p>"},{"location":"reference/errors/#kolena.errors.MissingTokenError","title":"<code>MissingTokenError</code>","text":"<p>             Bases: <code>KeyError</code>, <code>KolenaError</code></p> <p>Exception indicating that the client could not locate an API token.</p>"},{"location":"reference/errors/#kolena.errors.UninitializedError","title":"<code>UninitializedError</code>","text":"<p>             Bases: <code>InvalidClientStateError</code></p> <p>Exception indicating that the client has not been properly initialized before usage.</p>"},{"location":"reference/errors/#kolena.errors.DirectInstantiationError","title":"<code>DirectInstantiationError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the default constructor was used for a class that does not support direct instantiation. Available static constructors should be used when this exception is encountered.</p>"},{"location":"reference/errors/#kolena.errors.FrozenObjectError","title":"<code>FrozenObjectError</code>","text":"<p>             Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user attempted to modify a frozen object.</p>"},{"location":"reference/errors/#kolena.errors.UnauthenticatedError","title":"<code>UnauthenticatedError</code>","text":"<p>             Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating unauthenticated usage of the client.</p>"},{"location":"reference/errors/#kolena.errors.RemoteError","title":"<code>RemoteError</code>","text":"<p>             Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating that a remote error occurred in communications between the Kolena client and server.</p>"},{"location":"reference/errors/#kolena.errors.CustomMetricsException","title":"<code>CustomMetricsException</code>","text":"<p>             Bases: <code>KolenaError</code></p> <p>Exception indicating that there's an error when computing custom metrics.</p>"},{"location":"reference/errors/#kolena.errors.WorkflowMismatchError","title":"<code>WorkflowMismatchError</code>","text":"<p>             Bases: <code>KolenaError</code></p> <p>Exception indicating a workflow mismatch.</p>"},{"location":"reference/errors/#kolena.errors.NotFoundError","title":"<code>NotFoundError</code>","text":"<p>             Bases: <code>RemoteError</code></p> <p>Exception indicating an entity is not found</p>"},{"location":"reference/errors/#kolena.errors.NameConflictError","title":"<code>NameConflictError</code>","text":"<p>             Bases: <code>RemoteError</code></p> <p>Exception indicating the name of an entity is conflict</p>"},{"location":"reference/initialize/","title":"<code>kolena.initialize</code>","text":""},{"location":"reference/initialize/#kolena.initialize.initialize","title":"<code>initialize(*args, api_token=None, verbose=False, proxies=None, **kwargs)</code>","text":"<p>Initialize a client session.</p> <p>A session has a global scope and remains active until interpreter shutdown.</p> <p>Retrieve an API token from the   Developer page and make it available through one of the following options before initializing:</p> <ol> <li>Directly through the <code>api_token</code> keyword argument <pre><code>import kolena\n\nkolena.initialize(api_token=your_token, verbose=True)\n</code></pre></li> <li>Populate the <code>KOLENA_TOKEN</code> environment variable <pre><code>export KOLENA_TOKEN=\"********\"\n</code></pre></li> <li>Store in <code>.netrc</code> file ~/.netrc<pre><code>machine api.kolena.io password ********\n</code></pre></li> </ol> <pre><code>flowchart TD\n    Start[Get API token]\n    Step1{{api_token argument provided?}}\n    Step2{{KOLENA_TOKEN environment variable set?}}\n    Step3{{Token in .netrc file?}}\n    End[Use as API token]\n    Exception[MissingTokenError]\n    Start --&gt; Step1\n    Step1 --&gt;|No| Step2\n    Step2 --&gt;|No| Step3\n    Step3 --&gt;|No| Exception\n    Step1 --&gt;|Yes| End\n    Step2 --&gt;|Yes| End\n    Step3 --&gt;|Yes| End</code></pre> <p>Note</p> <p>As of version 0.29.0: the <code>entity</code> argument is no longer needed; the signature <code>initialize(entity, api_token)</code> has been deprecated and replaced by <code>initialize(api_token)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>api_token</code> <code>Optional[str]</code> <p>Optionally provide an API token, otherwise attempts to find a token in <code>$KOLENA_TOKEN</code> or <code>.netrc</code> file. This token is a secret and should be treated with caution.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Optionally configure client to run in verbose mode, providing more information about execution. All logging events are emitted as Python standard library <code>logging</code> events from the <code>\"kolena\"</code> logger as well as to stdout/stderr directly.</p> <code>False</code> <code>proxies</code> <code>Optional[Dict[str, str]]</code> <p>Optionally configure client to run with <code>http</code> or <code>https</code> proxies. The <code>proxies</code> parameter is passed through to the <code>requests</code> package and can be configured accordingly.</p> <code>None</code> <p>Raises:</p> Type Description <code>InvalidTokenError</code> <p>The provided <code>api_token</code> is not valid.</p> <code>InputValidationError</code> <p>The provided combination or number of args is not valid.</p> <code>MissingTokenError</code> <p>An API token could not be found.</p>"},{"location":"reference/pre-built/","title":"Pre-built Workflows","text":"<p>Ready-to-use workflows built with <code>kolena.workflow</code>.</p> <ul> <li> <p>  Object Detection (2D)</p> <p></p> <p>Object Detection on 2D images using axis-aligned bounding boxes.</p> </li> </ul>"},{"location":"reference/pre-built/classification/","title":"Classification","text":"<p>Experimental Feature</p> <p>This pre-built workflow is an experimental feature. Experimental features are under active development and may occasionally undergo API-breaking changes.</p> <p>Classification is a machine learning task aiming to group objects and ideas into preset categories. Classification models used in machine learning predict the likelihood or probability that the data will fall into one of the predetermined categories.</p> <p>There are different types of classification models:</p> Classification Type Description Binary Classification model predicts a single class, using a threshold on prediction confidence to bisect the test set Multiclass Classification model predicts a single class from more than two classes, with highest prediction confidence Multi-label Classification model predicts multiple classes, with each prediction over a threshold considered positive (i.e. ensemble of binary classifiers) <p>This pre-built workflow is work in progress; however, you can refer to the workflow implementation for binary and multiclass types from the examples below:</p> <ul> <li> <p>  Example: Binary Classification</p> <p></p> <p>Binary Classification of class \"Dog\" using the Dogs vs. Cats dataset</p> </li> <li> <p>  Example: Multiclass Classification</p> <p></p> <p>Multiclass Classification using the CIFAR-10 dataset</p> </li> </ul>"},{"location":"reference/pre-built/classification/#utility-methods","title":"Utility Methods","text":""},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.compute_confusion_matrix","title":"<code>compute_confusion_matrix(ground_truths, inferences, title='Confusion Matrix', labels=None)</code>","text":"<p>Computes confusion matrix given a list of ground truth and inference labels.</p> <p>For a binary classification case, a 2x2 confusion matrix with the count of TP, FP, FN, and TP is computed.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[str]</code> <p>The ground truth labels.</p> required <code>inferences</code> <code>List[str]</code> <p>The inference labels.</p> required <code>title</code> <code>Optional[str]</code> <p>The title of confusion matrix.</p> <code>'Confusion Matrix'</code> <code>labels</code> <code>Optional[List[str]]</code> <p>The list of labels to index the matrix. This may be used to reorder or select a subset of labels. By default, labels that appear at least once in <code>ground_truths</code> or <code>inferences</code> are used in sorted order.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ConfusionMatrix]</code> <p>The <code>ConfusionMatrix</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.compute_roc_curves","title":"<code>compute_roc_curves(ground_truths, inferences, labels=None, title=None)</code>","text":"<p>Computes OvR (one-vs-rest) ROC (receiver operating characteristic) curves for each class appears in <code>ground_truths</code> if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Optional[Label]]</code> <p>The list of ground truth <code>Label</code>. For binary classification, the negative class can be <code>None</code>.</p> required <code>inferences</code> <code>List[List[ScoredLabel]]</code> <p>The list of inference <code>ScoredLabel</code>. For <code>N</code>-class problems, each inference is expected to contain <code>N</code> entries, one for each class and its associated confidence score.</p> required <code>labels</code> <code>Optional[List[str]]</code> <p>The labels to plot. If not specified, classes appear in <code>ground_truths</code> are used. Use <code>labels</code> to specify the evaluating classes especially if <code>ground_truths</code> only have negative classes.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[CurvePlot]</code> <p>A <code>CurvePlot</code> if there is any valid <code>Curve</code> computed; otherwise, <code>None</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.compute_threshold_curves","title":"<code>compute_threshold_curves(ground_truths, inferences, thresholds=None)</code>","text":"<p>Computes scores (i.e. Precision, Recall and F1-score) vs. threshold curves for a single class presented in <code>inferences</code>.</p> <p>Expects <code>ground_truths</code> and <code>inferences</code> correspond to the same sample for the same given index.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Optional[Label]]</code> <p>The list of ground truth <code>Label</code>s. For binary classification, the negative class can be <code>None</code>.</p> required <code>inferences</code> <code>List[ScoredLabel]</code> <p>The list of inference <code>ScoredLabel</code>s. The length of <code>inferences</code> must match the length of <code>ground_truths</code>. The list should only include inferences of a specific class to plot the threshold curves for.</p> required <code>thresholds</code> <code>Optional[List[float]]</code> <p>The list of thresholds to plot with. If not specified, all the unique confidence scores are used as thresholds, including evenly spaced thresholds from 0 to 1 with 0.1 step.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[List[Curve]]</code> <p>A list of <code>Curve</code>s if there is any valid <code>Curve</code> computed; otherwise, <code>None</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.create_histogram","title":"<code>create_histogram(values, range, title='', x_label='', y_label='')</code>","text":"<p>Creates a <code>Histogram</code> for the specified range and the number of bins.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[float]</code> <p>The list of confidence scores to plot.</p> required <code>range</code> <code>Tuple[float, float, int]</code> <p>The min, max and # of bins of the histogram.</p> required <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>''</code> <code>x_label</code> <code>str</code> <p>The label on the x-axis.</p> <code>''</code> <code>y_label</code> <code>str</code> <p>The label on the y-axis.</p> <code>''</code> <p>Returns:</p> Type Description <code>Histogram</code> <p>The <code>Histogram</code>.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.get_histogram_range","title":"<code>get_histogram_range(values)</code>","text":"<p>Computes an ideal range for a confidence score histograms given a list of confidence scores.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[float]</code> <p>The list of confidence scores, [0, 1].</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[float, float, int]]</code> <p>A tuple of min, max and # of bins for a confidence score histograms. The range is rounded up/down to the nearest 0.02. The bin size is 0.02.</p>"},{"location":"reference/pre-built/classification/#kolena._experimental.classification.utils.get_label_confidence","title":"<code>get_label_confidence(label, inference_labels)</code>","text":"<p>Returns the confidence score of the specified <code>label</code> from a list of confidence scores for each label.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The label whose confidence score to return.</p> required <code>inference_labels</code> <code>List[ScoredLabel]</code> <p>The list of confidence scores for each label. For <code>N</code>-class problem, expected to have <code>N</code> entries, one for each class.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The confidence score of the specified <code>label</code>. If the <code>label</code> doesn't exist in <code>inference_labels</code> then returns 0.</p>"},{"location":"reference/pre-built/object-detection-2d/","title":"Object Detection (2D)","text":"<p>Experimental Feature</p> <p>This pre-built workflow is an experimental feature. Experimental features are under active development and may occasionally undergo API-breaking changes.</p> <p>Object Detection (OD) is a computer vision task that aims to classify and locate objects of interest presented in an image. So, it can be viewed as a combination of localization and classification tasks.</p> <p>This pre-built workflow is prepared for a 2D Object Detection problem and here is an example of using this workflow on the COCO dataset.</p> <ul> <li> <p>  Example: Object Detection (2D) \u2197</p> <p></p> <p>2D Object Detection using the COCO dataset</p> </li> </ul>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.TestSample","title":"<code>TestSample</code>","text":"<p>             Bases: <code>Image</code></p> <p>The <code>Image</code> sample type for the pre-built 2D Object Detection workflow.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.TestSample.metadata","title":"<code>metadata: Metadata = dataclasses.field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The optional <code>Metadata</code> dictionary.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.GroundTruth","title":"<code>GroundTruth</code>","text":"<p>             Bases: <code>GroundTruth</code></p> <p>Ground truth type for the pre-built 2D Object Detection workflow.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.GroundTruth.bboxes","title":"<code>bboxes: List[LabeledBoundingBox]</code>  <code>instance-attribute</code>","text":"<p>The ground truth <code>LabeledBoundingBox</code>es associated with an image.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.GroundTruth.ignored_bboxes","title":"<code>ignored_bboxes: List[LabeledBoundingBox] = dataclasses.field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The ground truth <code>LabeledBoundingBox</code>es to be ignored in evaluation associated with an image.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.Inference","title":"<code>Inference</code>","text":"<p>             Bases: <code>Inference</code></p> <p>Inference type for the pre-built 2D Object Detection workflow.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.Inference.bboxes","title":"<code>bboxes: List[ScoredLabeledBoundingBox]</code>  <code>instance-attribute</code>","text":"<p>The inference <code>ScoredLabeledBoundingBox</code>es associated with an image.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.Inference.ignored","title":"<code>ignored: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the image (and its associated inference <code>bboxes</code>) should be ignored in evaluating the results of the model.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ThresholdConfiguration","title":"<code>ThresholdConfiguration</code>","text":"<p>             Bases: <code>EvaluatorConfiguration</code></p> <p>Confidence and IoU \u2197 threshold configuration for the pre-built 2D Object Detection workflow. Specify a confidence and IoU threshold to apply to all classes.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ThresholdConfiguration.threshold_strategy","title":"<code>threshold_strategy: Union[Literal['F1-Optimal'], float] = 'F1-Optimal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The confidence threshold strategy. It can either be a fixed confidence threshold such as <code>0.3</code> or <code>0.75</code>, or the F1-optimal threshold by default.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ThresholdConfiguration.iou_threshold","title":"<code>iou_threshold: float = 0.5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The IoU \u2197 threshold, defaulting to <code>0.5</code>.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ThresholdConfiguration.min_confidence_score","title":"<code>min_confidence_score: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The minimum confidence score to consider for the evaluation. This is usually set to reduce noise by excluding inferences with low confidence score.</p>"},{"location":"reference/pre-built/object-detection-2d/#kolena._experimental.object_detection.ObjectDetectionEvaluator","title":"<code>ObjectDetectionEvaluator(configurations=None)</code>","text":"<p>             Bases: <code>Evaluator</code></p> <p>This <code>ObjectDetectionEvaluator</code> transforms inferences into metrics for the object detection workflow for a single class or multiple classes.</p> <p>When a <code>ThresholdConfiguration</code> is configured to use an F1-Optimal threshold strategy, the evaluator requires that the first test case retrieved for a test suite contains the complete sample set.</p> <p>For additional functionality, see the associated base class documentation.</p>"},{"location":"reference/workflow/","title":"<code>kolena.workflow</code>","text":"<ul> <li>  Developer Guide: Building a Workflow \u2197</li> <li>  Examples: <code>kolena/examples</code> \u2197</li> </ul> <p><code>kolena.workflow</code> contains the definitions to build a workflow:</p> <ol> <li> <p>Design data types, including any <code>annotations</code> or <code>assets</code>:</p> <p>Defining a workflow</p> <p><code>TestSample</code>, <code>GroundTruth</code>, and <code>Inference</code> can be thought of as the data model, or schema, for a workflow.</p> <p>  Learn more \u2197</p> <ul> <li><code>TestSample</code>: model inputs, e.g. images, videos, documents</li> <li><code>GroundTruth</code>: expected model outputs</li> <li><code>Inference</code>: real model outputs</li> </ul> </li> <li> <p>Define metrics and how they are computed:</p> <ul> <li><code>Evaluator</code>: metrics computation engine</li> </ul> </li> <li> <p>Create tests:</p> <p>Managing tests</p> <p>See the test case and test suite developer guide for an introduction to the test case and test suite concept.</p> <ul> <li><code>TestCase</code>: a test dataset, or a slice thereof</li> <li><code>TestSuite</code>: a collection of test cases</li> </ul> </li> <li> <p>Test models:</p> <ul> <li><code>Model</code>: descriptor for a model</li> <li><code>test</code>: interface to run tests</li> </ul> </li> </ol>"},{"location":"reference/workflow/annotation/","title":"Annotations: <code>kolena.workflow.annotation</code>","text":"<p>Annotations are visualized in Kolena as overlays on top of <code>TestSample</code> objects.</p> Annotation Valid <code>TestSample</code> Types <code>BoundingBox</code> <code>Image</code>, <code>Video</code> <code>BoundingBox3D</code> <code>PointCloud</code> <code>Polygon</code> <code>Image</code>, <code>Video</code> <code>Polyline</code> <code>Image</code>, <code>Video</code> <code>Keypoints</code> <code>Image</code>, <code>Video</code> <code>SegmentationMask</code> <code>Image</code>, <code>Video</code> <code>BitmapMask</code> <code>Image</code>, <code>Video</code> <code>Label</code> <code>Text</code>, <code>Document</code>, <code>Image</code>, <code>PointCloud</code>, <code>Audio</code>, <code>Video</code> <code>TimeSegment</code> <code>Audio</code>, <code>Video</code> <p>For example, when viewing images in the Studio, any annotations (such as lists of <code>BoundingBox</code> objects) present in the <code>TestSample</code>, <code>GroundTruth</code>, <code>Inference</code>, or <code>MetricsTestSample</code> objects are rendered on top of the image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ClassificationLabel","title":"<code>ClassificationLabel = Label</code>  <code>module-attribute</code>","text":"<p>Alias for <code>Label</code>.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredClassificationLabel","title":"<code>ScoredClassificationLabel = ScoredLabel</code>  <code>module-attribute</code>","text":"<p>Alias for <code>ScoredLabel</code>.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Annotation","title":"<code>Annotation</code>","text":"<p>             Bases: <code>TypedDataObject[_AnnotationType]</code></p> <p>The base class for all annotation types.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox","title":"<code>BoundingBox</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices.</p> <p>The reserved fields <code>width</code>, <code>height</code>, <code>area</code>, and <code>aspect_ratio</code> are automatically populated with values derived from the provided coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The top left vertex (in <code>(x, y)</code> pixel coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The bottom right vertex (in <code>(x, y)</code> pixel coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox","title":"<code>LabeledBoundingBox</code>","text":"<p>             Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox","title":"<code>ScoredBoundingBox</code>","text":"<p>             Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox","title":"<code>ScoredLabeledBoundingBox</code>","text":"<p>             Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices, a string label, and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon","title":"<code>Polygon</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of <code>(x, y)</code> pixel coordinates comprising the boundary of this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon","title":"<code>LabeledPolygon</code>","text":"<p>             Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon","title":"<code>ScoredPolygon</code>","text":"<p>             Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon","title":"<code>ScoredLabeledPolygon</code>","text":"<p>             Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates with a string label and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints","title":"<code>Keypoints</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Array of any number of keypoints specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of discrete <code>(x, y)</code> pixel coordinates comprising this keypoints annotation.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline","title":"<code>Polyline</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Polyline with any number of vertices specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of connected <code>(x, y)</code> pixel coordinates comprising this polyline.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D","title":"<code>BoundingBox3D</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Three-dimensional cuboid bounding box in a right-handed coordinate system.</p> <p>Specified by <code>(x, y, z)</code> coordinates for the <code>center</code> of the cuboid, <code>(x, y, z)</code> <code>dimensions</code>, and a <code>rotation</code> parameter specifying the degrees of rotation about each axis <code>(x, y, z)</code> ranging <code>[-\u03c0, \u03c0]</code>.</p> <p>The reserved field <code>volume</code> is automatically derived from the provided <code>dimensions</code>.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.center","title":"<code>center: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> coordinates specifying the center of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.dimensions","title":"<code>dimensions: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> measurements specifying the dimensions of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.rotations","title":"<code>rotations: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p>Rotations in degrees about each <code>(x, y, z)</code> axis.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D","title":"<code>LabeledBoundingBox3D</code>","text":"<p>             Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D","title":"<code>ScoredBoundingBox3D</code>","text":"<p>             Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D","title":"<code>ScoredLabeledBoundingBox3D</code>","text":"<p>             Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label and float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask","title":"<code>SegmentationMask</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Raster segmentation mask. The <code>locator</code> is the URL to the image file representing the segmentation mask.</p> <p>The segmentation mask must be rendered as a single-channel, 8-bit-depth (grayscale) image. For the best results, use a lossless file format such as PNG. Each pixel's value is the numerical ID of its class label, as specified in the <code>labels</code> map. Any pixel value not present in the <code>labels</code> map is rendered as part of the background.</p> <p>For example, <code>labels = {255: \"object\"}</code> will highlight all pixels with the value of 255 as <code>\"object\"</code>. Every other pixel value will be transparent.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.labels","title":"<code>labels: Dict[int, str]</code>  <code>instance-attribute</code>","text":"<p>Mapping of unique label IDs (pixel values) to unique label values.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the segmentation mask image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask","title":"<code>BitmapMask</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Arbitrary bitmap mask. The <code>locator</code> is the URL to the image file representing the mask.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the bitmap data.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Label","title":"<code>Label</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Label, e.g. for classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Label.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>String label for this classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabel","title":"<code>ScoredLabel</code>","text":"<p>             Bases: <code>Label</code></p> <p>Label with accompanying score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabel.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>Score associated with this label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.TimeSegment","title":"<code>TimeSegment</code>","text":"<p>             Bases: <code>Annotation</code></p> <p>Segment of time in the associated audio or video file.</p> <p>When a <code>group</code> is specified, segments are displayed on Kolena with different colors for each group present in a <code>List[TimeSegment]</code>. Example usage:</p> <pre><code>transcription: List[TimeSegment] = [\n    LabeledTimeSegment(group=\"A\", label=\"Knock, knock.\", start=0, end=1),\n    LabeledTimeSegment(group=\"B\", label=\"Who's there?\", start=2, end=3),\n    LabeledTimeSegment(group=\"A\", label=\"Example.\", start=3.5, end=4),\n    LabeledTimeSegment(group=\"B\", label=\"Example who?\", start=4.5, end=5.5),\n    LabeledTimeSegment(group=\"A\", label=\"Example illustrating two-person dialogue using `group`.\", start=6, end=9),\n]\n</code></pre>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.TimeSegment.start","title":"<code>start: float</code>  <code>instance-attribute</code>","text":"<p>Start time, in seconds, of this segment.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.TimeSegment.end","title":"<code>end: float</code>  <code>instance-attribute</code>","text":"<p>End time, in seconds, of this segment.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledTimeSegment","title":"<code>LabeledTimeSegment</code>","text":"<p>             Bases: <code>TimeSegment</code></p> <p>Time segment with accompanying label, e.g. audio transcription.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledTimeSegment.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this time segment.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredTimeSegment","title":"<code>ScoredTimeSegment</code>","text":"<p>             Bases: <code>TimeSegment</code></p> <p>Time segment with additional float score, representing e.g. model prediction confidence.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredTimeSegment.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this time segment.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledTimeSegment","title":"<code>ScoredLabeledTimeSegment</code>","text":"<p>             Bases: <code>TimeSegment</code></p> <p>Time segment with accompanying label and score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledTimeSegment.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this time segment.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledTimeSegment.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this time segment.</p>"},{"location":"reference/workflow/asset/","title":"Assets: <code>kolena.workflow.asset</code>","text":"<p>Assets are additional files linked to the <code>TestSample</code>, <code>GroundTruth</code>, or <code>Inference</code> objects for your workflow. Assets can be visualized in the Kolena Studio when exploring your test cases or model results.</p> <p>The following asset types are available:</p> <ul> <li><code>ImageAsset</code></li> <li><code>PlainTextAsset</code></li> <li><code>BinaryAsset</code></li> <li><code>PointCloudAsset</code></li> <li><code>VideoAsset</code></li> </ul>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.Asset","title":"<code>Asset</code>","text":"<p>             Bases: <code>TypedDataObject[_AssetType]</code></p> <p>Base class for all asset types.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset","title":"<code>ImageAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>An image in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this image in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-image-asset.png</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset","title":"<code>PlainTextAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A plain text file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this text file in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-text-asset.txt</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BinaryAsset","title":"<code>BinaryAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A binary file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BinaryAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this text file in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-binary-asset.bin</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset","title":"<code>PointCloudAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A three-dimensional point cloud located in a cloud bucket. Points are assumed to be specified in a right-handed, Z-up coordinate system with the origin around the sensor that captured the point cloud.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this point cloud in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-point-cloud.pcd</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset","title":"<code>BaseVideoAsset</code>","text":"<p>             Bases: <code>Asset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset","title":"<code>VideoAsset</code>","text":"<p>             Bases: <code>BaseVideoAsset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail image.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.start","title":"<code>start: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.end","title":"<code>end: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/define-workflow/","title":"<code>kolena.workflow.define_workflow</code>","text":""},{"location":"reference/workflow/define-workflow/#kolena.workflow.define_workflow.define_workflow","title":"<code>define_workflow(name, test_sample_type, ground_truth_type, inference_type)</code>","text":"<p>Define a new workflow, specifying its test sample, ground truth, and inference types.</p> <pre><code>from kolena.workflow import define_workflow\n\nfrom my_code import MyTestSample, MyGroundTruth, MyInference\n\n_, TestCase, TestSuite, Model = define_workflow(\n    \"My Workflow\",\n    MyTestSample,   # extends e.g. kolena.workflow.Image (or uses directly)\n    MyGroundTruth,  # extends kolena.workflow.GroundTruth\n    MyInference,    # extends kolena.workflow.Inference\n)\n</code></pre> <p><code>define_workflow</code> is provided as a convenience method to create the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects for a new workflow. These objects can also be defined manually by subclassing them and binding the <code>workflow</code> class variable:</p> <pre><code>from kolena.workflow import TestCase\n\nfrom my_code import my_workflow\n\nclass MyTestCase(TestCase):\n    workflow = my_workflow\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the workflow.</p> required <code>test_sample_type</code> <code>Type[TestSample]</code> <p>The type of the <code>TestSample</code> for this workflow.</p> required <code>ground_truth_type</code> <code>Type[GroundTruth]</code> <p>The type of the <code>GroundTruth</code> for this workflow.</p> required <code>inference_type</code> <code>Type[Inference]</code> <p>The type of the <code>Inference</code> for this workflow.</p> required <p>Returns:</p> Type Description <code>Tuple[Workflow, Type[TestCase], Type[TestSuite], Type[Model]]</code> <p>The <code>Workflow</code> object for this workflow along with the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects to use when creating and running tests for this workflow.</p>"},{"location":"reference/workflow/evaluator/","title":"<code>kolena.workflow.Evaluator</code>","text":"<p>Simplified interface for <code>Evaluator</code> implementations.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSample","title":"<code>MetricsTestSample</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Test-sample-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Examples here may include the number of true positive detections on an image, the mean IOU of inferred polygon(s) with ground truth polygon(s), etc.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestCase","title":"<code>MetricsTestCase</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Test-case-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-case-level metrics are aggregate metrics like <code>precision</code>, <code>recall</code>, and <code>f1_score</code>. Any and all aggregate metrics that fit a workflow should be defined here.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestCase--nesting-aggregate-metrics","title":"Nesting Aggregate Metrics","text":"<p><code>MetricsTestCase</code> supports nesting metrics objects, for e.g. reporting class-level metrics within a test case that contains multiple classes. Example usage:</p> <pre><code>@dataclass(frozen=True)\nclass PerClassMetrics(MetricsTestCase):\n    Class: str\n    Precision: float\n    Recall: float\n    F1: float\n    AP: float\n\n@dataclass(frozen=True)\nclass TestCaseMetrics(MetricsTestCase):\n    macro_Precision: float\n    macro_Recall: float\n    macro_F1: float\n    mAP: float\n    PerClass: List[PerClassMetrics]\n</code></pre> <p>Any <code>str</code>-type fields (e.g. <code>Class</code> in the above example) will be used as identifiers when displaying nested metrics on Kolena. For best results, include at least one <code>str</code>-type field in nested metrics definitions.</p> <p>When comparing nested metrics from multiple models, an <code>int</code>-type column with any of the following names will be used for sample size in statistical significance calculations: <code>N</code>, <code>n</code>, <code>nTestSamples</code>, <code>n_test_samples</code>, <code>sampleSize</code>, <code>sample_size</code>, <code>SampleSize</code>.</p> <p>For a detailed overview of this feature, see the   Nesting Test Case Metrics advanced usage guide.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSuite","title":"<code>MetricsTestSuite</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Test-suite-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-suite-level metrics typically measure performance across test cases, e.g. penalizing variance across different subsets of a benchmark.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.EvaluatorConfiguration","title":"<code>EvaluatorConfiguration</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Configuration for an <code>Evaluator</code>.</p> <p>Example evaluator configurations may specify:</p> <ul> <li>Fixed confidence thresholds at which detections are discarded.</li> <li>Different algorithms/strategies used to compute confidence thresholds     (e.g. \"accuracy optimal\" for a classification-type workflow).</li> </ul>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.EvaluatorConfiguration.display_name","title":"<code>display_name()</code>  <code>abstractmethod</code>","text":"<p>The name to display for this configuration in Kolena. Must be implemented when extending <code>EvaluatorConfiguration</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator","title":"<code>Evaluator(configurations=None)</code>","text":"<p>An <code>Evaluator</code> transforms inferences into metrics.</p> <p>Metrics are computed at the individual test sample level (<code>MetricsTestSample</code>), in aggregate at the test case level (<code>MetricsTestCase</code>), and across populations at the test suite level (<code>MetricsTestSuite</code>).</p> <p>Test-case-level plots (<code>Plot</code>) may also be computed.</p> <p>Parameters:</p> Name Type Description Default <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>The configurations at which to perform evaluation. Instance methods such as <code>compute_test_sample_metrics</code> are called once per test case per configuration.</p> <code>None</code>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.configurations","title":"<code>configurations: List[EvaluatorConfiguration] = configurations or []</code>  <code>instance-attribute</code>","text":"<p>The configurations with which to perform evaluation, provided on instantiation.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.display_name","title":"<code>display_name()</code>","text":"<p>The name to display for this evaluator in Kolena. Defaults to the name of this class.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_sample_metrics","title":"<code>compute_test_sample_metrics(test_case, inferences, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute metrics for every test sample in a test case, i.e. one <code>MetricsTestSample</code> object for each of the provided test samples.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The <code>TestCase</code> to which the provided test samples and ground truths belong.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[TestSample, MetricsTestSample]]</code> <p><code>TestSample</code>-level metrics for each provided test sample.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_metrics","title":"<code>compute_test_case_metrics(test_case, inferences, metrics, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute aggregate metrics (<code>MetricsTestCase</code>) across a test case.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case in question.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>The <code>TestSample</code>-level metrics computed by <code>compute_test_sample_metrics</code>, provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>MetricsTestCase</code> <p><code>TestCase</code>-level metrics for the provided test case.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_plots","title":"<code>compute_test_case_plots(test_case, inferences, metrics, configuration=None)</code>","text":"<p>Optionally compute any number of plots to visualize the results for a test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case in question</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>The <code>TestSample</code>-level metrics computed by <code>compute_test_sample_metrics</code>, provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>the evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[List[Plot]]</code> <p>Zero or more plots for this test case at this configuration.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_suite_metrics","title":"<code>compute_test_suite_metrics(test_suite, metrics, configuration=None)</code>","text":"<p>Optionally compute <code>TestSuite</code>-level metrics (<code>MetricsTestSuite</code>) across the provided <code>test_suite</code>.</p> <p>Parameters:</p> Name Type Description Default <code>test_suite</code> <code>TestSuite</code> <p>The test suite in question.</p> required <code>metrics</code> <code>List[Tuple[TestCase, MetricsTestCase]]</code> <p>The <code>TestCase</code>-level metrics computed by <code>compute_test_case_metrics</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>The evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[MetricsTestSuite]</code> <p>The <code>TestSuite</code>-level metrics for this test suite.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.BasicEvaluatorFunction","title":"<code>BasicEvaluatorFunction = Union[ConfiguredEvaluatorFunction, UnconfiguredEvaluatorFunction]</code>  <code>module-attribute</code>","text":"<p><code>BasicEvaluatorFunction</code> provides a function-based evaluator interface that takes the inferences for all test samples in a test suite and a <code>TestCases</code> as input and computes the corresponding test-sample-level, test-case-level, and test-suite-level metrics (and optionally plots) as output.</p> <p>Example implementation, relying on <code>compute_per_sample</code> and <code>compute_aggregate</code> functions implemented elsewhere:</p> <pre><code>def evaluate(\n    test_samples: List[TestSample],\n    ground_truths: List[GroundTruth],\n    inferences: List[Inference],\n    test_cases: TestCases,\n    # configuration: EvaluatorConfiguration,  # uncomment when configuration is used\n) -&gt; EvaluationResults:\n    # compute per-sample metrics for each test sample\n    per_sample_metrics = [compute_per_sample(gt, inf) for gt, inf in zip(ground_truths, inferences)]\n\n    # compute aggregate metrics across all test cases using `test_cases.iter(...)`\n    aggregate_metrics: List[Tuple[TestCase, MetricsTestCase]] = []\n    for test_case, *s in test_cases.iter(test_samples, ground_truths, inferences, per_sample_metrics):\n        # subset of `test_samples`/`ground_truths`/`inferences`/`test_sample_metrics` in given test case\n        tc_test_samples, tc_ground_truths, tc_inferences, tc_per_sample_metrics = s\n        aggregate_metrics.append((test_case, compute_aggregate(tc_per_sample_metrics)))\n\n    # if desired, compute and add `plots_test_case` and `metrics_test_suite`\n    return EvaluationResults(\n        metrics_test_sample=list(zip(test_samples, per_sample_metrics)),\n        metrics_test_case=aggregate_metrics,\n    )\n</code></pre> <p>The control flow is in general more streamlined than with <code>Evaluator</code>, but requires a couple of assumptions to hold:</p> <ul> <li>Test-sample-level metrics do not vary by test case</li> <li>Ground truths corresponding to a given test sample do not vary by test case</li> </ul> <p>This <code>BasicEvaluatorFunction</code> is provided to the test run at runtime, and is expected to have the following signature:</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>A list of distinct <code>TestSample</code> values that correspond to all test samples in the test run.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>A list of <code>GroundTruth</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>inferences</code> <code>List[Inference]</code> <p>A list of <code>Inference</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>test_cases</code> <code>TestCases</code> <p>An instance of <code>TestCases</code>, used to provide iteration groupings for evaluating test-case-level metrics.</p> required <code>evaluator_configuration</code> <code>EvaluatorConfiguration</code> <p>The <code>EvaluatorConfiguration</code> to use when performing the evaluation. This parameter may be omitted in the function definition for implementations that do not use any configuration object.</p> required <p>Returns:</p> Type Description <code>EvaluationResults</code> <p>An <code>EvaluationResults</code> object tracking the test-sample-level, test-case-level and test-suite-level metrics and plots for the input collection of test samples.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases","title":"<code>TestCases</code>","text":"<p>Provides an iterator method for grouping test-sample-level metric results with the test cases that they belong to.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases.iter","title":"<code>iter(test_samples, ground_truths, inferences, metrics_test_sample)</code>  <code>abstractmethod</code>","text":"<p>Matches test sample metrics to the corresponding test cases that they belong to.</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>All unique test samples within the test run, sequenced in the same order as the other parameters.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>Ground truths corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>inferences</code> <code>List[Inference]</code> <p>Inferences corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>metrics_test_sample</code> <code>List[MetricsTestSample]</code> <p>Test-sample-level metrics corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestCase, List[TestSample], List[GroundTruth], List[Inference], List[MetricsTestSample]]]</code> <p>Iterator that groups each test case in the test run to the lists of member test samples, inferences, and test-sample-level metrics.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults","title":"<code>EvaluationResults</code>","text":"<p>A bundle of metrics computed for a test run grouped at the test-sample-level, test-case-level, and test-suite-level. Optionally includes <code>Plot</code>s at the test-case-level.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_sample","title":"<code>metrics_test_sample: List[Tuple[BaseTestSample, BaseMetricsTestSample]]</code>  <code>instance-attribute</code>","text":"<p>Sample-level metrics, extending <code>MetricsTestSample</code>, for every provided test sample.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_case","title":"<code>metrics_test_case: List[Tuple[TestCase, MetricsTestCase]]</code>  <code>instance-attribute</code>","text":"<p>Aggregate metrics, extending <code>MetricsTestCase</code>, computed across each test case yielded from <code>TestCases.iter</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.plots_test_case","title":"<code>plots_test_case: List[Tuple[TestCase, List[Plot]]] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional test-case-level plots.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_suite","title":"<code>metrics_test_suite: Optional[MetricsTestSuite] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional test-suite-level metrics, extending <code>MetricsTestSuite</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.no_op_evaluator","title":"<code>no_op_evaluator(test_samples, ground_truths, inferences, test_cases)</code>","text":"<p>A no-op implementation of the Kolena <code>Evaluator</code> that will bypass evaluation but make <code>Inference</code>s accessible in the platform.</p> <pre><code>from kolena.workflow import no_op_evaluator\nfrom kolena.workflow import test\n\ntest(model, test_suite, no_op_evaluator)\n</code></pre>"},{"location":"reference/workflow/ground-truth/","title":"<code>kolena.workflow.GroundTruth</code>","text":"<p>The ground truth associated with a <code>TestSample</code>. Typically, a ground truth will represent the expected output of a model when given a test sample and will be manually annotated by a human.</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\n\nfrom kolena.workflow import GroundTruth\nfrom kolena.workflow.annotation import Polyline, SegmentationMask\n\n@dataclass(frozen=True)\nclass AvGroundTruth(GroundTruth):\n    road_area: SegmentationMask\n    lane_boundaries: List[Polyline]\n    visibility_score: int\n</code></pre> <p>A <code>TestCase</code> holds a list of test samples (model inputs) paired with ground truths (expected outputs).</p>"},{"location":"reference/workflow/ground-truth/#kolena.workflow.ground_truth.GroundTruth","title":"<code>GroundTruth</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>The ground truth against which a model is evaluated.</p> <p>A test case contains one or more <code>TestSample</code> objects each paired with a ground truth object. During evaluation, these test samples, ground truths, and your model's inferences are provided to the <code>Evaluator</code> implementation.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>For <code>Composite</code>, each object can contain multiple basic test sample elements. To associate a set of attributes and/or annotations as the ground truth to a target test sample element, declare annotations by extending <code>DataObject</code> and use the same attribute name as used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding ground truth type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\n\nfrom kolena.workflow import DataObject, GroundTruth\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\n    bounding_box: BoundingBox\n    keypoints: Keypoints\n\n@dataclass(frozen=True)\nclass FacePair(GroundTruth):\n    source: FaceRegion\n    target: FaceRegion\n    is_same_person: bool\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/inference/","title":"<code>kolena.workflow.Inference</code>","text":"<p>The output from a <code>Model</code>. In other words, a model is a deterministic transformation from a <code>TestSample</code> to an <code>Inference</code>.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\n\nfrom kolena.workflow import Inference\nfrom kolena.workflow.annotation import Keypoints\n\n@dataclass(frozen=True)\nclass PoseEstimate(Inference):\n    skeleton: Optional[Keypoints] = None  # leave empty if nothing is detected\n    confidence: Optional[float] = None\n</code></pre>"},{"location":"reference/workflow/inference/#kolena.workflow.inference.Inference","title":"<code>Inference</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>The inference produced by a model.</p> <p>Typically the structure of this object closely mirrors the structure of the <code>GroundTruth</code> for a workflow, but this is not a requirement.</p> <p>During evaluation, the <code>TestSample</code> objects, ground truth objects, and these inference objects are provided to the <code>Evaluator</code> implementation to compute metrics.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>A model processing a <code>Composite</code> test sample can produce an inference result for each of its elements. To associate an inference result to each test sample element, put the attributes and/or annotations inside a <code>DataObject</code> and use the same attribute name as that used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding inference type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\n\nfrom kolena.workflow import DataObject, Inference\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\n    bounding_box: BoundingBox\n    keypoints: Keypoints\n\n@dataclass(frozen=True)\nclass FacePair(Inference):\n    source: FaceRegion\n    target: FaceRegion\n    similarity: float\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/io/","title":"<code>kolena.workflow.io</code>","text":""},{"location":"reference/workflow/io/#kolena.workflow.io.dataframe_to_csv","title":"<code>dataframe_to_csv(df, *args, **kwargs)</code>","text":"<p>Helper function to export pandas DataFrame containing annotation or asset to CSV format.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>positional arguments to <code>pandas.DataFrame.to_csv</code>.</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to <code>pandas.DataFrame.to_csv</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>None or str.</p>"},{"location":"reference/workflow/io/#kolena.workflow.io.dataframe_from_csv","title":"<code>dataframe_from_csv(*args, **kwargs)</code>","text":"<p>Helper function to load pandas DataFrame exported to CSV with <code>dataframe_to_csv</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>positional arguments to <code>pandas.DataFrame.read_csv</code>.</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to <code>pandas.DataFrame.read_csv</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame.</p>"},{"location":"reference/workflow/io/#kolena.workflow.io.dataframe_from_json","title":"<code>dataframe_from_json(*args, **kwargs)</code>","text":"<p>Helper function to load pandas DataFrame containing annotation or asset from JSON file or string.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>positional arguments to <code>pandas.DataFrame.read_json</code>.</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to <code>pandas.DataFrame.read_json</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame.</p>"},{"location":"reference/workflow/metrics/","title":"<code>kolena.workflow.metrics</code>","text":""},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.InferenceMatches","title":"<code>InferenceMatches</code>","text":"<p>             Bases: <code>Generic[GT, Inf]</code></p> <p>The result of <code>match_inferences</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences. After applying some confidence threshold on returned inference objects, <code>InferenceMatches</code> can be used to calculate metrics such as precision and recall.</p> <p>Objects are of type <code>BoundingBox</code> or <code>Polygon</code>, depending on the type of inputs provided to <code>match_inferences</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.InferenceMatches.matched","title":"<code>matched: List[Tuple[GT, Inf]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IoU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.InferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[GT]</code>  <code>instance-attribute</code>","text":"<p>Unmatched ground truth objects. Considered as false negatives.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.InferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.MulticlassInferenceMatches","title":"<code>MulticlassInferenceMatches</code>","text":"<p>             Bases: <code>Generic[GT_Multiclass, Inf_Multiclass]</code></p> <p>The result of <code>match_inferences_multiclass</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences.</p> <p>Unmatched ground truths may be matched with an inference of a different class when no inference of its own class is suitable, i.e. a \"confused\" match. <code>MultiClassInferenceMatches</code> can be used to calculate metrics such as precision and recall per class, after applying some confidence threshold on the returned inference objects.</p> <p>Objects are of type <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code>, depending on the type of inputs provided to <code>match_inferences_multiclass</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.MulticlassInferenceMatches.matched","title":"<code>matched: List[Tuple[GT_Multiclass, Inf_Multiclass]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IoU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.MulticlassInferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[Tuple[GT_Multiclass, Optional[Inf_Multiclass]]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of unmatched ground truth objects with its confused inference object (i.e. IoU above threshold with mismatching <code>label</code>), if such an inference exists. Considered as false negatives and \"confused\" detections.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.MulticlassInferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf_Multiclass]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.accuracy","title":"<code>accuracy(true_positives, false_positives, false_negatives, true_negatives)</code>","text":"<p>Accuracy represents the proportion of inferences that are correct (including both positives and negatives).</p> \\[ \\text{Accuracy} = \\frac{\\text{# TP} + \\text{# TN}} {\\text{# TP} + \\text{# FP} + \\text{# FN} + \\text{# TN}} \\] <ul> <li>  Metrics Glossary: Accuracy \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive inferences.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required <code>true_negatives</code> <code>int</code> <p>Number of true negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.f1_score","title":"<code>f1_score(true_positives, false_positives, false_negatives)</code>","text":"<p>F<sub>1</sub>-score is the harmonic mean between <code>precision</code> and <code>recall</code>.</p> \\[ \\begin{align} \\text{F1} &amp;= \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} \\\\[1em] &amp;= 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\end{align} \\] <ul> <li>  Metrics Glossary: F<sub>1</sub>-score \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive inferences.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.fpr","title":"<code>fpr(true_negatives, false_positives)</code>","text":"<p>False positive rate represents the proportion of negative ground truths that were incorrectly predicted as positive by the model.</p> \\[ \\text{FPR} = \\frac{\\text{# False Positives}}{\\text{# False Positives} + \\text{# True Negatives}} \\] <ul> <li>  Metrics Glossary: False Positive Rate \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_negatives</code> <code>int</code> <p>Number of true negatives.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.precision","title":"<code>precision(true_positives, false_positives)</code>","text":"<p>Precision represents the proportion of inferences that are correct.</p> \\[ \\text{Precision} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Positives}} \\] <ul> <li>  Metrics Glossary: Precision \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positive inferences.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.recall","title":"<code>recall(true_positives, false_negatives)</code>","text":"<p>Recall (TPR or sensitivity) represents the proportion of ground truths that were successfully predicted.</p> \\[ \\text{Recall} = \\frac{\\text{# True Positives}}{\\text{# True Positives} + \\text{# False Negatives}} \\] <ul> <li>  Metrics Glossary: Recall \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>int</code> <p>Number of true positive inferences.</p> required <code>false_negatives</code> <code>int</code> <p>Number of false negatives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.specificity","title":"<code>specificity(true_negatives, false_positives)</code>","text":"<p>Specificity (TNR) represents the proportion of negative ground truths that were correctly predicted.</p> \\[ \\text{Specificity} = \\frac{\\text{# True Negatives}}{\\text{# True Negatives} + \\text{# False Positives}} \\] <ul> <li>  Metrics Glossary: Specificity \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>true_negatives</code> <code>int</code> <p>Number of true negatives.</p> required <code>false_positives</code> <code>int</code> <p>Number of false positives.</p> required"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.iou","title":"<code>iou(a, b)</code>","text":"<p>Compute the Intersection Over Union (IoU) of two geometries.</p> <ul> <li>  Metrics Glossary: Intersection over Union (IoU) \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[BoundingBox, Polygon]</code> <p>The first geometry in computation.</p> required <code>b</code> <code>Union[BoundingBox, Polygon]</code> <p>The second geometry in computation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The value of the IoU between geometries <code>a</code> and <code>b</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences","title":"<code>match_inferences(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher does not consider labels, which is appropriate for single class object matching. To match with multiple classes (i.e. heeding <code>label</code> classifications), use the multiclass matcher <code>match_inferences_multiclass</code>.</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IoU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <ul> <li>  Metrics Glossary: Geometry Matching \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Geometry]</code> <p>A list of <code>BoundingBox</code> or <code>Polygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredGeometry]</code> <p>A list of <code>ScoredBoundingBox</code> or <code>ScoredPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[Geometry]]</code> <p>Optionally specify a list of <code>BoundingBox</code> or <code>Polygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>InferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IoU (intersection over union, see <code>iou</code>) threshold for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>InferenceMatches[GT, Inf]</code> <p><code>InferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives) and unmatched inferences (false positives).</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences_multiclass","title":"<code>match_inferences_multiclass(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher considers <code>label</code> values matching per class. After matching inferences and ground truths with equivalent <code>label</code> values, unmatched inferences and unmatched ground truths are matched once more to identify confused matches, where localization succeeded (i.e. IoU above <code>iou_threshold</code>) but classification failed (i.e. mismatching <code>label</code> values).</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IoU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <ul> <li>  Metrics Glossary: Geometry Matching \u2197</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[LabeledGeometry]</code> <p>A list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredLabeledGeometry]</code> <p>A list of <code>ScoredLabeledBoundingBox</code> or <code>ScoredLabeledPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[LabeledGeometry]]</code> <p>Optionally specify a list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>MulticlassInferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IoU threshold cutoff for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>MulticlassInferenceMatches[GT_Multiclass, Inf_Multiclass]</code> <p><code>MulticlassInferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives), and unmatched inferences (false positives).</p>"},{"location":"reference/workflow/model/","title":"<code>kolena.worfklow.Model</code>","text":""},{"location":"reference/workflow/model/#kolena.workflow.model.Model","title":"<code>Model(name, infer=None, metadata=None, tags=None)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>The descriptor of a model tested on Kolena. A model is a deterministic transformation from <code>TestSample</code> inputs to <code>Inference</code> outputs.</p> <p>Rather than importing this class directly, use the <code>Model</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this model. Automatically populated when constructing via the model type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name of the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.metadata","title":"<code>metadata: Dict[str, Any]</code>  <code>instance-attribute</code>","text":"<p>Unstructured metadata associated with the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.tags","title":"<code>tags: Set[str]</code>  <code>instance-attribute</code>","text":"<p>Tags associated with this model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.infer","title":"<code>infer: Optional[Callable[[TestSample], Inference]]</code>  <code>instance-attribute</code>","text":"<p>Function transforming a <code>TestSample</code> for a workflow into an <code>Inference</code> object. Required when using <code>test</code> or <code>TestRun.run</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.create","title":"<code>create(name, infer=None, metadata=None, tags=None)</code>  <code>classmethod</code>","text":"<p>Create a new model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the new model to create.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional unstructured metadata to store with this model.</p> <code>None</code> <code>tags</code> <code>Optional[Set[str]]</code> <p>Optional set of tags to associate with this model.</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>The newly created model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load","title":"<code>load(name, infer=None)</code>  <code>classmethod</code>","text":"<p>Load an existing model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model to load.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load_inferences","title":"<code>load_inferences(test_case)</code>","text":"<p>Load all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case for which to load inferences.</p> required <p>Returns:</p> Type Description <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The ground truths and inferences for all test samples in the test case.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.iter_inferences","title":"<code>iter_inferences(test_case)</code>","text":"<p>Iterate over all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case over which to iterate inferences.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestSample, GroundTruth, Inference]]</code> <p>Iterator exposing the ground truths and inferences for all test samples in the test case.</p>"},{"location":"reference/workflow/plot/","title":"Plots: <code>kolena.workflow.plot</code>","text":"<p>This module surfaces plot definitions to visualize test-case-level data. Evaluator implementations can optionally compute plots using these definitions for visualization on the   Results page.</p> <p>The following plot types are available:</p> <ul> <li><code>CurvePlot</code></li> <li><code>Histogram</code></li> <li><code>BarPlot</code></li> <li><code>ConfusionMatrix</code></li> </ul>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.NumberSeries","title":"<code>NumberSeries = Sequence[Union[float, int]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.NullableNumberSeries","title":"<code>NullableNumberSeries = Sequence[Union[float, int, None]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values or <code>None</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.AxisConfig","title":"<code>AxisConfig</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Configuration for the format of a given axis on a plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.AxisConfig.type","title":"<code>type: Literal[linear, log]</code>  <code>instance-attribute</code>","text":"<p>Type of axis to display. Supported options are <code>linear</code> and <code>log</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Plot","title":"<code>Plot</code>","text":"<p>             Bases: <code>TypedDataObject[_PlotType]</code></p> <p>A data visualization shown when exploring model results in the web platform.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve","title":"<code>Curve</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>A single series on a <code>CurvePlot</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.x","title":"<code>x: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>x</code> coordinates of this curve. Length must match the provided <code>y</code> coordinates.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.y","title":"<code>y: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>y</code> coordinates of this curve. Length must match the provided <code>x</code> coordinates.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.label","title":"<code>label: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify an additional label (in addition to the associated test case) to apply to this curve, for use when e.g. there are multiple curves generated per test case.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Curve.extra","title":"<code>extra: Optional[Dict[str, NumberSeries]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify additional series shown when hovering over the plot. For example, when plotting a precision-recall curve, it is desirable to include an extra series <code>threshold</code> to specify the confidence threshold value at which a given precision-recall point occurs.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot","title":"<code>CurvePlot</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A plot visualizing one or more curves per test case.</p> <p>Examples include Receiver Operating Characteristic (ROC) curves, Precision versus Recall (PR) curves, Detection-Error Tradeoff (DET) curves, etc.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.curves","title":"<code>curves: List[Curve]</code>  <code>instance-attribute</code>","text":"<p>A test case may generate zero or more curves on a given plot. However, under most circumstances, a single curve per test case is desirable.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.CurvePlot.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram","title":"<code>Histogram</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A plot visualizing distribution of one or more continuous values, e.g. distribution of an error metric across all samples within a test case.</p> <p>For visualization of discrete values, see <code>BarPlot</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.buckets","title":"<code>buckets: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>A Histogram requires intervals to bucket the data. For <code>n</code> buckets, <code>n+1</code> consecutive bounds must be specified in increasing order.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.frequency","title":"<code>frequency: Union[NumberSeries, Sequence[NumberSeries]]</code>  <code>instance-attribute</code>","text":"<p>For <code>n</code> buckets, there are <code>n</code> frequencies corresponding to the height of each bucket. The frequency at index <code>i</code> corresponds to the bucket with bounds (<code>i</code>, <code>i+1</code>) in <code>buckets</code>.</p> <p>To specify multiple distributions for a given test case, multiple frequency series can be provided, corresponding e.g. to the distribution for a given class within a test case, with name specified in <code>labels</code>.</p> <p>Specify a list of labels corresponding to the different <code>frequency</code> series when multiple series are provided. Can be omitted when a single <code>frequency</code> series is provided.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.labels","title":"<code>labels: Optional[List[str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Specify the label corresponding to a given distribution when multiple are specified in <code>frequency</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.Histogram.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot","title":"<code>BarPlot</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A plot visualizing a set of bars per test case.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis along which the bars are laid out (<code>labels</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis corresponding to bar height (<code>values</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.labels","title":"<code>labels: Sequence[Union[str, int, float]]</code>  <code>instance-attribute</code>","text":"<p>Labels for each bar with corresponding height specified in <code>values</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.values","title":"<code>values: NullableNumberSeries</code>  <code>instance-attribute</code>","text":"<p>Values for each bar with corresponding label specified in <code>labels</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.BarPlot.config","title":"<code>config: Optional[AxisConfig] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Custom format options to allow for control over the display of the numerical plot axis (<code>values</code>).</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix","title":"<code>ConfusionMatrix</code>","text":"<p>             Bases: <code>Plot</code></p> <p>A confusion matrix. Example:</p> <pre><code>ConfusionMatrix(\n    title=\"Cat and Dog Confusion\",\n    labels=[\"Cat\", \"Dog\"],\n    matrix=[[90, 10], [5, 95]],\n)\n</code></pre> <p>Yields a confusion matrix of the form:</p> <pre><code>            Predicted\n\n            Cat   Dog\n           +----+----+\n       Cat | 90 | 10 |\nActual     +----+----+\n       Dog |  5 | 95 |\n           +----+----+\n</code></pre>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.labels","title":"<code>labels: List[str]</code>  <code>instance-attribute</code>","text":"<p>The labels corresponding to each entry in the square <code>matrix</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.matrix","title":"<code>matrix: Sequence[NullableNumberSeries]</code>  <code>instance-attribute</code>","text":"<p>A square matrix, typically representing the number of matches between class <code>i</code> and class <code>j</code>.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.x_label","title":"<code>x_label: str = 'Predicted'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The label for the <code>x</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/plot/#kolena.workflow.plot.ConfusionMatrix.y_label","title":"<code>y_label: str = 'Actual'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The label for the <code>y</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/test-case/","title":"<code>kolena.workflow.TestCase</code>","text":""},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase","title":"<code>TestCase(name, version=None, description=None, test_samples=None, reset=False)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test case holds a list of test samples paired with ground truths representing a testing dataset or a slice of a testing dataset.</p> <p>Rather than importing this class directly, use the <code>TestCase</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test case. Automatically populated when constructing via test case type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test case. Cannot be changed after creation.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test case. A test case's version is automatically incremented whenever it is edited via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test case. Can be edited at any time via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor","title":"<code>Editor(description, reset)</code>","text":""},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.add","title":"<code>add(test_sample, ground_truth)</code>","text":"<p>Add a test sample to the test case. When the test sample already exists in the test case, its ground truth is overwritten with the ground truth provided here.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to add.</p> required <code>ground_truth</code> <code>GroundTruth</code> <p>The ground truth for the test sample.</p> required"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.Editor.remove","title":"<code>remove(test_sample)</code>","text":"<p>Remove a test sample from the test case. Does nothing if the test sample is not in the test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to remove.</p> required"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.create","title":"<code>create(name, description=None, test_samples=None)</code>  <code>classmethod</code>","text":"<p>Create a new test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test case to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test case to create.</p> <code>None</code> <code>test_samples</code> <code>Optional[List[Tuple[TestSample, GroundTruth]]]</code> <p>Optionally specify a set of test samples and ground truths to populate the test case.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The newly created test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test case to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The loaded test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load all <code>TestSample</code>s and <code>GroundTruth</code>s contained in this test case.</p> <p>Returns:</p> Type Description <code>List[Tuple[TestSample, GroundTruth]]</code> <p>A list of each test sample, paired with its ground truth, in this test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through all <code>TestSample</code>s and <code>GroundTruth</code>s contained in this test case.</p> <p>Returns:</p> Type Description <code>Iterator[Tuple[TestSample, GroundTruth]]</code> <p>An iterator yielding each test sample, paired with its ground truth, in this test case.</p>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test case in a context:</p> <pre><code>with test_case.edit() as editor:\n    # perform as many editing actions as desired\n    editor.add(...)\n    editor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear all existing test samples in the test case.</p> <code>False</code>"},{"location":"reference/workflow/test-case/#kolena.workflow.test_case.TestCase.init_many","title":"<code>init_many(data, reset=False)</code>  <code>classmethod</code>","text":"<p>Experimental</p> <p>This function is considered experimental, so beware that it is subject to changes even in patch releases.</p> <p>Create, load or edit multiple test cases.</p> <pre><code>test_cases = TestCase.init_many([\n    (\"test-case 1\", [(test_sample_1, ground_truth_1), ...]),\n    (\"test-case 2\", [(test_sample_2, ground_truth_2), ...])\n])\n\ntest_suite = TestSuite(\"my test suite\", test_cases=test_cases)\n</code></pre> <p>Changes are committed to the Kolena platform together. If there is an error, none of the edits would take effect.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Tuple[str, List[Tuple[TestSample, GroundTruth]]]]</code> <p>A list of tuples where each tuple is a test case name and a set of test samples and ground truths tuples for the test case.</p> required <code>reset</code> <code>bool</code> <p>If a test case of the same name already exists, overwrite with the provided test_samples.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[TestCase]</code> <p>The test cases.</p>"},{"location":"reference/workflow/test-run/","title":"<code>kolena.workflow.test</code>","text":""},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun","title":"<code>TestRun(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A <code>Model</code> tested on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Optional[Union[Evaluator, BasicEvaluatorFunction]]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>a list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>overwrites existing inferences if set.</p> <code>False</code>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.run","title":"<code>run()</code>","text":"<p>Run the testing process, first extracting inferences for all test samples in the test suite then performing evaluation.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>List[TestSample]</code> <p>a list of all test samples in the test suite still requiring inferences.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>Iterator[TestSample]</code> <p>an iterator over each test sample still requiring inferences.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.upload_inferences","title":"<code>upload_inferences(inferences)</code>","text":"<p>Upload inferences from a model.</p> <p>Parameters:</p> Name Type Description Default <code>inferences</code> <code>List[Tuple[TestSample, Inference]]</code> <p>the inferences, paired with their corresponding test samples, to upload.</p> required"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.TestRun.evaluate","title":"<code>evaluate()</code>","text":"<p>Perform evaluation by computing metrics for individual test samples, in aggregate across test cases, and across the complete test suite at each <code>EvaluatorConfiguration</code>.</p>"},{"location":"reference/workflow/test-run/#kolena.workflow.test_run.test","title":"<code>test(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>Test a <code>Model</code> on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <pre><code>from kolena.workflow import test\n\ntest(model, test_suite, evaluator, reset=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested, implementing the <code>infer</code> method.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Optional[Union[Evaluator, BasicEvaluatorFunction]]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>A list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code>"},{"location":"reference/workflow/test-sample/","title":"<code>kolena.workflow.TestSample</code>","text":"<p>Test samples are the inputs to your models when testing.</p> <p>For example, for a model that processes specific regions within a larger image, its test sample may be defined:</p> <pre><code>from dataclasses import dataclass\n\nfrom kolena.workflow import Image\nfrom kolena.workflow.annotation import BoundingBox\n\n@dataclass(frozen=True)\nclass ImageWithRegion(Image):\n    region: BoundingBox\n\nexample = ImageWithRegion(\n    locator=\"s3://my-bucket/example-image.png\",  # field from Image base class\n    region=BoundingBox(top_left=(0, 0), bottom_right=(100, 100)),\n)\n</code></pre>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Metadata","title":"<code>Metadata = Dict[str, Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool, List[Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool]]]]</code>  <code>module-attribute</code>","text":"<p>Type of the <code>metadata</code> field that can be included on <code>TestSample</code> definitions. String (<code>str</code>) keys and scalar values (<code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>None</code>) as well as scalar list values are permitted.</p> <pre><code>from dataclasses import dataclass, field\nfrom kolena.workflow import Image, Metadata\n\n@dataclass(frozen=True)\nclass ImageWithMetadata(Image):\n    metadata: Metadata = field(default_factory=dict)\n</code></pre>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.TestSample","title":"<code>TestSample</code>","text":"<p>             Bases: <code>TypedDataObject[_TestSampleType]</code></p> <p>The inputs to a model.</p> <p>Test samples can be customized as necessary for a workflow by extending this class or one of the built-in test sample types.</p> <p>Extensions to the <code>TestSample</code> class may define a <code>metadata</code> field of type <code>Metadata</code> containing a dictionary of scalar properties associated with the test sample, intended for use when sorting or filtering test samples.</p> <p>Kolena handles the <code>metadata</code> field differently from other test sample fields. Updates to the <code>metadata</code> object for a given test sample are merged with previously uploaded metadata. As such, <code>metadata</code> for a given test sample within a test case is not immutable, and should not be relied on when an implementation of <code>Model</code> computes inferences, or when an implementation of <code>Evaluator</code> evaluates metrics.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Composite","title":"<code>Composite</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A test sample composed of multiple basic <code>TestSample</code> elements.</p> <p>An example application would be each test sample is a pair of face images, and the goal is to predict whether the two images are of the same person. For this use-case the test sample can be defined as:</p> <pre><code>class FacePairSample(Composite):\n    source: Image\n    target: Image\n</code></pre> <p>To facilitate visualization for this kind of use cases, see usage of <code>GroundTruth</code> and <code>Inference</code>.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Image","title":"<code>Image</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>An image located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Image.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The URL of this image, using e.g. <code>s3</code>, <code>gs</code>, or <code>https</code> scheme (<code>s3://my-bucket/path/to/image.png</code>).</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair","title":"<code>ImagePair</code>","text":"<p>             Bases: <code>Composite</code></p> <p>Two <code>Image</code>s paired together.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair.a","title":"<code>a: Image</code>  <code>instance-attribute</code>","text":"<p>The left <code>Image</code> in the image pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImagePair.b","title":"<code>b: Image</code>  <code>instance-attribute</code>","text":"<p>The right <code>Image</code> in the image pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Text","title":"<code>Text</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>An inline text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Text.text","title":"<code>text: str</code>  <code>instance-attribute</code>","text":"<p>The text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText","title":"<code>ImageText</code>","text":"<p>             Bases: <code>Composite</code></p> <p>An image paired with a text snippet.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText.image","title":"<code>image: Image</code>  <code>instance-attribute</code>","text":"<p>The <code>Image</code> in this image-text pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.ImageText.text","title":"<code>text: Text</code>  <code>instance-attribute</code>","text":"<p>The text snippet in this image-text pair.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.BaseVideo","title":"<code>BaseVideo</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.BaseVideo.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video","title":"<code>Video</code>","text":"<p>             Bases: <code>BaseVideo</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.start","title":"<code>start: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Video.end","title":"<code>end: Optional[float] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Document","title":"<code>Document</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A remotely linked document, e.g. PDF or TXT file.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Document.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the document.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.PointCloud","title":"<code>PointCloud</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>A pointcloud file located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.PointCloud.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The URL of the pointcloud file, using e.g. <code>s3</code>, <code>gs</code>, or <code>https</code> scheme (<code>s3://my-bucket/path/to/image.pcd</code>).</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Audio","title":"<code>Audio</code>","text":"<p>             Bases: <code>TestSample</code></p> <p>An audio file located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test-sample/#kolena.workflow.test_sample.Audio.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the audio file.</p>"},{"location":"reference/workflow/test-suite/","title":"<code>kolena.workflow.TestSuite</code>","text":""},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite","title":"<code>TestSuite(name, version=None, description=None, test_cases=None, reset=False, tags=None)</code>","text":"<p>             Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test suite groups together one or more test cases. Typically a test suite represents a benchmark test dataset, with test cases representing different meaningful subsets, or slices, or this benchmark.</p> <p>Rather than importing this class directly, use the <code>TestSuite</code> type definition returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test suite. Automatically populated when constructing via test suite type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test suite. A test suite's version is automatically incremented whenever it is edited via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test suite. Can be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.test_cases","title":"<code>test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The <code>TestCase</code> objects belonging to this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.tags","title":"<code>tags: Set[str]</code>  <code>instance-attribute</code>","text":"<p>The tags associated with this test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor","title":"<code>Editor(test_cases, description, tags, reset)</code>","text":""},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.add","title":"<code>add(test_case)</code>","text":"<p>Add a test case to this test suite. If a different version of the test case already exists in this test suite, it is replaced.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to add to the test suite.</p> required"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.Editor.remove","title":"<code>remove(test_case)</code>","text":"<p>Remove a test case from this test suite. Does nothing if the test case is not in the test suite.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to remove.</p> required"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.create","title":"<code>create(name, description=None, test_cases=None, tags=None)</code>  <code>classmethod</code>","text":"<p>Create a new test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test suite to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test suite to create.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally specify a list of test cases to populate the test suite.</p> <code>None</code> <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to attach to the test suite.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The newly created test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test suite to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test suite to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The loaded test suite.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load_all","title":"<code>load_all(*, tags=None)</code>  <code>classmethod</code>","text":"<p>Load the latest version of all non-archived test suites with this workflow.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to apply as a filter. The loaded test suites will include only test suites with tags matching each of these specified tags, i.e. <code>test_suite.tags.intersection(tags) == tags</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[TestSuite]</code> <p>The latest version of all non-archived test suites, with matching tags when specified.</p>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test suite in a context:</p> <pre><code>with test_suite.edit() as editor:\n    # perform as many editing actions as desired\n    editor.add(...)\n    editor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear all existing test cases in the test suite.</p> <code>False</code>"},{"location":"reference/workflow/test-suite/#kolena.workflow.test_suite.TestSuite.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load test samples for all test cases within this test suite.</p> <p>Returns:</p> Type Description <code>List[Tuple[TestCase, List[TestSample]]]</code> <p>A list of <code>TestCase</code>s, each paired with the list of <code>TestSample</code>s it contains.</p>"},{"location":"reference/workflow/visualization/","title":"<code>kolena.workflow.visualization</code>","text":""},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.Colormap","title":"<code>Colormap(fade_low_activation=True)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A <code>Colormap</code> maps a pixel intensity to RGBA.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.Colormap.fade_low_activation","title":"<code>fade_low_activation: bool = fade_low_activation</code>  <code>instance-attribute</code>","text":"<p>Fades out the regions with low activation by applying zero alpha value if set <code>True</code>; otherwise, activation map is shown as is without any fading applied. By default, it's set to <code>True</code>. This option makes the overlay visualization better by highlighting only the important regions.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.Colormap.red","title":"<code>red(intensity)</code>  <code>abstractmethod</code>","text":"<p>Maps a grayscale pixel intensity to color red: [0, 255]</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.Colormap.green","title":"<code>green(intensity)</code>  <code>abstractmethod</code>","text":"<p>Maps a grayscale pixel intensity to color green: [0, 255]</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.Colormap.blue","title":"<code>blue(intensity)</code>  <code>abstractmethod</code>","text":"<p>Maps a grayscale pixel intensity to color blue: [0, 255]</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.Colormap.alpha","title":"<code>alpha(intensity)</code>","text":"<p>Maps the grayscale pixel intensity to alpha: [0, 255]. If <code>fade_low_activation</code> is False, then it returns the maximum alpha value.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.ColormapJet","title":"<code>ColormapJet(fade_low_activation=True)</code>","text":"<p>             Bases: <code>Colormap</code></p> <p>The MATLAB \"Jet\" color palette is a standard palette used for scientific and mathematical data.</p> <p>It is defined as a linear ramp between the following colours: \"#00007F\", \"blue\", \"#007FFF\", \"cyan\", \"#7FFF7F\", \"yellow\", \"#FF7F00\", \"red\", \"#7F0000\"</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.colorize_activation_map","title":"<code>colorize_activation_map(activation_map, colormap=ColormapJet())</code>","text":"<p>Applies the specified colormap to the activation map.</p> <p>Parameters:</p> Name Type Description Default <code>activation_map</code> <code>ndarray</code> <p>A 2D numpy array, shaped (h, w) or (h, w, 1), of the activation map in <code>np.uint8</code> or <code>float</code> ranging [0, 1].</p> required <code>colormap</code> <code>Optional[Colormap]</code> <p>The colormap used to colorize the input activation map. Defaults to the MATLAB \"Jet\" colormap.</p> <code>ColormapJet()</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The colorized activation map in RGBA format, in (h, w, 4) shape.</p>"},{"location":"reference/workflow/visualization/#kolena.workflow.visualization.encode_png","title":"<code>encode_png(image, mode)</code>","text":"<p>Encodes an image into an in-memory PNG file that is represented as binary data. It is used when you want to upload a 2 or 3-dimensional image in a NumPy array format to cloud.</p> <p>It can be used in conjunction with <code>colorized_activation_map</code> when uploading an activation map.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>A 2D or 3D NumPy array, shaped either <code>(h, w)</code>, <code>(h, w, 1)</code>, <code>(h, w, 3)</code>, or <code>(h, w, 4)</code></p> required <code>mode</code> <code>str</code> <p>A PIL mode</p> required <p>Returns:</p> Type Description <code>BytesIO</code> <p>The in-memory PNG file represented as binary data.</p>"}]}